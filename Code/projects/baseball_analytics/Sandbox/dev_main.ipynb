{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "37b6a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pybaseball as pyb\n",
    "import pybaseball.cache # Ensure caching is imported\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from rapidfuzz import process\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f549c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Build the PostgreSQL connection string\n",
    "DB_URL = f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASS']}@{os.environ['DB_HOST']}:5432/{os.environ['DB_NAME']}\"\n",
    "\n",
    "# Create the engine object for connecting\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "print(\"Database connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b060cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample function; the final ETL will be more complex.\n",
    "def extract_batting_data(year=2025):\n",
    "    # fgs is FanGraphs Season Stats\n",
    "    df = pyb.batting_stats(year)\n",
    "    # Rename columns to match your SQL schema (e.g., 'BB' for Walks, 'K' for Strikeouts)\n",
    "    df = df.rename(columns={'ID': 'player_id', 'Tm': 'team_id'})\n",
    "    return df\n",
    "\n",
    "current_year_batting = extract_batting_data(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_historical_batting(start_year=2024, end_year=2025):\n",
    "    \"\"\"Pulls batting stats for a range of years from FanGraphs and combines them.\"\"\"\n",
    "    \n",
    "    all_data_frames = []\n",
    "    \n",
    "    # Create the list of years to pull\n",
    "    years = range(start_year, end_year + 1)\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"-> Pulling FanGraphs data for {year}...\")\n",
    "        try:\n",
    "            # 1. Extraction: Pull data for a single year\n",
    "            df = pyb.batting_stats(year)\n",
    "            \n",
    "            # CRITICAL: Add a 'Season' column for historical tracking\n",
    "            df['Season'] = year \n",
    "            \n",
    "            # 2. Transformation (Initial Rename/Select)\n",
    "            # You must select and rename columns here to match your PostgreSQL schema\n",
    "            \n",
    "            # Example: Select and rename columns (Adjust this based on your exact schema!)\n",
    "            df = df.rename(columns={'Name': 'player_name', 'PlayerId': 'player_id', \n",
    "                                    'G': 'games_played', 'HR': 'home_runs', 'SO': 'strikeouts'})\n",
    "            \n",
    "            # 3. Append: Add the processed year's data to the list\n",
    "            all_data_frames.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to pull data for {year}: {e}\")\n",
    "            \n",
    "    # 4. Concatenate: Merge all DataFrames into one large DataFrame\n",
    "    if all_data_frames:\n",
    "        historical_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        print(f\"✅ Successfully combined data from {len(years)} seasons into {len(historical_df)} total rows.\")\n",
    "        return historical_df\n",
    "    \n",
    "    return pd.DataFrame() # Return empty if no data was pulled\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    full_batting_df = extract_historical_batting()\n",
    "    \n",
    "    # ... (Then proceed to your advanced metrics calculation and load_data function) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_statcast_data(start_date, end_date):\n",
    "    \"\"\"Pulls granular, pitch-by-pitch data for a specified date range.\"\"\"\n",
    "    print(f\"-> Pulling Statcast data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # pybaseball statcast function is designed to handle this extraction\n",
    "    raw_statcast_df = pyb.statcast(start_dt=start_date, end_dt=end_date)\n",
    "    \n",
    "    if raw_statcast_df is None or raw_statcast_df.empty:\n",
    "        print(\"Warning: No Statcast data returned for this date range.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return raw_statcast_df\n",
    "\n",
    "\n",
    "# Example\n",
    "test_start_date = '2025-10-28'\n",
    "test_end_date = '2025-10-30' \n",
    "\n",
    "daily_data = extract_statcast_data(test_start_date, test_end_date)\n",
    "print(f\"Successfully extracted {len(daily_data)} individual pitches/events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable pybaseball caching to speed up repeated queries\n",
    "pybaseball.cache.enable()\n",
    "\n",
    "# Set the connection engine using your environment variables (as previously defined)\n",
    "# engine = create_engine(DB_URL) \n",
    "\n",
    "def update_statcast_data(engine: Engine, days_to_keep: int = 400):\n",
    "    \"\"\"\n",
    "    Pulls recent Statcast data and keeps a rolling window of data \n",
    "    in the PostgreSQL statcast_pitches table.\n",
    "    \"\"\"\n",
    "    today = date.today()\n",
    "    \n",
    "    # We pull data from the end of last season (or roughly 400 days ago) \n",
    "    # up to yesterday to ensure we have a full window for rolling metrics.\n",
    "    start_date = today - timedelta(days=days_to_keep) \n",
    "    end_date = today - timedelta(days=1)\n",
    "    \n",
    "    start_dt_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_dt_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Starting Statcast ETL: Pulling data from {start_dt_str} to {end_dt_str}\")\n",
    "    \n",
    "    try:\n",
    "        # Use the general statcast function for league-wide pitch data\n",
    "        # 'statcast' is an alias for the league-wide Statcast search\n",
    "        df = pyb.statcast(start_dt=start_dt_str, end_dt=end_dt_str)\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"No new Statcast data retrieved. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # 1. Cleaning/Selection (CRITICAL)\n",
    "        # Select ONLY the columns you need to prevent errors during loading.\n",
    "        # Statcast column names are long; we'll rename them to match the SQL table.\n",
    "        df_clean = df.rename(columns={\n",
    "            'game_date': 'game_date', \n",
    "            'game_pk': 'game_pk', \n",
    "            'inning': 'inning', \n",
    "            'batter': 'batter_id', \n",
    "            'pitcher': 'pitcher_id', \n",
    "            'stand': 'stand', \n",
    "            'p_throws': 'p_throws',\n",
    "            'events': 'events',\n",
    "            'description': 'description',\n",
    "            'launch_speed': 'launch_speed', \n",
    "            'launch_angle': 'launch_angle',\n",
    "            'bb_type': 'bb_type',\n",
    "            'pitch_type': 'pitch_type',\n",
    "            'release_speed': 'release_speed',\n",
    "            'spin_rate': 'spin_rate'\n",
    "        })\n",
    "\n",
    "        # Only keep the columns that exist in our SQL schema\n",
    "        columns_to_keep = [col for col in df_clean.columns if col in ['game_date', 'game_pk', 'inning', 'batter_id', 'pitcher_id', 'stand', 'p_throws', 'events', 'description', 'launch_speed', 'launch_angle', 'bb_type', 'pitch_type', 'release_speed', 'spin_rate']]\n",
    "        final_df = df_clean[columns_to_keep]\n",
    "\n",
    "        # # 2. Loading: Use 'replace' for the first run, then switch to 'append' \n",
    "        # # for a daily ETL to avoid re-downloading old data.\n",
    "        # # Since we are pulling a large historical range, we should replace the table.\n",
    "        final_df.to_sql('statcast_pitches', engine, if_exists='append', index=False, chunksize=5000)\n",
    "        \n",
    "        # print(f\"✅ Successfully loaded {len(final_df)} rows of Statcast data into 'statcast_pitches'.\")\n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Statcast ETL Failed: {e}\")\n",
    "\n",
    "# Example Run (assuming 'engine' is defined with your credentials)\n",
    "statcast_data_df = update_statcast_data(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a049992",
   "metadata": {},
   "source": [
    "### This will update the table statcast_pitches in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d67664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database is empty. Starting full initial load (400 days)...\n",
      "Starting DAILY Statcast ETL: Pulling data from 2024-11-08 to 2025-12-11\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:14<00:00, 17.16it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 770795 new rows into 'statcast_pitches'...\n",
      "✅ Successfully appended 770795 new rows of Statcast data.\n"
     ]
    }
   ],
   "source": [
    "pybaseball.cache.enable() # Enable caching for reliability\n",
    "\n",
    "def update_statcast_data(engine: Engine):\n",
    "    \"\"\"\n",
    "    Pulls Statcast data starting from the day AFTER the last record in the database\n",
    "    to ensure only new events are downloaded and appended.\n",
    "    \"\"\"\n",
    "    \n",
    "    today = date.today()\n",
    "    \n",
    "    # --- STEP 1: FIND LAST DATE IN DB ---\n",
    "    try:\n",
    "        # Query the database to find the latest game_date currently stored\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(\n",
    "                text(\"SELECT MAX(game_date) FROM statcast_pitches;\")\n",
    "            ).scalar()\n",
    "        \n",
    "        # If the table is empty, start from 400 days ago (initial load range)\n",
    "        if result is None:\n",
    "            print(\"Database is empty. Starting full initial load (400 days)...\")\n",
    "            last_date = today - timedelta(days=400)\n",
    "        else:\n",
    "            # Start the new pull from the day AFTER the last record\n",
    "            last_date = result.date()\n",
    "            print(f\"Latest game_date found in DB: {last_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR querying database for last date: {e}. Defaulting to last 5 days.\")\n",
    "        last_date = today - timedelta(days=5)\n",
    "\n",
    "    \n",
    "    # --- STEP 2: DEFINE NEW EXTRACTION RANGE ---\n",
    "    start_date = last_date + timedelta(days=1)\n",
    "    end_date = today - timedelta(days=1) # Pull up to yesterday, as today's games aren't finished\n",
    "\n",
    "    start_dt_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_dt_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    if start_date >= end_date:\n",
    "        print(f\"Data is up to date as of {end_dt_str}. No new extraction needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting DAILY Statcast ETL: Pulling data from {start_dt_str} to {end_dt_str}\")\n",
    "    \n",
    "    # --- STEP 3: EXTRACTION ---\n",
    "    try:\n",
    "        df = pyb.statcast(start_dt=start_dt_str, end_dt=end_dt_str)\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            print(\"No new Statcast data retrieved for this date range. Exiting.\")\n",
    "            return\n",
    "\n",
    "        #  --- STEP 4: TRANSFORMATION ---        \n",
    "        # Handle data types before loading (optional, but good practice)\n",
    "        df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "        \n",
    "        # # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(df)} new rows into 'statcast_pitches'...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            'statcast_pitches', \n",
    "            engine, \n",
    "            if_exists='replace', # CRITICAL: Append new data to the existing table\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully appended {len(df)} new rows of Statcast data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Statcast ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n",
    "# Execute the daily update\n",
    "update_statcast_data(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036bfa2",
   "metadata": {},
   "source": [
    "### Update players table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98b29d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 25901 new rows into 'players'...\n",
      "✅ Players successfully added 25901 new rows of players data.\n"
     ]
    }
   ],
   "source": [
    "def update_players(engine: Engine):   \n",
    "    try:\n",
    "        df = pyb.chadwick_register()\n",
    "        \n",
    "        # # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(df)} new rows into 'players'...\")\n",
    "        \n",
    "        df.to_sql(\n",
    "            'players', \n",
    "            engine, \n",
    "            if_exists='replace',\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Players successfully added {len(df)} new rows of players data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Statcast ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n",
    "# Execute the players function\n",
    "update_players(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6532c",
   "metadata": {},
   "source": [
    "### Get team stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_batting  = pyb.team_batting(2025)\n",
    "team_pitching = pyb.team_pitching(2025)\n",
    "team_fielding = pyb.team_fielding(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f0b3a",
   "metadata": {},
   "source": [
    "### Get players stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36acbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_stats  = pyb.batting_stats(2025,  qual=0)\n",
    "pitching_stats = pyb.pitching_stats(2025, qual=0)\n",
    "fielding_stats = pyb.fielding_stats(2025, qual=0)\n",
    "running_stats  = pyb.statcast_sprint_speed(2025, 50) #players with at least 50 opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c652b",
   "metadata": {},
   "source": [
    "### Get scores last n days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6ef12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for all games played from 2025-12-11 to 2025-12-11...\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-11 and 2025-12-11.\n",
      "Searching for all games played from 2025-12-05 to 2025-12-11...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-05 and 2025-12-11.\n",
      "Searching for all games played from 2025-11-27 to 2025-12-11...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-11-27 and 2025-12-11.\n",
      "Searching for all games played from 2025-11-12 to 2025-12-11...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-11-12 and 2025-12-11.\n",
      "Searching for all games played from 2025-10-13 to 2025-12-11...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 13.37it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 5290 pitch events.\n",
      "Searching for all games played from 2025-09-13 to 2025-12-11...\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:04<00:00, 15.87it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 77978 pitch events.\n"
     ]
    }
   ],
   "source": [
    "import pybaseball as pyb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_game_results_last_n_days(n_days=90):\n",
    "    \"\"\"\n",
    "    Pulls raw pitch-by-pitch data for all games played in the last 'n_days' \n",
    "    and then extracts the final score for each game.\n",
    "    \"\"\"\n",
    "    today = datetime.date.today()\n",
    "    \n",
    "    # 1. Calculate the start and end dates for the 90-day range\n",
    "    end_date = today - datetime.timedelta(days=1)  # Search up to yesterday\n",
    "    start_date = today - datetime.timedelta(days=n_days)\n",
    "    \n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Searching for all games played from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "    try:\n",
    "        # 2. Pull all pitch-by-pitch data in that range\n",
    "        all_data_in_range = pyb.statcast(start_dt=start_date_str, end_dt=end_date_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving Statcast data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if all_data_in_range.empty:\n",
    "        print(f\"No games found between {start_date_str} and {end_date_str}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Successfully pulled {len(all_data_in_range)} pitch events.\")\n",
    "\n",
    "    # 3. Sort the data chronologically by game_pk, inning, etc.\n",
    "    data_sorted = all_data_in_range.sort_values(\n",
    "        by=['game_pk', 'inning', 'inning_topbot', 'at_bat_number', 'pitch_number'],\n",
    "        ascending=True\n",
    "    )\n",
    "\n",
    "    # 4. Group by game_pk and take the last row (which contains the final score)\n",
    "    final_events = data_sorted.groupby('game_pk').tail(1).reset_index(drop=True)\n",
    "    \n",
    "    # 5. Extract and rename the relevant columns for the final scoreboard\n",
    "    scoreboard = final_events[[\n",
    "        'game_date', \n",
    "        'home_team', \n",
    "        'away_team', \n",
    "        'home_score', \n",
    "        'away_score'\n",
    "    ]].copy()\n",
    "    \n",
    "    scoreboard.rename(columns={\n",
    "        'home_score': 'Home_Final_Score',\n",
    "        'away_score': 'Away_Final_Score',\n",
    "        'game_date': 'Date'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # 6. Determine the Winner\n",
    "    scoreboard['Winner'] = scoreboard.apply(\n",
    "        lambda row: row['home_team'] if row['Home_Final_Score'] > row['Away_Final_Score'] else row['away_team'],\n",
    "        axis=1\n",
    "    )\n",
    "    scoreboard['Result'] = (\n",
    "        scoreboard['Winner'] + ' wins ' + \n",
    "        scoreboard['Home_Final_Score'].astype(str) + '-' + \n",
    "        scoreboard['Away_Final_Score'].astype(str)\n",
    "    )\n",
    "    \n",
    "    return scoreboard[['Date', 'away_team', 'home_team', 'Away_Final_Score', 'Home_Final_Score', 'Winner', 'Result']]\n",
    "\n",
    "# --- EXECUTION ---\n",
    "results_yesterday_df    = get_game_results_last_n_days(n_days= 1)\n",
    "results_last_7_days_df  = get_game_results_last_n_days(n_days= 7)\n",
    "results_last_15_days_df = get_game_results_last_n_days(n_days= 15)\n",
    "results_last_30_days_df = get_game_results_last_n_days(n_days= 30)\n",
    "results_last_60_days_df = get_game_results_last_n_days(n_days= 60)\n",
    "results_last_90_days_df = get_game_results_last_n_days(n_days= 90)\n",
    "\n",
    "\n",
    "# if not results_last_90_days_df.empty:\n",
    "#     print(f\"\\n--- Game Results from the Last 90 Days ({len(results_last_90_days_df)} Games Found) ---\")\n",
    "#     print(results_last_90_days_df.tail(10)) # Print the last 10 games found\n",
    "# else:\n",
    "#     print(\"\\nNo games were found in the last 90 days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a4903",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c2c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 10.73it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "df = pyb.statcast(start_dt='2025-10-01', end_dt='2025-10-30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams_split(split_type, clean_mode):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Define year\n",
    "    year = datetime.now().year\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)  \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=total%7CLast%20{split_type}%20days%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=hmvis%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'first_batter_game':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=leado%7C1st%20Batter%20G%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_power_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Power%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_weak_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Finesse%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    # For each team:\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "            driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_less_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3C%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_greater_or_equal_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3E%3D%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    \n",
    "    # Name of the table\n",
    "    datatable_id = 'split1'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} ({split_type}) table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()    \n",
    "    \n",
    "    if clean_mode == 1:\n",
    "        # Remove 'Roe' exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "\n",
    "        # Rename last 3 columns\n",
    "        new_column_names = [\"BAbip\", \"tOPS+\", \"sOPS+\"]  # New names for last 3 columns\n",
    "        df.columns.values[-3:] = new_column_names  # Assign new names\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "    else:\n",
    "        # Remove 'Roe' and GS exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "        df[0] = df[0].str.replace('GS', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Remove rows where column 'A' contains 'Rk', but keep the first row\n",
    "        df = df[~((df.index > 0) & (df[0].str.contains('Rk', na=False)))]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "\n",
    "        # Remove the last 2 columns\n",
    "        df = df.iloc[:, :-2]\n",
    "\n",
    "        # New column names\n",
    "        new_column_names = ['Team', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'RBI', 'SB',\n",
    "                            'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDP', 'HBP', 'SH',\n",
    "                            'SF', 'IBB', 'ROE', 'BAbip', 'tOPS+', 'sOPS+']\n",
    "\n",
    "        # Rename all columns\n",
    "        df.columns = new_column_names\n",
    "\n",
    "    return df\n",
    "\n",
    "# Call the function to get the teams split data\n",
    "team_vs_lhp             = teams_split(split_type= 'LHP',  clean_mode= 0) # GS empty\n",
    "team_vs_rhp             = teams_split(split_type= 'RHP',  clean_mode= 0) # GS empty\n",
    "team_vs_lh_starters     = teams_split(split_type= 'LH',   clean_mode= 1)\n",
    "team_vs_rh_starters     = teams_split(split_type= 'RH',   clean_mode= 1)\n",
    "team_last_seven_days    = teams_split(split_type= '7',    clean_mode= 1)\n",
    "team_last_fourteen_days = teams_split(split_type= '14',   clean_mode= 1)\n",
    "team_last_28_days       = teams_split(split_type= '28',   clean_mode= 1)\n",
    "team_home_games         = teams_split(split_type= 'Home', clean_mode= 1)\n",
    "team_away_games         = teams_split(split_type= 'Away', clean_mode= 1)\n",
    "team_first_batter_game  = teams_split(split_type= 'first_batter_game', clean_mode= 0) # GS empty\n",
    "team_vs_power_pitcher   = teams_split(split_type= 'vs_power_pitcher',  clean_mode= 0) # GS empty\n",
    "team_vs_weak_pitcher    = teams_split(split_type= 'vs_weak_pitcher',   clean_mode= 0) # GS empty\n",
    "team_vs_power_team      = teams_split(split_type= 'vs_greater_or_equal_than_500_WP', clean_mode= 1)\n",
    "team_vs_weak_team       = teams_split(split_type= 'vs_less_than_500_WP',             clean_mode= 1)\n",
    "\n",
    "# # Direct matchups\n",
    "team_laa = teams_split(split_type= 'ANA', clean_mode= 1)\n",
    "team_ari = teams_split(split_type= 'ARI', clean_mode= 1)\n",
    "team_atl = teams_split(split_type= 'ATL', clean_mode= 1)\n",
    "team_bal = teams_split(split_type= 'BAL', clean_mode= 1)\n",
    "team_bos = teams_split(split_type= 'BOS', clean_mode= 1)\n",
    "team_chc = teams_split(split_type= 'CHC', clean_mode= 1)\n",
    "team_chw = teams_split(split_type= 'CHW', clean_mode= 1)\n",
    "team_cin = teams_split(split_type= 'CIN', clean_mode= 1)\n",
    "team_cle = teams_split(split_type= 'CLE', clean_mode= 1)\n",
    "team_col = teams_split(split_type= 'COL', clean_mode= 1)\n",
    "team_det = teams_split(split_type= 'DET', clean_mode= 1)\n",
    "team_hou = teams_split(split_type= 'HOU', clean_mode= 1)\n",
    "team_kcr = teams_split(split_type= 'KCR', clean_mode= 1)\n",
    "team_lad = teams_split(split_type= 'LAD', clean_mode= 1)\n",
    "team_mia = teams_split(split_type= 'FLA', clean_mode= 1) \n",
    "team_mil = teams_split(split_type= 'MIL', clean_mode= 1)\n",
    "team_min = teams_split(split_type= 'MIN', clean_mode= 1)\n",
    "team_nym = teams_split(split_type= 'NYM', clean_mode= 1)\n",
    "team_nyy = teams_split(split_type= 'NYY', clean_mode= 1)\n",
    "team_oak = teams_split(split_type= 'OAK', clean_mode= 1)\n",
    "team_phi = teams_split(split_type= 'PHI', clean_mode= 1)\n",
    "team_pit = teams_split(split_type= 'PIT', clean_mode= 1)\n",
    "team_sdp = teams_split(split_type= 'SDP', clean_mode= 1)\n",
    "team_sea = teams_split(split_type= 'SEA', clean_mode= 1)\n",
    "team_sfg = teams_split(split_type= 'SFG', clean_mode= 1)\n",
    "team_stl = teams_split(split_type= 'STL', clean_mode= 1)\n",
    "team_tbr = teams_split(split_type= 'TBD', clean_mode= 1)\n",
    "team_tex = teams_split(split_type= 'TEX', clean_mode= 1)\n",
    "team_tor = teams_split(split_type= 'TOR', clean_mode= 1)\n",
    "team_wsn = teams_split(split_type= 'WSN', clean_mode= 1)\n",
    "\n",
    "# Dictionary of dataframes for the teams\n",
    "dic_team = {\n",
    "    'LAA': team_laa,\n",
    "    'AZ': team_ari,\n",
    "    'ATL': team_atl,\n",
    "    'BAL': team_bal,\n",
    "    'BOS': team_bos,\n",
    "    'CHC': team_chc,\n",
    "    'CHW': team_chw,\n",
    "    'CIN': team_cin,\n",
    "    'CLE': team_cle,\n",
    "    'COL': team_col,\n",
    "    'DET': team_det,\n",
    "    'HOU': team_hou,\n",
    "    'KC': team_kcr,\n",
    "    'LAD': team_lad,\n",
    "    'MIA': team_mia,\n",
    "    'MIL': team_mil,\n",
    "    'MIN': team_min,\n",
    "    'NYM': team_nym,\n",
    "    'NYY': team_nyy,\n",
    "    'ATH': team_oak,\n",
    "    'PHI': team_phi,\n",
    "    'PIT': team_pit,\n",
    "    'SD': team_sdp,\n",
    "    'SEA': team_sea,\n",
    "    'SF': team_sfg,\n",
    "    'STL': team_stl,\n",
    "    'TB': team_tbr,\n",
    "    'TEX': team_tex,\n",
    "    'TOR': team_tor,\n",
    "    'WSH': team_wsn   \n",
    "    }\n",
    "\n",
    "# Add an ID column with the dictionary key as the identifier\n",
    "for key, df in dic_team.items():\n",
    "    df['ID'] = key  # Assign the dictionary key as the ID\n",
    "\n",
    "# Concatenate all dataFrames in the dictionary\n",
    "direct_matches = pd.concat(dic_team.values(), ignore_index=True)  # Resets index\n",
    "\n",
    "dic_splits = {\n",
    "    'team_vs_lhp'        :team_vs_lhp,        \n",
    "    'team_vs_rhp'        :team_vs_rhp,\n",
    "    'team_vs_lh_starters':team_vs_lh_starters,\n",
    "    'team_vs_rh_starters':team_vs_rh_starters,\n",
    "    'team_last_seven_days':team_last_seven_days,\n",
    "    'team_last_fourteen_days':team_last_fourteen_days,\n",
    "    'team_last_28_days':team_last_28_days,\n",
    "    'team_home_games':team_home_games,\n",
    "    'team_away_games':team_away_games,\n",
    "    'team_first_batter_game':team_first_batter_game,\n",
    "    'team_vs_power_pitcher':team_vs_power_pitcher,\n",
    "    'team_vs_weak_pitcher':team_vs_weak_pitcher,\n",
    "    'team_vs_power_team':team_vs_power_team,\n",
    "    'team_vs_weak_team':team_vs_weak_team      \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0229e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C1st%20Batter%20G%7CNYY%7C2025%7Cbat%7CAB%7C\n",
      "Table team_split1 (1st%20Batter vs NYY) loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def players_split(split_type, team_abv, clean_mode):\n",
    "    # --- 1. SETUP (Only here because the logic is complex) ---\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    year = datetime.now().year\n",
    "    datatable_id = 'team_split1'\n",
    "    \n",
    "    # --- 2. URL CONSTRUCTION ---\n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days (uses all columns)\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters (uses all columns)\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games (uses all columns)\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season (uses all columns)\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month (uses all columns)\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == 'C' or split_type == '1B' or split_type == '2B' or split_type == '3B' \\\n",
    "        or split_type == 'SS' or split_type == 'LF' or split_type == 'CF' or split_type == 'RF' \\\n",
    "        or split_type == 'DH' or split_type == 'PH': # for each position\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=defp%7Cas%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    elif split_type == '1st%20Batter':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    elif split_type == 'first_batter_game':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=leado%7C1st%20Batter%20G%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    elif split_type == 'vs_power_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Power%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    elif split_type == 'vs_weak_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Finesse%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    # For each team:\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "            driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    elif split_type == 'vs_less_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3C%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    elif split_type == 'vs_greater_or_equal_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3E%3D%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    else:\n",
    "        # Handle other split types or raise an error\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame() # Return empty DataFrame on failure\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    # --- 3. WAIT AND EXTRACT ---\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Table {datatable_id} ({split_type} vs {team_abv}) loaded successfully.\")\n",
    "\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        text_content = table_element.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        driver.quit() # Close the driver in a finally block to ensure it closes\n",
    "        \n",
    "    # --- 4. DATA PROCESSING (Keep this logic) ---\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # ... (Your cleanup logic for clean_mode 0 or 1 remains here) ...\n",
    "    # This is where the complex splitting, renaming, and removal happens.\n",
    "    if clean_mode == 1:\n",
    "        #Remove 'GS' from the first row since it is empty\n",
    "        df.loc[0, 0] = df.loc[0, 0].replace('GS', '')\n",
    "        \n",
    "        # Define the correct headers\n",
    "        all_headers = [\n",
    "        'Name', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'RBI',\n",
    "        'SB', 'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDBP',\n",
    "        'HBP', 'SH', 'SF', 'IBB', 'ROE', 'BABIP', 'tOPS+', 'sOPS+'\n",
    "        ]\n",
    "        \n",
    "    elif clean_mode == 2:        \n",
    "        # Define the correct headers\n",
    "        all_headers = [\n",
    "        'Name', 'G', 'GS', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'RBI',\n",
    "        'SB', 'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDBP',\n",
    "        'HBP', 'SH', 'SF', 'IBB', 'ROE', 'BABIP', 'tOPS+', 'sOPS+'\n",
    "        ]\n",
    "\n",
    "    elif clean_mode == 3:\n",
    "        #Remove some empty values from the first row\n",
    "        df.loc[0, 0] = df.loc[0, 0].replace('GS', '')\n",
    "        df.loc[0, 0] = df.loc[0, 0].replace('R', '')\n",
    "        df.loc[0, 0] = df.loc[0, 0].replace('SB', '')\n",
    "        df.loc[0, 0] = df.loc[0, 0].replace('CS', '')\n",
    "        \n",
    "        # Define the correct headers\n",
    "        all_headers = [\n",
    "        'Name', 'G', 'PA', 'AB', 'H', '2B', '3B', 'HR', 'RBI',\n",
    "        'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDBP',\n",
    "        'HBP', 'SH', 'SF', 'IBB', 'ROE', 'BABIP', 'tOPS+', 'sOPS+'\n",
    "        ]\n",
    "        \n",
    "    # Remove last row\n",
    "    df = df.iloc[:-1]\n",
    "\n",
    "    # Remove rows where column 'A' contains 'Rk', but keep the first row\n",
    "    #df = df[~((df.index > 0) & (df[0].str.contains('Rk', na=False)))]\n",
    "\n",
    "    # 2. Use Regex to split the column into exactly two parts: Name/Rk and everything else (the stats)\n",
    "    # Regex Pattern Breakdown:\n",
    "    # (^.*?)      -> Capturing Group 1: Match and capture everything from the start (^) non-greedily (.*?)\n",
    "    # (\\s\\d+)     -> The split transition: Look for a space (\\s) followed immediately by a digit (\\d+).\n",
    "    #               This finds the first number after the name.\n",
    "    #               Crucially, we DON'T include this space-plus-digit in Group 1.\n",
    "    # (.*$)       -> Capturing Group 2: Match and capture everything from that point until the end ($).\n",
    "    regex_pattern = r\"^(.*?)\\s(\\d+.*$)\"\n",
    "\n",
    "    # Extract the two parts into new columns:\n",
    "    df[['Rk_Name_Combined', 'Stats_Combined']] = df[0].str.extract(regex_pattern, expand=True)\n",
    "    \n",
    "    # Remove the first row\n",
    "    df = df.iloc[1:]\n",
    "    \n",
    "    # NEW LOGIC: VERIFY AND DROP NON-CONFORMING STATS ROWS ---\n",
    "\n",
    "    # 1. Calculate the number of spaces (separators) in each row\n",
    "    space_counts = df['Stats_Combined'].str.count(' ')\n",
    "\n",
    "    # 2. Find the maximum number of spaces, which defines the expected structure\n",
    "    max_spaces = space_counts.max()\n",
    "\n",
    "    print(f\"Maximum spaces (expected separators): {max_spaces}\")\n",
    "\n",
    "    # 1. Calculate the number of spaces (separators) in each row\n",
    "    space_counts = df['Stats_Combined'].str.count(' ')\n",
    "\n",
    "    # 2. Find the maximum number of spaces, which defines the expected structure\n",
    "    max_spaces = space_counts.max()\n",
    "\n",
    "    print(f\"Maximum spaces (expected separators): {max_spaces}\")\n",
    "\n",
    "    # 3. Identify the indices of the rows that do NOT have the maximum number of spaces\n",
    "    indices_to_drop = space_counts[space_counts < max_spaces].index\n",
    "\n",
    "    # 4. Check if any rows need to be dropped\n",
    "    if indices_to_drop.empty:\n",
    "        print(\"All rows have a consistent number of spaces. Proceeding with split.\")\n",
    "    else:\n",
    "        num_dropped = len(indices_to_drop)\n",
    "        \n",
    "        # 5. Drop the rows that do not match the expected structure\n",
    "        df = df.drop(index=indices_to_drop)\n",
    "        \n",
    "        # Reset the index after dropping rows\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Removed {num_dropped} row(s) because they did not have the expected number of spaces ({max_spaces}).\")    \n",
    "    \n",
    "    # 3. Split the 'Stats_Combined' column on all spaces\n",
    "    # The Name is now safe, and the stats are simple space-separated values.\n",
    "    stats_split = df['Stats_Combined'].str.split(expand=True)\n",
    "\n",
    "    # 4. Combine the Name/Rank column with the split statistics\n",
    "    # We use iloc to get the columns we want.\n",
    "    final_df = pd.concat([\n",
    "        df['Rk_Name_Combined'], # The Name/Rank column\n",
    "        stats_split             # All the statistical columns\n",
    "    ], axis=1)\n",
    "\n",
    "    # --- Final Cleanup (Optional, but recommended) ---\n",
    "    # Separate the Rk from the Name in the first column for a cleaner table.\n",
    "    final_df[['Rk', 'Name']] = final_df['Rk_Name_Combined'].str.split(' ', n=1, expand=True)\n",
    "    final_df = final_df.drop(columns=['Rk_Name_Combined', 'Rk'])\n",
    "\n",
    "    # Move the last column to be the first column (Name)\n",
    "    final_df = final_df[['Name'] + [col for col in final_df.columns if col != 'Name']]\n",
    "\n",
    "    # 5. Apply the headers\n",
    "    final_df.columns = all_headers\n",
    "    \n",
    "    # Placeholder for the cleanup logic to make the function runnable\n",
    "    if final_df.empty: return pd.DataFrame() \n",
    "    # NOTE: You would re-insert your full cleanup logic here.\n",
    "\n",
    "    return final_df # Return the resulting DataFrame\n",
    "\n",
    "# --- EXECUTION ---\n",
    "#team_abbreviations = ['NYY', 'BOS', 'TBR', 'TOR', 'BAL'] # Use a small list for testing\n",
    "team_abbreviations = ['NYY'] # Use a small list for testing\n",
    "clean_mode_one   = []\n",
    "clean_mode_two   = []\n",
    "clean_mode_three = []\n",
    "\n",
    "def create_and_append_table(df_source, split_type, team_abv, df_clean_mode):\n",
    "    \"\"\"Creates and appends a table with split type and team abbreviation.\n",
    "\n",
    "    Args:\n",
    "        df_source (_type_): _description_\n",
    "        split_type (_type_): _description_\n",
    "        team_abv (_type_): _description_\n",
    "        df_clean_mode (_type_): _description_\n",
    "    \"\"\"\n",
    "    if not df_source.empty:\n",
    "        df_source['Split_Type'] = split_type\n",
    "        df_source['Team'] = team_abv\n",
    "        df_clean_mode.append(df_source)\n",
    "\n",
    "for team_abv in team_abbreviations:\n",
    "    # PLATOON SPLITS\n",
    "    df_lhp = players_split(split_type='LHP', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_lhp, 'vs_LHP', team_abv, clean_mode_one)\n",
    "\n",
    "    df_rhp = players_split(split_type='RHP', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_rhp, 'vs_RHP', team_abv, clean_mode_one)\n",
    "    \n",
    "    df_lh = players_split(split_type='LH', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_lh, 'vs_LH_starters', team_abv, clean_mode_two)\n",
    "    \n",
    "    df_rh = players_split(split_type='RH', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_rh, 'vs_RH_starters', team_abv, clean_mode_two)\n",
    "    \n",
    "    # # LAST N DAYS\n",
    "    df_last_seven_days = players_split(split_type='7', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_last_seven_days, 'last_seven_days', team_abv, clean_mode_two)\n",
    "    \n",
    "    df_last_fourteen = players_split(split_type='14', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_last_fourteen, 'last_fourteen_days', team_abv, clean_mode_two)\n",
    "\n",
    "    df_last_twenty_eight = players_split(split_type='28', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_last_twenty_eight, 'last_twenty_eight_days', team_abv, clean_mode_two)\n",
    "    \n",
    "    # # HOME OR AWAY\n",
    "    df_home = players_split(split_type='Home', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_home, 'home', team_abv, clean_mode_two)\n",
    "    \n",
    "    df_away = players_split(split_type='Away', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_away, 'away', team_abv, clean_mode_two)\n",
    "\n",
    "    # # FIRST AND SECOND HALF\n",
    "    df_first_half = players_split(split_type='1st', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_first_half, 'first_half', team_abv, clean_mode_two)\n",
    "\n",
    "    df_second_half = players_split(split_type='2nd', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_second_half, 'second_half', team_abv, clean_mode_two)\n",
    "\n",
    "    # MONTH\n",
    "    df_march_april = players_split(split_type='April%2FMarch', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_march_april, 'march_april', team_abv, clean_mode_two)\n",
    "\n",
    "    df_may = players_split(split_type='May', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_may, 'may', team_abv, clean_mode_two)\n",
    "\n",
    "    df_june = players_split(split_type='June', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_june, 'june', team_abv, clean_mode_two)\n",
    "\n",
    "    df_july = players_split(split_type='July', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_july, 'july', team_abv, clean_mode_two)\n",
    "\n",
    "    df_august = players_split(split_type='August', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_august, 'august', team_abv, clean_mode_two)\n",
    "        \n",
    "    df_sept_oct = players_split(split_type='Sept%2FOct', team_abv=team_abv, clean_mode= 2)\n",
    "    create_and_append_table(df_sept_oct, 'sept_oct', team_abv, clean_mode_two)\n",
    "\n",
    "    # DEFENSIVE POSITIONS\n",
    "    df_catcher = players_split(split_type='C', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_catcher, 'catcher', team_abv, clean_mode_one)\n",
    "        \n",
    "    df_first_base = players_split(split_type='1B', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_first_base, 'first_base', team_abv, clean_mode_one)\n",
    "\n",
    "    df_second_base = players_split(split_type='2B', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_second_base, 'second_base', team_abv, clean_mode_one)\n",
    "\n",
    "    df_third_base = players_split(split_type='3B', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_third_base, 'third_base', team_abv, clean_mode_one)\n",
    "\n",
    "    df_shortstop = players_split(split_type='SS', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_shortstop, 'shortstop', team_abv, clean_mode_one)\n",
    "\n",
    "    df_left_field = players_split(split_type='LF', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_left_field, 'left_field', team_abv, clean_mode_one)\n",
    "\n",
    "    df_center_field = players_split(split_type='CF', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_center_field, 'center_field', team_abv, clean_mode_one)\n",
    "        \n",
    "    df_right_field = players_split(split_type='RF', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_right_field, 'right_field', team_abv, clean_mode_one)\n",
    "        \n",
    "    df_designated_hitter = players_split(split_type='DH', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_designated_hitter, 'designated_hitter', team_abv, clean_mode_one)\n",
    "        \n",
    "    df_pinch_hitter = players_split(split_type='PH', team_abv=team_abv, clean_mode= 1)\n",
    "    create_and_append_table(df_pinch_hitter, 'pinch_hitter', team_abv, clean_mode_one)\n",
    "    \n",
    "    # LEADING OFF INNING\n",
    "    df_first_batter_of_the_game = players_split(split_type='1st%20Batter', team_abv=team_abv, clean_mode= 3)\n",
    "    create_and_append_table(df_first_batter_of_the_game, 'first_batter_of_game', team_abv, clean_mode_three)\n",
    "\n",
    "final_clean_mode_one   = pd.concat(clean_mode_one,   ignore_index=True)\n",
    "final_clean_mode_two   = pd.concat(clean_mode_two,   ignore_index=True)\n",
    "final_clean_mode_three = pd.concat(clean_mode_three, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C1st%20Batter%20G%7CNYY%7C2025%7Cbat%7CAB%7C\n",
      "Table team_split1 (1st%20Batter vs NYY) loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. SETUP (Only here because the logic is complex) ---\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "driver = webdriver.Chrome(options=options)\n",
    "year = datetime.now().year\n",
    "datatable_id = 'team_split1'\n",
    "clean_mode = 3\n",
    "\n",
    "split_type = '1st%20Batter'\n",
    "team_abv   = 'NYY'\n",
    "# --- 2. URL CONSTRUCTION ---\n",
    "if split_type == '1st%20Batter':\n",
    "    url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# --- 3. WAIT AND EXTRACT ---\n",
    "datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "try:\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "    )\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Table {datatable_id} ({split_type} vs {team_abv}) loaded successfully.\")\n",
    "\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "    \n",
    "finally:\n",
    "    driver.quit() # Close the driver in a finally block to ensure it closes\n",
    "    \n",
    "# --- 4. DATA PROCESSING (Keep this logic) ---\n",
    "rows = text_content.split(\"\\n\")\n",
    "table_data = [row.split(\"\\t\") for row in rows]\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "# ... (Your cleanup logic for clean_mode 0 or 1 remains here) ...\n",
    "# This is where the complex splitting, renaming, and removal happens.\n",
    "if clean_mode == 3:\n",
    "    #Remove some empty values from the first row\n",
    "    df.loc[0, 0] = df.loc[0, 0].replace('GS', '')\n",
    "    df.loc[0, 0] = df.loc[0, 0].replace('R', '')\n",
    "    df.loc[0, 0] = df.loc[0, 0].replace('SB', '')\n",
    "    df.loc[0, 0] = df.loc[0, 0].replace('CS', '')\n",
    "    \n",
    "    # Define the correct headers\n",
    "    all_headers = [\n",
    "    'Name', 'G', 'PA', 'AB', 'H', '2B', '3B', 'HR', 'RBI',\n",
    "    'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDBP',\n",
    "    'HBP', 'SH', 'SF', 'IBB', 'ROE', 'BABIP', 'tOPS+', 'sOPS+'\n",
    "    ]\n",
    "    \n",
    "# Remove last row\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "# Remove rows where column 'A' contains 'Rk', but keep the first row\n",
    "#df = df[~((df.index > 0) & (df[0].str.contains('Rk', na=False)))]\n",
    "\n",
    "# 2. Use Regex to split the column into exactly two parts: Name/Rk and everything else (the stats)\n",
    "# Regex Pattern Breakdown:\n",
    "# (^.*?)      -> Capturing Group 1: Match and capture everything from the start (^) non-greedily (.*?)\n",
    "# (\\s\\d+)     -> The split transition: Look for a space (\\s) followed immediately by a digit (\\d+).\n",
    "#               This finds the first number after the name.\n",
    "#               Crucially, we DON'T include this space-plus-digit in Group 1.\n",
    "# (.*$)       -> Capturing Group 2: Match and capture everything from that point until the end ($).\n",
    "regex_pattern = r\"^(.*?)\\s(\\d+.*$)\"\n",
    "\n",
    "# Extract the two parts into new columns:\n",
    "df[['Rk_Name_Combined', 'Stats_Combined']] = df[0].str.extract(regex_pattern, expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2fb5320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove the first row\n",
    "df = df.iloc[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d082d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum spaces (expected separators): 23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NEW LOGIC: VERIFY AND DROP NON-CONFORMING STATS ROWS ---\n",
    "\n",
    "# 1. Calculate the number of spaces (separators) in each row\n",
    "space_counts = df['Stats_Combined'].str.count(' ')\n",
    "\n",
    "# 2. Find the maximum number of spaces, which defines the expected structure\n",
    "max_spaces = space_counts.max()\n",
    "\n",
    "print(f\"Maximum spaces (expected separators): {max_spaces}\")\n",
    "\n",
    "# 1. Calculate the number of spaces (separators) in each row\n",
    "space_counts = df['Stats_Combined'].str.count(' ')\n",
    "\n",
    "# 2. Find the maximum number of spaces, which defines the expected structure\n",
    "max_spaces = space_counts.max()\n",
    "\n",
    "print(f\"Maximum spaces (expected separators): {max_spaces}\")\n",
    "\n",
    "\n",
    "# 3. Identify the indices of the rows that do NOT have the maximum number of spaces\n",
    "indices_to_drop = space_counts[space_counts < max_spaces].index\n",
    "\n",
    "# 4. Check if any rows need to be dropped\n",
    "if indices_to_drop.empty:\n",
    "    print(\"All rows have a consistent number of spaces. Proceeding with split.\")\n",
    "else:\n",
    "    num_dropped = len(indices_to_drop)\n",
    "    \n",
    "    # 5. Drop the rows that do not match the expected structure\n",
    "    df = df.drop(index=indices_to_drop)\n",
    "    \n",
    "    # Reset the index after dropping rows\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Removed {num_dropped} row(s) because they did not have the expected number of spaces ({max_spaces}).\")\n",
    "    \n",
    "# --- END OF NEW LOGIC ---\n",
    "\n",
    "# Now the 'Stats_Combined' column in 'df' only contains rows that are guaranteed\n",
    "# to have the correct number of data points for the final split.\n",
    "\n",
    "# Continue with the rest of your splitting process:\n",
    "# stats_split = df['Stats_Combined'].str.split(expand=True)\n",
    "# ... and the rest of your cleanup/concat logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fec282d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Split the 'Stats_Combined' column on all spaces\n",
    "# The Name is now safe, and the stats are simple space-separated values.\n",
    "stats_split = df['Stats_Combined'].str.split(expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168eefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Combine the Name/Rank column with the split statistics\n",
    "# We use iloc to get the columns we want.\n",
    "final_df = pd.concat([\n",
    "    df['Rk_Name_Combined'], # The Name/Rank column\n",
    "    stats_split             # All the statistical columns\n",
    "], axis=1)\n",
    "\n",
    "# Remove the first row\n",
    "final_df = final_df.iloc[1:]\n",
    "\n",
    "# --- Final Cleanup (Optional, but recommended) ---\n",
    "# Separate the Rk from the Name in the first column for a cleaner table.\n",
    "final_df[['Rk', 'Name']] = final_df['Rk_Name_Combined'].str.split(' ', n=1, expand=True)\n",
    "final_df = final_df.drop(columns=['Rk_Name_Combined', 'Rk'])\n",
    "\n",
    "# Move the last column to be the first column (Name)\n",
    "final_df = final_df[['Name'] + [col for col in final_df.columns if col != 'Name']]\n",
    "\n",
    "# 5. Apply the headers\n",
    "final_df.columns = all_headers\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
