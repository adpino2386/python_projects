{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b6a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pybaseball as pyb\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f549c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Build the PostgreSQL connection string\n",
    "DB_URL = f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASS']}@{os.environ['DB_HOST']}:5432/{os.environ['DB_NAME']}\"\n",
    "\n",
    "# Create the engine object for connecting\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "print(\"Database connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b060cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample function; your final ETL will be more complex.\n",
    "def extract_batting_data(year=2025):\n",
    "    # fgs is FanGraphs Season Stats\n",
    "    df = pyb.batting_stats(year)\n",
    "    # Rename columns to match your SQL schema (e.g., 'BB' for Walks, 'K' for Strikeouts)\n",
    "    df = df.rename(columns={'ID': 'player_id', 'Tm': 'team_id'})\n",
    "    return df\n",
    "\n",
    "current_year_batting = extract_batting_data(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fced1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Pulling FanGraphs data for 2004...\n",
      "-> Pulling FanGraphs data for 2005...\n",
      "-> Pulling FanGraphs data for 2006...\n",
      "-> Pulling FanGraphs data for 2007...\n",
      "-> Pulling FanGraphs data for 2008...\n",
      "-> Pulling FanGraphs data for 2009...\n",
      "-> Pulling FanGraphs data for 2010...\n",
      "-> Pulling FanGraphs data for 2011...\n",
      "-> Pulling FanGraphs data for 2012...\n",
      "-> Pulling FanGraphs data for 2013...\n",
      "-> Pulling FanGraphs data for 2014...\n",
      "-> Pulling FanGraphs data for 2015...\n",
      "-> Pulling FanGraphs data for 2016...\n",
      "-> Pulling FanGraphs data for 2017...\n",
      "-> Pulling FanGraphs data for 2018...\n",
      "-> Pulling FanGraphs data for 2019...\n",
      "-> Pulling FanGraphs data for 2020...\n",
      "-> Pulling FanGraphs data for 2021...\n",
      "-> Pulling FanGraphs data for 2022...\n",
      "-> Pulling FanGraphs data for 2023...\n",
      "-> Pulling FanGraphs data for 2024...\n",
      "-> Pulling FanGraphs data for 2025...\n",
      "✅ Successfully combined data from 22 seasons into 3184 total rows.\n"
     ]
    }
   ],
   "source": [
    "def extract_historical_batting(start_year=2004, end_year=2025):\n",
    "    \"\"\"Pulls batting stats for a range of years from FanGraphs and combines them.\"\"\"\n",
    "    \n",
    "    all_data_frames = []\n",
    "    \n",
    "    # Create the list of years to pull\n",
    "    years = range(start_year, end_year + 1)\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"-> Pulling FanGraphs data for {year}...\")\n",
    "        try:\n",
    "            # 1. Extraction: Pull data for a single year\n",
    "            df = pyb.batting_stats(year)\n",
    "            \n",
    "            # CRITICAL: Add a 'Season' column for historical tracking\n",
    "            df['Season'] = year \n",
    "            \n",
    "            # 2. Transformation (Initial Rename/Select)\n",
    "            # You must select and rename columns here to match your PostgreSQL schema\n",
    "            \n",
    "            # Example: Select and rename columns (Adjust this based on your exact schema!)\n",
    "            df = df.rename(columns={'Name': 'player_name', 'PlayerId': 'player_id', \n",
    "                                    'G': 'games_played', 'HR': 'home_runs', 'SO': 'strikeouts'})\n",
    "            \n",
    "            # 3. Append: Add the processed year's data to the list\n",
    "            all_data_frames.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to pull data for {year}: {e}\")\n",
    "            \n",
    "    # 4. Concatenate: Merge all DataFrames into one large DataFrame\n",
    "    if all_data_frames:\n",
    "        historical_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        print(f\"✅ Successfully combined data from {len(years)} seasons into {len(historical_df)} total rows.\")\n",
    "        return historical_df\n",
    "    \n",
    "    return pd.DataFrame() # Return empty if no data was pulled\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    full_batting_df = extract_historical_batting()\n",
    "    \n",
    "    # ... (Then proceed to your advanced metrics calculation and load_data function) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43dc54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Pulling Statcast data from 2025-10-28 to 2025-10-30...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:01<00:02,  1.17s/it]c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\datahelpers\\postprocessing.py:59: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  data_copy[column] = data_copy[column].apply(pd.to_datetime, errors='ignore', format=date_format)\n",
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.22it/s]c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\datahelpers\\postprocessing.py:59: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  data_copy[column] = data_copy[column].apply(pd.to_datetime, errors='ignore', format=date_format)\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 581 individual pitches/events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    }
   ],
   "source": [
    "import pybaseball as pyb\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def extract_statcast_data(start_date, end_date):\n",
    "    \"\"\"Pulls granular, pitch-by-pitch data for a specified date range.\"\"\"\n",
    "    print(f\"-> Pulling Statcast data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # pybaseball statcast function is designed to handle this extraction\n",
    "    raw_statcast_df = pyb.statcast(start_dt=start_date, end_dt=end_date)\n",
    "    \n",
    "    if raw_statcast_df is None or raw_statcast_df.empty:\n",
    "        print(\"Warning: No Statcast data returned for this date range.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return raw_statcast_df\n",
    "\n",
    "\n",
    "# Example\n",
    "test_start_date = '2025-10-28'\n",
    "test_end_date = '2025-10-30' \n",
    "\n",
    "daily_data = extract_statcast_data(test_start_date, test_end_date)\n",
    "print(f\"Successfully extracted {len(daily_data)} individual pitches/events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dbdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Statcast ETL: Pulling data from 2024-11-03 to 2025-12-07\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259/259 [00:11<00:00, 21.78it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    }
   ],
   "source": [
    "import pybaseball as pyb\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy.engine import Engine\n",
    "import pybaseball.cache\n",
    "\n",
    "# Enable pybaseball caching to speed up repeated queries\n",
    "pybaseball.cache.enable()\n",
    "\n",
    "# Set the connection engine using your environment variables (as previously defined)\n",
    "# engine = create_engine(DB_URL) \n",
    "\n",
    "def update_statcast_data(engine: Engine, days_to_keep: int = 400):\n",
    "    \"\"\"\n",
    "    Pulls recent Statcast data and keeps a rolling window of data \n",
    "    in the PostgreSQL statcast_pitches table.\n",
    "    \"\"\"\n",
    "    today = date.today()\n",
    "    \n",
    "    # We pull data from the end of last season (or roughly 400 days ago) \n",
    "    # up to yesterday to ensure we have a full window for rolling metrics.\n",
    "    start_date = today - timedelta(days=days_to_keep) \n",
    "    end_date = today - timedelta(days=1)\n",
    "    \n",
    "    start_dt_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_dt_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Starting Statcast ETL: Pulling data from {start_dt_str} to {end_dt_str}\")\n",
    "    \n",
    "    try:\n",
    "        # Use the general statcast function for league-wide pitch data\n",
    "        # 'statcast' is an alias for the league-wide Statcast search\n",
    "        df = pyb.statcast(start_dt=start_dt_str, end_dt=end_dt_str)\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"No new Statcast data retrieved. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # 1. Cleaning/Selection (CRITICAL)\n",
    "        # Select ONLY the columns you need to prevent errors during loading.\n",
    "        # Statcast column names are long; we'll rename them to match the SQL table.\n",
    "        df_clean = df.rename(columns={\n",
    "            'game_date': 'game_date', \n",
    "            'game_pk': 'game_pk', \n",
    "            'inning': 'inning', \n",
    "            'batter': 'batter_id', \n",
    "            'pitcher': 'pitcher_id', \n",
    "            'stand': 'stand', \n",
    "            'p_throws': 'p_throws',\n",
    "            'events': 'events',\n",
    "            'description': 'description',\n",
    "            'launch_speed': 'launch_speed', \n",
    "            'launch_angle': 'launch_angle',\n",
    "            'bb_type': 'bb_type',\n",
    "            'pitch_type': 'pitch_type',\n",
    "            'release_speed': 'release_speed',\n",
    "            'spin_rate': 'spin_rate'\n",
    "        })\n",
    "\n",
    "        # Only keep the columns that exist in our SQL schema\n",
    "        columns_to_keep = [col for col in df_clean.columns if col in ['game_date', 'game_pk', 'inning', 'batter_id', 'pitcher_id', 'stand', 'p_throws', 'events', 'description', 'launch_speed', 'launch_angle', 'bb_type', 'pitch_type', 'release_speed', 'spin_rate']]\n",
    "        final_df = df_clean[columns_to_keep]\n",
    "\n",
    "        # # 2. Loading: Use 'replace' for the first run, then switch to 'append' \n",
    "        # # for a daily ETL to avoid re-downloading old data.\n",
    "        # # Since we are pulling a large historical range, we should replace the table.\n",
    "        final_df.to_sql('statcast_pitches', engine, if_exists='append', index=False, chunksize=5000)\n",
    "        \n",
    "        # print(f\"✅ Successfully loaded {len(final_df)} rows of Statcast data into 'statcast_pitches'.\")\n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Statcast ETL Failed: {e}\")\n",
    "\n",
    "# Example Run (assuming 'engine' is defined with your credentials)\n",
    "statcast_data_df = update_statcast_data(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d67664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybaseball as pyb\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy import text \n",
    "import pybaseball.cache # Ensure caching is imported\n",
    "\n",
    "pybaseball.cache.enable() # Enable caching for reliability\n",
    "\n",
    "def update_statcast_data(engine: Engine):\n",
    "    \"\"\"\n",
    "    Pulls Statcast data starting from the day AFTER the last record in the database\n",
    "    to ensure only new events are downloaded and appended.\n",
    "    \"\"\"\n",
    "    \n",
    "    today = date.today()\n",
    "    \n",
    "    # --- STEP 1: FIND LAST DATE IN DB ---\n",
    "    try:\n",
    "        # Query the database to find the latest game_date currently stored\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(\n",
    "                text(\"SELECT MAX(game_date) FROM statcast_pitches;\")\n",
    "            ).scalar()\n",
    "        \n",
    "        # If the table is empty, start from 400 days ago (initial load range)\n",
    "        if result is None:\n",
    "            print(\"Database is empty. Starting full initial load (400 days)...\")\n",
    "            last_date = today - timedelta(days=400)\n",
    "        else:\n",
    "            # Start the new pull from the day AFTER the last record\n",
    "            last_date = result.date()\n",
    "            print(f\"Latest game_date found in DB: {last_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR querying database for last date: {e}. Defaulting to last 5 days.\")\n",
    "        last_date = today - timedelta(days=5)\n",
    "\n",
    "    \n",
    "    # --- STEP 2: DEFINE NEW EXTRACTION RANGE ---\n",
    "    start_date = last_date + timedelta(days=1)\n",
    "    end_date = today - timedelta(days=1) # Pull up to yesterday, as today's games aren't finished\n",
    "\n",
    "    start_dt_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_dt_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    if start_date >= end_date:\n",
    "        print(f\"Data is up to date as of {end_dt_str}. No new extraction needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting DAILY Statcast ETL: Pulling data from {start_dt_str} to {end_dt_str}\")\n",
    "    \n",
    "    # --- STEP 3: EXTRACTION ---\n",
    "    try:\n",
    "        df = pyb.statcast(start_dt=start_dt_str, end_dt=end_dt_str)\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            print(\"No new Statcast data retrieved for this date range. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # --- STEP 4: CLEANING AND RENAMING ---\n",
    "        \n",
    "        # Rename columns to match the PostgreSQL 'statcast_pitches' table schema\n",
    "        df_clean = df.rename(columns={\n",
    "            'game_date': 'game_date', \n",
    "            'game_pk': 'game_pk', \n",
    "            'inning': 'inning', \n",
    "            'batter': 'batter_id', \n",
    "            'pitcher': 'pitcher_id', \n",
    "            'stand': 'stand', \n",
    "            'p_throws': 'p_throws',\n",
    "            'events': 'events',\n",
    "            'description': 'description',\n",
    "            'launch_speed': 'launch_speed', \n",
    "            'launch_angle': 'launch_angle',\n",
    "            'bb_type': 'bb_type',\n",
    "            'pitch_type': 'pitch_type',\n",
    "            'release_speed': 'release_speed',\n",
    "            'spin_rate': 'spin_rate'\n",
    "        })\n",
    "\n",
    "        # Only keep the columns that exist in our SQL schema\n",
    "        columns_to_keep = [\n",
    "            'game_date', 'game_pk', 'inning', 'batter_id', 'pitcher_id', 'stand', 'p_throws', \n",
    "            'events', 'description', 'launch_speed', 'launch_angle', 'bb_type', 'pitch_type', \n",
    "            'release_speed', 'spin_rate'\n",
    "        ]\n",
    "        \n",
    "        # Filter the DataFrame to include only the necessary columns\n",
    "        final_df = df_clean[columns_to_keep].copy()\n",
    "        \n",
    "        # Handle data types before loading (optional, but good practice)\n",
    "        final_df['game_date'] = pd.to_datetime(final_df['game_date'])\n",
    "\n",
    "        # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(final_df)} new rows into 'statcast_pitches'...\")\n",
    "        \n",
    "        final_df.to_sql(\n",
    "            'statcast_pitches', \n",
    "            engine, \n",
    "            if_exists='append', # CRITICAL: Append new data to the existing table\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully appended {len(final_df)} new rows of Statcast data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Statcast ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc9530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest game_date found in DB: 2025-11-01\n",
      "Starting DAILY Statcast ETL: Pulling data from 2025-11-02 to 2025-12-07\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new Statcast data retrieved for this date range. Exiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "update_statcast_data(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
