{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01078c0e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b6a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pybaseball as pyb\n",
    "import pybaseball.cache # Ensure caching is imported\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy import text\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "from rapidfuzz import process\n",
    "import re\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import pylahman\n",
    "import statsapi\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891da24a",
   "metadata": {},
   "source": [
    "### Load environment and connect to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f549c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Build the PostgreSQL connection string\n",
    "DB_URL = f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASS']}@{os.environ['DB_HOST']}:5432/{os.environ['DB_NAME']}\"\n",
    "\n",
    "# Create the engine object for connecting\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "print(\"Database connection established.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036bfa2",
   "metadata": {},
   "source": [
    "### Create dim_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_players(engine: Engine):   \n",
    "    try:\n",
    "        players_lahman = pylahman.People()\n",
    "        player_chadwick = pyb.chadwick_register()\n",
    "\n",
    "        # Join lahman and chadwick on key identifiers and bring all the columns from lahman\n",
    "        # Ignore if key_bbref is empty in chadwick\n",
    "        players_chadwick_clean = player_chadwick[player_chadwick['key_retro'].notna()]\n",
    "        players_lahman_clean   = players_lahman[players_lahman['retroID'].notna()]\n",
    "\n",
    "        players_df = pd.merge(\n",
    "            players_chadwick_clean,\n",
    "            players_lahman_clean,\n",
    "            left_on=['key_retro'],\n",
    "            right_on=['retroID'],\n",
    "            how='left',\n",
    "        )\n",
    "\n",
    "        # Remove unnecesary columns and drop them from the dataframe\n",
    "        cols_to_remove = ['retroID', 'bbrefID', 'mlb_played_first', 'mlb_played_last']\n",
    "        players_df = players_df.drop(columns= cols_to_remove)\n",
    "\n",
    "        # Rename the fields\n",
    "        rename_map = {\n",
    "            # IDs\n",
    "            \"key_mlbam\":     \"key_mlbam\",\n",
    "            \"key_retro\":     \"key_retro\",\n",
    "            \"key_bbref\":     \"key_bbref\",\n",
    "            \"key_fangraphs\": \"key_fangraphs\",\n",
    "            \"ID\":            \"id_lahman\",\n",
    "            \"playerID\":      \"player_id_lahman\",\n",
    "\n",
    "            # Names\n",
    "            \"name_last\":     \"last_name_chadwick\",\n",
    "            \"name_first\":    \"first_name_chadwick\",\n",
    "            \"nameLast\":      \"last_name_lahman\",\n",
    "            \"nameFirst\":     \"first_name_lahman\",\n",
    "            \"nameGiven\":     \"first_and_second_name_lahman\",\n",
    "\n",
    "            # Debut/Final game\n",
    "            \"debut\":         \"debut\",\n",
    "            \"finalGame\":     \"final_game\",\n",
    "\n",
    "            # Info\n",
    "            \"weight\":        \"weight\",\n",
    "            \"height\":        \"height\",\n",
    "            \"bats\":          \"bats\",\n",
    "            \"throws\":        \"throws\",\n",
    "\n",
    "            # Birth/Death\n",
    "            \"birthYear\":     \"birth_year\",\n",
    "            \"birthMonth\":    \"birth_month\",\n",
    "            \"birthDay\":      \"birth_day\",\n",
    "            \"birthCity\":     \"birth_city\",\n",
    "            \"birthCountry\":  \"birth_country\",\n",
    "            \"birthState\":    \"birth_state\",\n",
    "            \"deathYear\":     \"death_year\",\n",
    "            \"deathMonth\":    \"death_month\",\n",
    "            \"deathDay\":      \"death_day\",\n",
    "            \"deathCountry\":  \"death_country\",\n",
    "            \"deathState\":    \"death_state\",\n",
    "            \"deathCity\":     \"death_city\",\n",
    "        }\n",
    "\n",
    "        # Apply the rename\n",
    "        players_df = players_df.rename(columns= rename_map)\n",
    "\n",
    "        # Order the new columns\n",
    "        ordered_cols = [\n",
    "            \"key_mlbam\",\n",
    "            \"key_retro\",\n",
    "            \"key_bbref\",\n",
    "            \"key_fangraphs\",\n",
    "            \"id_lahman\",\n",
    "            \"player_id_lahman\",\n",
    "            \"last_name_chadwick\",\n",
    "            \"first_name_chadwick\",\n",
    "            \"last_name_lahman\",\n",
    "            \"first_name_lahman\",\n",
    "            \"first_and_second_name_lahman\",\n",
    "            \"debut\",\n",
    "            \"final_game\",\n",
    "            \"weight\",\n",
    "            \"height\",\n",
    "            \"bats\",\n",
    "            \"throws\",\n",
    "            \"birth_year\",\n",
    "            \"birth_month\",\n",
    "            \"birth_day\",\n",
    "            \"birth_city\",\n",
    "            \"birth_country\",\n",
    "            \"birth_state\",\n",
    "            \"death_year\",\n",
    "            \"death_month\",\n",
    "            \"death_day\",\n",
    "            \"death_country\",\n",
    "            \"death_state\",\n",
    "            \"death_city\"\n",
    "        ]\n",
    "\n",
    "        # Apply the order\n",
    "        players_df = players_df[ordered_cols]\n",
    "\n",
    "        # This selects only columns with numbers and fills their nulls with -1\n",
    "        numeric_cols = players_df.select_dtypes(include=['number']).columns\n",
    "        players_df[numeric_cols] = players_df[numeric_cols].fillna(-1)\n",
    "\n",
    "        # Replace nulls in the text columns\n",
    "        text_cols = [\n",
    "            \"key_retro\",\n",
    "            \"key_bbref\",\n",
    "            \"player_id_lahman\",\n",
    "            \"last_name_chadwick\",\n",
    "            \"first_name_chadwick\",\n",
    "            \"last_name_lahman\",\n",
    "            \"first_name_lahman\",\n",
    "            \"first_and_second_name_lahman\",\n",
    "            \"bats\",\n",
    "            \"throws\",\n",
    "            \"birth_city\",\n",
    "            \"birth_country\",\n",
    "            \"birth_state\",\n",
    "            \"death_country\",\n",
    "            \"death_state\",\n",
    "            \"death_city\"\n",
    "        ]\n",
    "\n",
    "        # Convert to a standard object type first and then fill the nulls with N/A\n",
    "        for col in text_cols:\n",
    "            players_df[col] = players_df[col].astype(object).fillna('N/A')\n",
    "            \n",
    "\n",
    "        # List the date columns\n",
    "        date_cols = [\n",
    "            \"debut\",\n",
    "            \"final_game\"\n",
    "        ]\n",
    "        # Fill null dates with January 1st, 1700\n",
    "        for col in date_cols:\n",
    "            players_df[col] = players_df[col].fillna(pd.Timestamp('1700-01-01'))\n",
    "\n",
    "        # Check for nulls in my table - there shouldn't be any\n",
    "        if (players_df.isnull().sum() == 0).all():\n",
    "            print(\"‚úÖ No nulls found.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è WARNING - There are nulls in some columns in the dataframe.\")\n",
    "\n",
    "        # # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(players_df)} new rows into 'players'...\")\n",
    "        \n",
    "        players_df.to_sql(\n",
    "            'players', \n",
    "            engine, \n",
    "            if_exists='replace',\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Players successfully added {len(players_df)} new rows of players data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n",
    "# Execute the players function\n",
    "update_players(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1195f56",
   "metadata": {},
   "source": [
    "### Create dim_franchises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_team_franchises(engine: Engine):\n",
    "    try:\n",
    "        # Import the franchises\n",
    "        #? Note: As of 2025-12-18 there is only data up to the 2024 season\n",
    "        team_franchises = pylahman.TeamsFranchises()\n",
    "        \n",
    "        # Data cleaning\n",
    "        # Identify all text columns\n",
    "        text_cols = team_franchises.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "        # Convert to object first, then fill (since the columns are literal strings)\n",
    "        for col in text_cols:\n",
    "            # Converting to object allows 'N/A' to be treated as a normal string\n",
    "            team_franchises[col] = team_franchises[col].astype(object).fillna('N/A')\n",
    "            \n",
    "            # Just in case some were literal 'nan' strings:\n",
    "            team_franchises[col] = team_franchises[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "        # Final verification\n",
    "        null_count = team_franchises[text_cols].isnull().sum().sum()\n",
    "        if null_count == 0:\n",
    "            print(\"‚úÖ All string columns are clean. No nulls found!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: {null_count} nulls still remain in text columns.\")\n",
    "            \n",
    "        \n",
    "        # Loading\n",
    "        print(f\"Loading {len(team_franchises)} new rows into 'team_franchises'...\")\n",
    "        \n",
    "        team_franchises.to_sql(\n",
    "            'team_franchises', \n",
    "            engine, \n",
    "            if_exists='replace',\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Team franchises successfully added {len(team_franchises)} new rows of data.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "        \n",
    "# Apply the function\n",
    "update_team_franchises(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b52b44",
   "metadata": {},
   "source": [
    "### Teams info ***NOT IN USE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe940a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_team_info(engine: Engine):\n",
    "#     try:\n",
    "#         team_info = pylahman.Teams()\n",
    "\n",
    "#         # Identify all text columns\n",
    "#         text_cols = team_info.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "#         # Convert to object first, then fill with N/A\n",
    "#         for col in text_cols:\n",
    "#             # Converting to object allows 'N/A' to be treated as a normal string\n",
    "#             team_info[col] = team_info[col].astype(object).fillna('N/A')\n",
    "            \n",
    "#             # Just in case some were literal 'nan' strings:\n",
    "#             team_info[col] = team_info[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "#         # This selects only columns with numbers and fills their nulls with -1\n",
    "#         numeric_cols = team_info.select_dtypes(include=['number']).columns\n",
    "#         team_info[numeric_cols] = team_info[numeric_cols].fillna(-1)\n",
    "\n",
    "#         # Final verification\n",
    "#         null_count_text    = team_info[text_cols].isnull().sum().sum()\n",
    "#         null_count_numeric = team_info[numeric_cols].isnull().sum().sum()\n",
    "#         total_nulls        = null_count_text + null_count_numeric\n",
    "\n",
    "#         if total_nulls == 0:\n",
    "#             print(\"‚úÖ All columns are clean. No nulls found!\")\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è Warning: {total_nulls} nulls still remain some columns.\")\n",
    "\n",
    "#         # Loading\n",
    "#         print(f\"Loading {len(team_info)} new rows into 'team_info'...\")\n",
    "        \n",
    "#         team_info.to_sql(\n",
    "#             'team_info', \n",
    "#             engine, \n",
    "#             if_exists='replace',\n",
    "#             index=False, \n",
    "#             chunksize=5000\n",
    "#         )\n",
    "        \n",
    "#         print(f\"‚úÖ Team information successfully added {len(team_info)} new rows of data.\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "\n",
    "# # Apply the function\n",
    "# update_team_info(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6532c",
   "metadata": {},
   "source": [
    "### Create fact_team_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 330 new rows into 'team_info'...\n",
      "‚úÖ Team information successfully added 330 new rows of data.\n",
      "Loading 330 new rows into 'team_info'...\n",
      "‚úÖ Team information successfully added 330 new rows of data.\n",
      "Loading 330 new rows into 'team_info'...\n",
      "‚úÖ Team information successfully added 330 new rows of data.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_team_tables(engine: Engine):    \n",
    "    def load_fact_team_tables(engine: Engine, df, category):\n",
    "        try:\n",
    "            table_name = 'fact_team_' + category\n",
    "            print(f\"üíæ Creating {table_name}...\")\n",
    "            \n",
    "            # Loading\n",
    "            print(f\"   üîÉ Loading {len(df)} rows...\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace',\n",
    "                index=False, \n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully added {len(df)} new rows of data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "    # Declare the years\n",
    "    current_year  = date.today().year\n",
    "    ten_years_ago = current_year - 10\n",
    "\n",
    "    # Import the team data for the last 10 years\n",
    "    fact_team_batting  = pyb.team_batting(ten_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_team_pitching = pyb.team_pitching(ten_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_team_fielding = pyb.team_fielding(ten_years_ago, current_year,  ind= 1, qual= 0)\n",
    "\n",
    "    # Apply the function\n",
    "    load_fact_team_tables(engine, fact_team_batting,  'batting')\n",
    "    load_fact_team_tables(engine, fact_team_pitching, 'pitching')\n",
    "    load_fact_team_tables(engine, fact_team_fielding, 'fielding')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f0b3a",
   "metadata": {},
   "source": [
    "### Create fact_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "      ‚¨áÔ∏è  IMPORTING BASEBALL STATS      \n",
      "             Please wait...             \n",
      "========================================\n",
      "\n",
      "üíæ Creating fact_player_batting...\n",
      "   üîÉ Loading 8673 rows...\n",
      "   ‚úÖ Successfully added 8673 new rows of data.\n",
      "üíæ Creating fact_player_pitching...\n",
      "   üîÉ Loading 5106 rows...\n",
      "   ‚úÖ Successfully added 5106 new rows of data.\n",
      "üíæ Creating fact_player_fielding...\n",
      "   üîÉ Loading 13553 rows...\n",
      "   ‚úÖ Successfully added 13553 new rows of data.\n",
      "üíæ Creating fact_player_running...\n",
      "   üîÉ Loading 3830 rows...\n",
      "   ‚úÖ Successfully added 3830 new rows of data.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_player_tables(engine: Engine):    \n",
    "    def load_fact_player_tables(engine: Engine, df, category):\n",
    "        try:\n",
    "            table_name = 'fact_player_' + category\n",
    "            print(f\"üíæ Creating {table_name}...\")\n",
    "            \n",
    "            # Loading\n",
    "            print(f\"   üîÉ Loading {len(df)} rows...\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace',\n",
    "                index=False, \n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully added {len(df)} new rows of data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "    # Declare the years\n",
    "    current_year   = date.today().year\n",
    "    five_years_ago = current_year - 5\n",
    "\n",
    "    # Import the team data for the last 10 years\n",
    "    # print(\"\\n\" + \"=\"*40)\n",
    "    # print(f\"{'‚¨áÔ∏è  Importing player stats':^40}\")\n",
    "    # print(f\"{'Please wait...':^40}\")\n",
    "    # print(\"=\"*40 + \"\\n\")\n",
    "    print(\"‚¨áÔ∏è  Importing player stats... please wait\")\n",
    "    \n",
    "    fact_player_batting  = pyb.batting_stats(five_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_player_pitching = pyb.pitching_stats(five_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_player_fielding = pyb.fielding_stats(five_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    \n",
    "    # Speed tables are by year - they do not include range\n",
    "    # Setup year range\n",
    "    #current_year = datetime.now().year\n",
    "    years = range(current_year - 9, current_year + 1) # Last 10 years including current\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for year in years:\n",
    "        #print(f\"Fetching sprint speed for {year}...\")\n",
    "        try:\n",
    "            # Fetch data\n",
    "            df = pyb.statcast_sprint_speed(year, 50)\n",
    "            \n",
    "            # Adding the years\n",
    "            df['Season'] = year\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch data for {year}: {e}\")\n",
    "\n",
    "    # Combine everything into one fact table\n",
    "    fact_player_running = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Apply the function\n",
    "    load_fact_player_tables(engine, fact_player_batting,  'batting')\n",
    "    load_fact_player_tables(engine, fact_player_pitching, 'pitching')\n",
    "    load_fact_player_tables(engine, fact_player_fielding, 'fielding')\n",
    "    load_fact_player_tables(engine, fact_player_running, 'running')\n",
    "    \n",
    "create_fact_player_tables(engine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical player stats - It has data from 1871 but it doesn't have last year (2025)\n",
    "player_batting_historical     = pylahman.Batting()\n",
    "player_pitching_historical    = pylahman.Pitching()\n",
    "player_fielding_historical    = pylahman.Fielding()\n",
    "player_appearances_historical = pylahman.Appearances()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c652b",
   "metadata": {},
   "source": [
    "### Get scores last n days *NOT IN USE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for all games played from 2025-12-17 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-17 and 2025-12-17.\n",
      "Searching for all games played from 2025-12-11 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-11 and 2025-12-17.\n",
      "Searching for all games played from 2025-12-03 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-03 and 2025-12-17.\n",
      "Searching for all games played from 2025-11-18 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-11-18 and 2025-12-17.\n",
      "Searching for all games played from 2025-10-19 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:03<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 2970 pitch events.\n",
      "Searching for all games played from 2025-09-19 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:04<00:00, 14.14it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 53951 pitch events.\n"
     ]
    }
   ],
   "source": [
    "# def get_game_results_last_n_days(n_days=90):\n",
    "#     \"\"\"\n",
    "#     Pulls raw pitch-by-pitch data for all games played in the last 'n_days' \n",
    "#     and then extracts the final score for each game.\n",
    "#     \"\"\"\n",
    "#     today = date.today()\n",
    "    \n",
    "#     # 1. Calculate the start and end dates for the 90-day range\n",
    "#     end_date = today - timedelta(days=1)  # Search up to yesterday\n",
    "#     start_date = today - timedelta(days=n_days)\n",
    "    \n",
    "#     start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "#     end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     print(f\"Searching for all games played from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "#     try:\n",
    "#         # 2. Pull all pitch-by-pitch data in that range\n",
    "#         all_data_in_range = pyb.statcast(start_dt=start_date_str, end_dt=end_date_str)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error retrieving Statcast data: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     if all_data_in_range.empty:\n",
    "#         print(f\"No games found between {start_date_str} and {end_date_str}.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     print(f\"Successfully pulled {len(all_data_in_range)} pitch events.\")\n",
    "\n",
    "#     # 3. Sort the data chronologically by game_pk, inning, etc.\n",
    "#     data_sorted = all_data_in_range.sort_values(\n",
    "#         by=['game_pk', 'inning', 'inning_topbot', 'at_bat_number', 'pitch_number'],\n",
    "#         ascending=True\n",
    "#     )\n",
    "\n",
    "#     # 4. Group by game_pk and take the last row (which contains the final score)\n",
    "#     final_events = data_sorted.groupby('game_pk').tail(1).reset_index(drop=True)\n",
    "    \n",
    "#     # 5. Extract and rename the relevant columns for the final scoreboard\n",
    "#     scoreboard = final_events[[\n",
    "#         'game_date', \n",
    "#         'home_team', \n",
    "#         'away_team', \n",
    "#         'home_score', \n",
    "#         'away_score'\n",
    "#     ]].copy()\n",
    "    \n",
    "#     scoreboard.rename(columns={\n",
    "#         'home_score': 'Home_Final_Score',\n",
    "#         'away_score': 'Away_Final_Score',\n",
    "#         'game_date': 'Date'\n",
    "#     }, inplace=True)\n",
    "    \n",
    "#     # 6. Determine the Winner\n",
    "#     scoreboard['Winner'] = scoreboard.apply(\n",
    "#         lambda row: row['home_team'] if row['Home_Final_Score'] > row['Away_Final_Score'] else row['away_team'],\n",
    "#         axis=1\n",
    "#     )\n",
    "#     scoreboard['Result'] = (\n",
    "#         scoreboard['Winner'] + ' wins ' + \n",
    "#         scoreboard['Home_Final_Score'].astype(str) + '-' + \n",
    "#         scoreboard['Away_Final_Score'].astype(str)\n",
    "#     )\n",
    "    \n",
    "#     return scoreboard[['Date', 'away_team', 'home_team', 'Away_Final_Score', 'Home_Final_Score', 'Winner', 'Result']]\n",
    "\n",
    "# # --- EXECUTION ---\n",
    "# results_yesterday_df    = get_game_results_last_n_days(n_days= 1)\n",
    "# results_last_7_days_df  = get_game_results_last_n_days(n_days= 7)\n",
    "# results_last_15_days_df = get_game_results_last_n_days(n_days= 15)\n",
    "# results_last_30_days_df = get_game_results_last_n_days(n_days= 30)\n",
    "# results_last_60_days_df = get_game_results_last_n_days(n_days= 60)\n",
    "# results_last_90_days_df = get_game_results_last_n_days(n_days= 90)\n",
    "\n",
    "\n",
    "# # if not results_last_90_days_df.empty:\n",
    "# #     print(f\"\\n--- Game Results from the Last 90 Days ({len(results_last_90_days_df)} Games Found) ---\")\n",
    "# #     print(results_last_90_days_df.tail(10)) # Print the last 10 games found\n",
    "# # else:\n",
    "# #     print(\"\\nNo games were found in the last 90 days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17fde4",
   "metadata": {},
   "source": [
    "### Create fact_statcast_pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab62df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for all games played from 2025-09-19 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:04<00:00, 13.99it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 53951 pitch events.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_statcast_events_pitch_by_pitch(engine: Engine, n_days= 90):\n",
    "    \"\"\"\n",
    "    Pulls raw pitch-by-pitch data for all games played in the last 'n_days' \n",
    "    and then extracts the final score for each game.\n",
    "    \"\"\"\n",
    "    def load_fact_statcast_events(engine: Engine, df):\n",
    "        try:\n",
    "            table_name = 'fact_statcast_pitches'\n",
    "            print(f\"üíæ Creating {table_name}...\")\n",
    "            \n",
    "            # Loading\n",
    "            print(f\"   üîÉ Loading {len(df)} rows...\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace',\n",
    "                index=False, \n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully added {len(df)} new rows of data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "    \n",
    "    # Get today's date\n",
    "    today = date.today()\n",
    "\n",
    "    # Calculate the start and end dates for the n-day range\n",
    "    end_date = today - timedelta(days= 1)  # Search up to yesterday\n",
    "    start_date = today - timedelta(days= n_days)\n",
    "\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Searching for all games played from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "    try:\n",
    "        # Pull all pitch-by-pitch data in that range\n",
    "        fact_statcast_pitches_last_n_days = pyb.statcast(start_dt= start_date_str, end_dt= end_date_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving Statcast data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if fact_statcast_pitches_last_n_days.empty:\n",
    "        print(f\"No games found between {start_date_str} and {end_date_str}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Successfully pulled {len(fact_statcast_pitches_last_n_days)} pitch events for the last {n_days} days.\")\n",
    "\n",
    "    # def filter_days(df, days):\n",
    "    #     cutoff = today - timedelta(days=days)\n",
    "    #     # Convert 'Date' column to datetime objects if they aren't already\n",
    "    #     df['game_date'] = pd.to_datetime(df['game_date']).dt.date\n",
    "    #     return df[df['game_date'] >= cutoff]\n",
    "\n",
    "    # # Sub-df from the main one\n",
    "    # results_1_day_df   = filter_days(fact_statcast_pitches_last_90_days, 1)\n",
    "    # results_7_days_df  = filter_days(fact_statcast_pitches_last_90_days, 7)\n",
    "    # results_30_days_df = filter_days(fact_statcast_pitches_last_90_days, 30)\n",
    "\n",
    "    # Apply the function\n",
    "    load_fact_statcast_events(engine, fact_statcast_pitches_last_n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a30db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max date in the fact_statcast_pitches\n",
    "def get_latest_date_from_db():\n",
    "    query = text(\"SELECT MAX(game_date) FROM fact_statcast_pitches\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query).scalar()\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Execute and calculate fetch window\n",
    "last_date = get_latest_date_from_db()\n",
    "\n",
    "if last_date:\n",
    "    # I want to start fetching from the day AFTER the last recorded date\n",
    "    fetch_start = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    # Fetch up to yesterday\n",
    "    fetch_end = (date.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    if fetch_start <= fetch_end:\n",
    "        print(f\"üîÑ Last data was {last_date}. Fetching from {fetch_start} to {fetch_end}...\")\n",
    "        new_data = pyb.statcast(start_dt=fetch_start, end_dt=fetch_end)\n",
    "        new_data.to_sql('fact_statcast_pitches', engine, if_exists='append', index=False)\n",
    "    else:\n",
    "        print(\"‚úÖ Database is already up to date.\")\n",
    "else:\n",
    "    print(\"Empty table. You need to run an initial seed fetch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f7f00",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d72a7b",
   "metadata": {},
   "source": [
    "### Splits by team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams_split(split_type, clean_mode):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Define year\n",
    "    year = datetime.now().year\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)  \n",
    "\n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=total%7CLast%20{split_type}%20days%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=hmvis%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'first_batter_game':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=leado%7C1st%20Batter%20G%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_power_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Power%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_weak_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Finesse%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    # For each team:\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "            driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_less_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3C%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_greater_or_equal_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3E%3D%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    \n",
    "    # Name of the table\n",
    "    datatable_id = 'split1'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} ({split_type}) table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()    \n",
    "    \n",
    "    if clean_mode == 1:\n",
    "        # Remove 'Roe' exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "\n",
    "        # Rename last 3 columns\n",
    "        new_column_names = [\"BAbip\", \"tOPS+\", \"sOPS+\"]  # New names for last 3 columns\n",
    "        df.columns.values[-3:] = new_column_names  # Assign new names\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "    else:\n",
    "        # Remove 'Roe' and GS exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "        df[0] = df[0].str.replace('GS', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Remove rows where column 'A' contains 'Rk', but keep the first row\n",
    "        df = df[~((df.index > 0) & (df[0].str.contains('Rk', na=False)))]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "\n",
    "        # Remove the last 2 columns\n",
    "        df = df.iloc[:, :-2]\n",
    "\n",
    "        # New column names\n",
    "        new_column_names = ['Team', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'RBI', 'SB',\n",
    "                            'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDP', 'HBP', 'SH',\n",
    "                            'SF', 'IBB', 'ROE', 'BAbip', 'tOPS+', 'sOPS+']\n",
    "\n",
    "        # Rename all columns\n",
    "        df.columns = new_column_names\n",
    "\n",
    "    return df\n",
    "\n",
    "# Call the function to get the teams split data\n",
    "team_vs_lhp             = teams_split(split_type= 'LHP',  clean_mode= 0) # GS empty\n",
    "team_vs_rhp             = teams_split(split_type= 'RHP',  clean_mode= 0) # GS empty\n",
    "team_vs_lh_starters     = teams_split(split_type= 'LH',   clean_mode= 1)\n",
    "team_vs_rh_starters     = teams_split(split_type= 'RH',   clean_mode= 1)\n",
    "team_last_seven_days    = teams_split(split_type= '7',    clean_mode= 1)\n",
    "team_last_fourteen_days = teams_split(split_type= '14',   clean_mode= 1)\n",
    "team_last_28_days       = teams_split(split_type= '28',   clean_mode= 1)\n",
    "team_home_games         = teams_split(split_type= 'Home', clean_mode= 1)\n",
    "team_away_games         = teams_split(split_type= 'Away', clean_mode= 1)\n",
    "team_first_batter_game  = teams_split(split_type= 'first_batter_game', clean_mode= 0) # GS empty\n",
    "team_vs_power_pitcher   = teams_split(split_type= 'vs_power_pitcher',  clean_mode= 0) # GS empty\n",
    "team_vs_weak_pitcher    = teams_split(split_type= 'vs_weak_pitcher',   clean_mode= 0) # GS empty\n",
    "team_vs_power_team      = teams_split(split_type= 'vs_greater_or_equal_than_500_WP', clean_mode= 1)\n",
    "team_vs_weak_team       = teams_split(split_type= 'vs_less_than_500_WP',             clean_mode= 1)\n",
    "\n",
    "# # Direct matchups\n",
    "team_laa = teams_split(split_type= 'ANA', clean_mode= 1)\n",
    "team_ari = teams_split(split_type= 'ARI', clean_mode= 1)\n",
    "team_atl = teams_split(split_type= 'ATL', clean_mode= 1)\n",
    "team_bal = teams_split(split_type= 'BAL', clean_mode= 1)\n",
    "team_bos = teams_split(split_type= 'BOS', clean_mode= 1)\n",
    "team_chc = teams_split(split_type= 'CHC', clean_mode= 1)\n",
    "team_chw = teams_split(split_type= 'CHW', clean_mode= 1)\n",
    "team_cin = teams_split(split_type= 'CIN', clean_mode= 1)\n",
    "team_cle = teams_split(split_type= 'CLE', clean_mode= 1)\n",
    "team_col = teams_split(split_type= 'COL', clean_mode= 1)\n",
    "team_det = teams_split(split_type= 'DET', clean_mode= 1)\n",
    "team_hou = teams_split(split_type= 'HOU', clean_mode= 1)\n",
    "team_kcr = teams_split(split_type= 'KCR', clean_mode= 1)\n",
    "team_lad = teams_split(split_type= 'LAD', clean_mode= 1)\n",
    "team_mia = teams_split(split_type= 'FLA', clean_mode= 1) \n",
    "team_mil = teams_split(split_type= 'MIL', clean_mode= 1)\n",
    "team_min = teams_split(split_type= 'MIN', clean_mode= 1)\n",
    "team_nym = teams_split(split_type= 'NYM', clean_mode= 1)\n",
    "team_nyy = teams_split(split_type= 'NYY', clean_mode= 1)\n",
    "team_oak = teams_split(split_type= 'OAK', clean_mode= 1)\n",
    "team_phi = teams_split(split_type= 'PHI', clean_mode= 1)\n",
    "team_pit = teams_split(split_type= 'PIT', clean_mode= 1)\n",
    "team_sdp = teams_split(split_type= 'SDP', clean_mode= 1)\n",
    "team_sea = teams_split(split_type= 'SEA', clean_mode= 1)\n",
    "team_sfg = teams_split(split_type= 'SFG', clean_mode= 1)\n",
    "team_stl = teams_split(split_type= 'STL', clean_mode= 1)\n",
    "team_tbr = teams_split(split_type= 'TBD', clean_mode= 1)\n",
    "team_tex = teams_split(split_type= 'TEX', clean_mode= 1)\n",
    "team_tor = teams_split(split_type= 'TOR', clean_mode= 1)\n",
    "team_wsn = teams_split(split_type= 'WSN', clean_mode= 1)\n",
    "\n",
    "# Dictionary of dataframes for the teams\n",
    "dic_team = {\n",
    "    'LAA': team_laa,\n",
    "    'AZ':  team_ari,\n",
    "    'ATL': team_atl,\n",
    "    'BAL': team_bal,\n",
    "    'BOS': team_bos,\n",
    "    'CHC': team_chc,\n",
    "    'CHW': team_chw,\n",
    "    'CIN': team_cin,\n",
    "    'CLE': team_cle,\n",
    "    'COL': team_col,\n",
    "    'DET': team_det,\n",
    "    'HOU': team_hou,\n",
    "    'KC':  team_kcr,\n",
    "    'LAD': team_lad,\n",
    "    'MIA': team_mia,\n",
    "    'MIL': team_mil,\n",
    "    'MIN': team_min,\n",
    "    'NYM': team_nym,\n",
    "    'NYY': team_nyy,\n",
    "    'ATH': team_oak,\n",
    "    'PHI': team_phi,\n",
    "    'PIT': team_pit,\n",
    "    'SD':  team_sdp,\n",
    "    'SEA': team_sea,\n",
    "    'SF':  team_sfg,\n",
    "    'STL': team_stl,\n",
    "    'TB':  team_tbr,\n",
    "    'TEX': team_tex,\n",
    "    'TOR': team_tor,\n",
    "    'WSH': team_wsn   \n",
    "    }\n",
    "\n",
    "# Add an ID column with the dictionary key as the identifier\n",
    "for key, df in dic_team.items():\n",
    "    df['ID'] = key  # Assign the dictionary key as the ID\n",
    "\n",
    "# Concatenate all dataFrames in the dictionary\n",
    "direct_matches = pd.concat(dic_team.values(), ignore_index=True)  # Resets index\n",
    "\n",
    "dic_splits = {\n",
    "    'team_vs_lhp'        :team_vs_lhp,        \n",
    "    'team_vs_rhp'        :team_vs_rhp,\n",
    "    'team_vs_lh_starters':team_vs_lh_starters,\n",
    "    'team_vs_rh_starters':team_vs_rh_starters,\n",
    "    'team_last_seven_days':team_last_seven_days,\n",
    "    'team_last_fourteen_days':team_last_fourteen_days,\n",
    "    'team_last_28_days':team_last_28_days,\n",
    "    'team_home_games':team_home_games,\n",
    "    'team_away_games':team_away_games,\n",
    "    'team_first_batter_game':team_first_batter_game,\n",
    "    'team_vs_power_pitcher':team_vs_power_pitcher,\n",
    "    'team_vs_weak_pitcher':team_vs_weak_pitcher,\n",
    "    'team_vs_power_team':team_vs_power_team,\n",
    "    'team_vs_weak_team':team_vs_weak_team      \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d645a94",
   "metadata": {},
   "source": [
    "### Create batting_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "537925e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scrape Job with Driver Reuse and Retry Logic...\n",
      "------------------------------\n",
      "[BAL - vs_LHP]: Table loaded successfully.\n",
      "SUCCESS: Appended 30 rows. Master DF size: 30\n",
      "[BAL - vs_RHP]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 64\n",
      "[BAL - last_7_days]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 79\n",
      "[BAL - last_14_days]: Table loaded successfully.\n",
      "SUCCESS: Appended 16 rows. Master DF size: 95\n",
      "[BAL - last_28_days]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 113\n",
      "[BAL - home_games]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 151\n",
      "[BAL - away_games]: Table loaded successfully.\n",
      "SUCCESS: Appended 37 rows. Master DF size: 188\n",
      "[BAL - vs_RH_Starters]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 226\n",
      "[BAL - vs_LH_Starters]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 264\n",
      "[BAL - 1st_Half]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 298\n",
      "[BAL - 2nd_Half]: Table loaded successfully.\n",
      "SUCCESS: Appended 31 rows. Master DF size: 329\n",
      "[BAL - April_March]: Table loaded successfully.\n",
      "SUCCESS: Appended 17 rows. Master DF size: 346\n",
      "[BAL - June_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 371\n",
      "[BAL - July_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 389\n",
      "[BAL - August_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 23 rows. Master DF size: 412\n",
      "[BAL - Sept_Oct_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 430\n",
      "[BAL - C_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 7 rows. Master DF size: 437\n",
      "[BAL - 1B_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 6 rows. Master DF size: 443\n",
      "[BAL - 2B_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 6 rows. Master DF size: 449\n",
      "[BAL - 3B_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 7 rows. Master DF size: 456\n",
      "[BAL - SS_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 4 rows. Master DF size: 460\n",
      "[BAL - LF_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 10 rows. Master DF size: 470\n",
      "[BAL - CF_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 9 rows. Master DF size: 479\n",
      "[BAL - RF_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 491\n",
      "[BAL - DH_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 21 rows. Master DF size: 512\n",
      "[BAL - PH_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 28 rows. Master DF size: 540\n",
      "[BAL - First_Batter_Game]: Table loaded successfully.\n",
      "SUCCESS: Appended 10 rows. Master DF size: 550\n",
      "[BAL - First_Batter_Inning]: Table loaded successfully.\n",
      "SUCCESS: Appended 32 rows. Master DF size: 582\n",
      "[BAL - Batting_1st]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 595\n",
      "[BAL - Batting_2nd]: Table loaded successfully.\n",
      "SUCCESS: Appended 20 rows. Master DF size: 615\n",
      "[BAL - Batting_3rd]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 627\n",
      "[BAL - Batting_4th]: Table loaded successfully.\n",
      "SUCCESS: Appended 20 rows. Master DF size: 647\n",
      "[BAL - Batting_5th]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 672\n",
      "[BAL - Batting_6th]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 697\n",
      "[BAL - Batting_7th]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 722\n",
      "[BAL - Batting_8th]: Table loaded successfully.\n",
      "SUCCESS: Appended 27 rows. Master DF size: 749\n",
      "[BAL - Batting_9th]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 787\n",
      "[BAL - in_the_lineup_1-3rd]: Table loaded successfully.\n",
      "SUCCESS: Appended 30 rows. Master DF size: 817\n",
      "[BAL - in_the_lineup_4-6th]: Table loaded successfully.\n",
      "SUCCESS: Appended 31 rows. Master DF size: 848\n",
      "[BAL - in_the_lineup_7-9th]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 882\n",
      "[BAL - vs_SP]: Table loaded successfully.\n",
      "SUCCESS: Appended 32 rows. Master DF size: 914\n",
      "[BAL - vs_RP]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 948\n",
      "[BAL - vs_Power_Pitchers]: Table loaded successfully.\n",
      "SUCCESS: Appended 31 rows. Master DF size: 979\n",
      "[BAL - vs_Finesse_Pitchers]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 1013\n",
      "[BAL - batting_vs_ANA]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1031\n",
      "[BAL - batting_vs_ARI]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1044\n",
      "[BAL - batting_vs_ATL]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 1056\n",
      "[BAL - batting_vs_BAL]: Table loaded successfully.\n",
      "RETRYING: Attempt 1/3 for BAL - batting_vs_BAL...\n",
      "[BAL - batting_vs_BAL]: Table loaded successfully.\n",
      "RETRYING: Attempt 2/3 for BAL - batting_vs_BAL...\n",
      "[BAL - batting_vs_BAL]: Table loaded successfully.\n",
      "RETRYING: Attempt 3/3 for BAL - batting_vs_BAL...\n",
      "Skipping BAL - batting_vs_BAL after 3 failed attempts.\n",
      "[BAL - batting_vs_BOS]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 1081\n",
      "[BAL - batting_vs_CHC]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1096\n",
      "[BAL - batting_vs_CHW]: Table loaded successfully.\n",
      "SUCCESS: Appended 22 rows. Master DF size: 1118\n",
      "[BAL - batting_vs_CIN]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1131\n",
      "[BAL - batting_vs_CLE]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1149\n",
      "[BAL - batting_vs_COL]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1162\n",
      "[BAL - batting_vs_DET]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1177\n",
      "[BAL - batting_vs_HOU]: Table loaded successfully.\n",
      "SUCCESS: Appended 16 rows. Master DF size: 1193\n",
      "[BAL - batting_vs_KCR]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1211\n",
      "[BAL - batting_vs_LAD]: Table loaded successfully.\n",
      "SUCCESS: Appended 14 rows. Master DF size: 1225\n",
      "[BAL - batting_vs_FLA]: Table loaded successfully.\n",
      "SUCCESS: Appended 14 rows. Master DF size: 1239\n",
      "[BAL - batting_vs_MIL]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1252\n",
      "[BAL - batting_vs_MIN]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1267\n",
      "[BAL - batting_vs_NYM]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1280\n",
      "[BAL - batting_vs_NYY]: Table loaded successfully.\n",
      "SUCCESS: Appended 23 rows. Master DF size: 1303\n",
      "[BAL - batting_vs_OAK]: Table loaded successfully.\n",
      "SUCCESS: Appended 21 rows. Master DF size: 1324\n",
      "[BAL - batting_vs_PHI]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1337\n",
      "[BAL - batting_vs_PIT]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1352\n",
      "[BAL - batting_vs_SDP]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1365\n",
      "[BAL - batting_vs_SEA]: Table loaded successfully.\n",
      "SUCCESS: Appended 20 rows. Master DF size: 1385\n",
      "[BAL - batting_vs_SFG]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 1397\n",
      "[BAL - batting_vs_STL]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1410\n",
      "[BAL - batting_vs_TBD]: Table loaded successfully.\n",
      "SUCCESS: Appended 22 rows. Master DF size: 1432\n",
      "[BAL - batting_vs_TEX]: Table loaded successfully.\n",
      "SUCCESS: Appended 21 rows. Master DF size: 1453\n",
      "[BAL - batting_vs_TOR]: Table loaded successfully.\n",
      "SUCCESS: Appended 22 rows. Master DF size: 1475\n",
      "[BAL - batting_vs_WSN]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1493\n",
      "[BAL - batting_Day_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 40 rows. Master DF size: 1533\n",
      "[BAL - batting_Night_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 37 rows. Master DF size: 1570\n",
      "[BAL - batting_Grass_Field_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 41 rows. Master DF size: 1611\n",
      "[BAL - batting_Artificial_Turf_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 30 rows. Master DF size: 1641\n",
      "------------------------------\n",
      "All tasks finished. Quitting driver.\n",
      "Scraping Complete.\n",
      "Final DataFrame Shape: (1641, 33)\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2025\n",
    "DATATABLE_ID = 'team_split1' \n",
    "MAX_RETRIES = 3 \n",
    "\n",
    "# 2. Define the lists for iteration\n",
    "team_abbreviations = ['BAL']\n",
    "split_parameters = [\n",
    "    {'type': 'LHP',            'desc': 'vs_LHP'},\n",
    "    {'type': 'RHP',            'desc': 'vs_RHP'},\n",
    "    {'type': '7',              'desc': 'last_7_days'},\n",
    "    {'type': '14',             'desc': 'last_14_days'},\n",
    "    {'type': '28',             'desc': 'last_28_days'},\n",
    "    {'type': 'Home',           'desc': 'home_games'},\n",
    "    {'type': 'Away',           'desc': 'away_games'},\n",
    "    {'type': 'RH',             'desc': 'vs_RH_Starters'},\n",
    "    {'type': 'LH',             'desc': 'vs_LH_Starters'},\n",
    "    {'type': '1st',            'desc': '1st_Half'},\n",
    "    {'type': '2nd',            'desc': '2nd_Half'},\n",
    "    {'type': 'April%2FMarch',  'desc': 'April_March'},\n",
    "    {'type': 'June',           'desc': 'June_Splits'},\n",
    "    {'type': 'July',           'desc': 'July_Splits'},\n",
    "    {'type': 'August',         'desc': 'August_Splits'},\n",
    "    {'type': 'Sept%2FOct',     'desc': 'Sept_Oct_Splits'},\n",
    "    {'type': 'C',              'desc':'C_Position'},\n",
    "    {'type': '1B',             'desc': '1B_Position'},\n",
    "    {'type': '2B',             'desc': '2B_Position'},\n",
    "    {'type': '3B',             'desc': '3B_Position'},\n",
    "    {'type': 'SS',             'desc': 'SS_Position'},\n",
    "    {'type': 'LF',             'desc': 'LF_Position'},\n",
    "    {'type': 'CF',             'desc': 'CF_Position'},\n",
    "    {'type': 'RF',             'desc': 'RF_Position'},\n",
    "    {'type': 'DH',             'desc': 'DH_Position'},\n",
    "    {'type': 'PH',             'desc': 'PH_Position'},\n",
    "    {'type': '1st%20Batter',   'desc': 'First_Batter_Game'},\n",
    "    {'type': 'Leadoff%20Inn.', 'desc': 'First_Batter_Inning'},\n",
    "    {'type': 'Batting%201st',  'desc': 'Batting_1st'},\n",
    "    {'type': 'Batting%202nd',  'desc': 'Batting_2nd'},\n",
    "    {'type': 'Batting%203rd',  'desc': 'Batting_3rd'},\n",
    "    {'type': 'Batting%204th',  'desc': 'Batting_4th'},\n",
    "    {'type': 'Batting%205th',  'desc': 'Batting_5th'},\n",
    "    {'type': 'Batting%206th',  'desc': 'Batting_6th'},\n",
    "    {'type': 'Batting%207th',  'desc': 'Batting_7th'},\n",
    "    {'type': 'Batting%208th',  'desc': 'Batting_8th'},\n",
    "    {'type': 'Batting%209th',  'desc': 'Batting_9th'},\n",
    "    {'type': '1-3',            'desc': 'in_the_lineup_1-3rd'},\n",
    "    {'type': '4-6',            'desc': 'in_the_lineup_4-6th'},\n",
    "    {'type': '7-9',            'desc': 'in_the_lineup_7-9th'},\n",
    "    {'type': 'SP',             'desc': 'vs_SP'},\n",
    "    {'type': 'RP',             'desc': 'vs_RP'},\n",
    "    {'type': 'Power',          'desc': 'vs_Power_Pitchers'},\n",
    "    {'type': 'Finesse',        'desc': 'vs_Finesse_Pitchers'},\n",
    "    {'type': 'ANA',            'desc': 'batting_vs_ANA'},\n",
    "    {'type': 'ARI',            'desc': 'batting_vs_ARI'},\n",
    "    {'type': 'ATL',            'desc': 'batting_vs_ATL'},\n",
    "    {'type': 'BAL',            'desc': 'batting_vs_BAL'},\n",
    "    {'type': 'BOS',            'desc': 'batting_vs_BOS'},\n",
    "    {'type': 'CHC',            'desc': 'batting_vs_CHC'},\n",
    "    {'type': 'CHW',            'desc': 'batting_vs_CHW'},\n",
    "    {'type': 'CIN',            'desc': 'batting_vs_CIN'},\n",
    "    {'type': 'CLE',            'desc': 'batting_vs_CLE'},\n",
    "    {'type': 'COL',            'desc': 'batting_vs_COL'},\n",
    "    {'type': 'DET',            'desc': 'batting_vs_DET'},\n",
    "    {'type': 'HOU',            'desc': 'batting_vs_HOU'},\n",
    "    {'type': 'KCR',            'desc': 'batting_vs_KCR'},\n",
    "    {'type': 'LAD',            'desc': 'batting_vs_LAD'},\n",
    "    {'type': 'FLA',            'desc': 'batting_vs_FLA'},\n",
    "    {'type': 'MIL',            'desc': 'batting_vs_MIL'},\n",
    "    {'type': 'MIN',            'desc': 'batting_vs_MIN'},\n",
    "    {'type': 'NYM',            'desc': 'batting_vs_NYM'},\n",
    "    {'type': 'NYY',            'desc': 'batting_vs_NYY'},\n",
    "    {'type': 'OAK',            'desc': 'batting_vs_OAK'},\n",
    "    {'type': 'PHI',            'desc': 'batting_vs_PHI'},\n",
    "    {'type': 'PIT',            'desc': 'batting_vs_PIT'},\n",
    "    {'type': 'SDP',            'desc': 'batting_vs_SDP'},\n",
    "    {'type': 'SEA',            'desc': 'batting_vs_SEA'},\n",
    "    {'type': 'SFG',            'desc': 'batting_vs_SFG'},\n",
    "    {'type': 'STL',            'desc': 'batting_vs_STL'},\n",
    "    {'type': 'TBD',            'desc': 'batting_vs_TBD'},\n",
    "    {'type': 'TEX',            'desc': 'batting_vs_TEX'},\n",
    "    {'type': 'TOR',            'desc': 'batting_vs_TOR'},\n",
    "    {'type': 'WSN',            'desc': 'batting_vs_WSN'},\n",
    "    {'type': 'Day',            'desc': 'batting_Day_Games'},\n",
    "    {'type': 'Night',          'desc': 'batting_Night_Games'},\n",
    "    {'type': 'Grass',          'desc': 'batting_Grass_Field_Games'},\n",
    "    {'type': 'Artif.%20Turf',  'desc': 'batting_Artificial_Turf_Games'}\n",
    "]\n",
    "\n",
    "# Helper function to initialize driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a new Selenium WebDriver instance.\"\"\"\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") \n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\") # Use a recent, common User-Agent\n",
    "    # NOTE: Keep the path correct my Brave installation\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\" \n",
    "    \n",
    "    # Attempt to start the driver with a timeout\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options) \n",
    "        driver.set_page_load_timeout(60)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize Chrome driver. Check Brave path and driver version. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  batter_split function\n",
    "def batter_split(driver, split_type, team_abv, year, datatable_id, description):\n",
    "    \n",
    "    # --- URL CONSTRUCTION --- \n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'C' or split_type == '1B' or split_type == '2B' or split_type == '3B' \\\n",
    "        or split_type == 'SS' or split_type == 'LF' or split_type == 'CF' or split_type == 'RF' \\\n",
    "        or split_type == 'DH' or split_type == 'PH': # for each position\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=defp%7Cas%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '1st%20Batter': # first batter of the game\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Leadoff%20Inn.': # first batter of the inning\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Batting%201st' or split_type == 'Batting%202nd' or split_type == 'Batting%203rd' \\\n",
    "        or split_type == 'Batting%204th' or split_type == 'Batting%205th' or split_type == 'Batting%206th' \\\n",
    "        or split_type == 'Batting%207th' or split_type == 'Batting%208th' or split_type == 'Batting%209th': # for each spot in the lineup\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=lineu%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '1-3' or split_type == '4-6' or split_type == '7-9': # for each third of the lineup\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=innng%7CInnings%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'SP' or split_type == 'RP': # vs SP or RP\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=times%7Cvs.%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Power' or split_type == 'avg.P%2FF' or split_type == 'Finesse':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=power%7Cvs.%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\" \n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=oppon%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Day' or split_type == 'Night' or split_type == 'Grass' or split_type == 'Artif.%20Turf':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=stad%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(f\"[{team_abv} - {description}]: Page load timed out (60s). Skipping or retrying...\")\n",
    "        return None # Let the main loop handle the retry/skip\n",
    "        \n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    \n",
    "    # --- SCRAPING LOGIC --- \n",
    "    try:\n",
    "        # Wait up to 30 seconds for the table\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        print(f\"[{team_abv} - {description}]: Table loaded successfully.\")\n",
    "    except Exception:\n",
    "        # This catches both TimeoutException and NoSuchElementException\n",
    "        print(f\"[{team_abv} - {description}]: Table element not found after 30s. Check site content.\")\n",
    "        return None \n",
    "\n",
    "    # Extract the full HTML, wrap in StringIO, read with pandas\n",
    "    table_html = table_element.get_attribute('outerHTML')\n",
    "    html_string = StringIO(table_html)\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(html_string, flavor='lxml') \n",
    "    except Exception as e:\n",
    "        print(f\"[{team_abv} - {description}]: Error parsing HTML with pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"[{team_abv} - {description}]: No tables found.\")\n",
    "        return None\n",
    "\n",
    "    # Create an explicit copy\n",
    "    df = tables[0].copy() \n",
    "    \n",
    "    # --- CLEANING LOGIC --- \n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns]\n",
    "\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df[df['Rk'] != 'Rk']\n",
    "        \n",
    "    df = df.iloc[:-1] # Remove last row (Totals)\n",
    "    \n",
    "    df['description'] = description\n",
    "    df['team'] = team_abv\n",
    "    df['year'] = YEAR\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Master loop with driver reuse and retry logic\n",
    "\n",
    "batting_splits = pd.DataFrame()\n",
    "driver = initialize_driver()\n",
    "\n",
    "if driver is None:\n",
    "    exit() # Stop if the driver failed to initialize\n",
    "\n",
    "print(\"Starting Scrape Job with Driver Reuse and Retry Logic...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # Outer loop for teams\n",
    "    for team_abv in team_abbreviations:\n",
    "        # Inner loop for splits\n",
    "        for split in split_parameters:\n",
    "            \n",
    "            # Retry loop for failed connection/table load\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Check if the driver is still alive (by checking its current URL)\n",
    "                    driver.current_url \n",
    "                    \n",
    "                    new_df = batter_split(\n",
    "                        driver=driver,\n",
    "                        split_type=split['type'], \n",
    "                        team_abv=team_abv, \n",
    "                        year=YEAR, \n",
    "                        datatable_id=DATATABLE_ID, \n",
    "                        description=split['desc']\n",
    "                    )\n",
    "                    \n",
    "                    if new_df is not None and not new_df.empty:\n",
    "                        batting_splits = pd.concat([batting_splits, new_df], ignore_index=True)\n",
    "                        print(f\"SUCCESS: Appended {len(new_df)} rows. Master DF size: {len(batting_splits)}\")\n",
    "                        break # Break the retry loop on success\n",
    "                    \n",
    "                    # If new_df is None (due to TimeoutException/Table not found), retry\n",
    "                    print(f\"RETRYING: Attempt {attempt + 1}/{MAX_RETRIES} for {team_abv} - {split['desc']}...\")\n",
    "                    time.sleep(2) # Short wait before retry\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    # CRITICAL: Driver died (Connection refused/lost)\n",
    "                    print(f\"\\n[FATAL ERROR] Driver connection lost for {team_abv} - {split['desc']}. Restarting driver...\")\n",
    "                    \n",
    "                    # Clean up the old session\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass # Ignore errors on quitting a dead driver\n",
    "                    \n",
    "                    # Restart the driver\n",
    "                    driver = initialize_driver()\n",
    "                    if driver is None:\n",
    "                        # If restart fails, stop the whole script\n",
    "                        raise SystemExit(\"Driver restart failed. Terminating.\")\n",
    "                        \n",
    "                    time.sleep(5) # Longer wait after a fatal crash\n",
    "                    print(\"Driver successfully restarted. Retrying scrape.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[{team_abv} - {split['desc']}]: Unhandled error: {e}\")\n",
    "                    break # Break retry loop on unexpected failure\n",
    "\n",
    "            # Check if retry failed all attempts and the split was not appended\n",
    "            else: \n",
    "                print(f\"Skipping {team_abv} - {split['desc']} after {MAX_RETRIES} failed attempts.\")\n",
    "                \n",
    "finally:\n",
    "    # 3. CLEANUP: Quit the driver ONCE after all loops are finished\n",
    "    print(\"-\" * 30)\n",
    "    print(\"All tasks finished. Quitting driver.\")\n",
    "    if 'driver' in locals() and driver:\n",
    "        driver.quit() \n",
    "    \n",
    "print(\"Scraping Complete.\")\n",
    "print(f\"Final DataFrame Shape: {batting_splits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cd99b",
   "metadata": {},
   "source": [
    "### Pitching splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "YEAR = 2025\n",
    "DATATABLE_ID = 'team_split1' \n",
    "MAX_RETRIES = 3 \n",
    "\n",
    "# 2. Define the lists for iteration\n",
    "team_abbreviations = ['BAL']\n",
    "split_parameters = [\n",
    "    {'type': 'LHB',                      'desc': 'vs_LHB'},\n",
    "    {'type': 'RHB',                      'desc': 'vs_RHB'},\n",
    "    {'type': '7',                        'desc': 'last_7_days'},\n",
    "    {'type': '14',                       'desc': 'last_14_days'},\n",
    "    {'type': '28',                       'desc': 'last_28_days'},\n",
    "    {'type': 'Home',                     'desc': 'home_games'},\n",
    "    {'type': 'Away',                     'desc': 'away_games'},\n",
    "    {'type': '1st',                      'desc': '1st_half'},\n",
    "    {'type': '2nd',                      'desc': '2nd_half'},\n",
    "    {'type': 'April%2FMarch',            'desc': 'april_march'},\n",
    "    {'type': 'June',                     'desc': 'june_splits'},\n",
    "    {'type': 'July',                     'desc': 'july_splits'},\n",
    "    {'type': 'August',                   'desc': 'august_splits'},\n",
    "    {'type': 'Sept%2FOct',               'desc': 'sept_oct_Splits'},\n",
    "    {'type': '1st%20Batter',             'desc': 'first_batter_game'},\n",
    "    {'type': 'Leadoff%20Inn.',           'desc': 'first_batter_inning'},\n",
    "    {'type': 'Batting%201st',            'desc': 'pitching_vs_1st'},\n",
    "    {'type': 'Batting%202nd',            'desc': 'pitching_vs_2nd'},\n",
    "    {'type': 'Batting%203rd',            'desc': 'pitching_vs_3rd'},\n",
    "    {'type': 'Batting%204th',            'desc': 'pitching_vs_4th'},\n",
    "    {'type': 'Batting%205th',            'desc': 'pitching_vs_5th'},\n",
    "    {'type': 'Batting%206th',            'desc': 'pitching_vs_6th'},\n",
    "    {'type': 'Batting%207th',            'desc': 'pitching_vs_7th'},\n",
    "    {'type': 'Batting%208th',            'desc': 'pitching_vs_8th'},\n",
    "    {'type': 'Batting%209th',            'desc': 'pitching_vs_9th'},\n",
    "    {'type': 'Starter',                  'desc': 'as_starter'},\n",
    "    {'type': 'Reliever',                 'desc': 'as_reliever'},\n",
    "    {'type': '0-2%20Runs',               'desc': 'run_support_0_2'},\n",
    "    {'type': '3-5%20Runs',               'desc': 'run_support_3_5'},\n",
    "    {'type': '6%2B%20Runs',              'desc': 'run_support_6_plus'},\n",
    "    {'type': 'Swung%20at%201st%20Pitch', 'desc': 'outcome_of_at_bat_when_swung_at_first_pitch'},\n",
    "    {'type': 'Took%201st%20Pitch',       'desc': 'outcome_of_at_bat_when_took_first_pitch'},\n",
    "    {'type': '0',                        'desc': '0_outs_in_the_inning'},\n",
    "    {'type': '1',                        'desc': '1_outs_in_the_inning'},\n",
    "    {'type': '2',                        'desc': '2_outs_in_the_inning'},\n",
    "    {'type': 'innng%7C1st',             'desc': 'pitching_in_1st_inning'},\n",
    "    {'type': 'innng%7C2nd',             'desc': 'pitching_in_2nd_inning'},\n",
    "    {'type': 'innng%7C3rd',             'desc': 'pitching_in_3rd_inning'},\n",
    "    {'type': 'innng%7C4th',             'desc': 'pitching_in_4th_inning'},\n",
    "    {'type': 'innng%7C5th',             'desc': 'pitching_in_5th_inning'},\n",
    "    {'type': 'innng%7C6th',             'desc': 'pitching_in_6th_inning'},\n",
    "    {'type': 'innng%7C7th',             'desc': 'pitching_in_7th_inning'},\n",
    "    {'type': 'innng%7C8th',             'desc': 'pitching_in_8th_inning'},\n",
    "    {'type': 'innng%7C9th',             'desc': 'pitching_in_9th_inning'},\n",
    "    {'type': 'ANA',                      'desc': 'pitching_vs_ANA'},\n",
    "    {'type': 'ARI',                      'desc': 'pitching_vs_ARI'},\n",
    "    {'type': 'ATL',                      'desc': 'pitching_vs_ATL'},\n",
    "    {'type': 'BAL',                      'desc': 'pitching_vs_BAL'},\n",
    "    {'type': 'BOS',                      'desc': 'pitching_vs_BOS'},\n",
    "    {'type': 'CHC',                      'desc': 'pitching_vs_CHC'},\n",
    "    {'type': 'CHW',                      'desc': 'pitching_vs_CHW'},\n",
    "    {'type': 'CIN',                      'desc': 'pitching_vs_CIN'},\n",
    "    {'type': 'CLE',                      'desc': 'pitching_vs_CLE'},\n",
    "    {'type': 'COL',                      'desc': 'pitching_vs_COL'},\n",
    "    {'type': 'DET',                      'desc': 'pitching_vs_DET'},\n",
    "    {'type': 'HOU',                      'desc': 'pitching_vs_HOU'},\n",
    "    {'type': 'KCR',                      'desc': 'pitching_vs_KCR'},\n",
    "    {'type': 'LAD',                      'desc': 'pitching_vs_LAD'},\n",
    "    {'type': 'FLA',                      'desc': 'pitching_vs_FLA'},\n",
    "    {'type': 'MIL',                      'desc': 'pitching_vs_MIL'},\n",
    "    {'type': 'MIN',                      'desc': 'pitching_vs_MIN'},\n",
    "    {'type': 'NYM',                      'desc': 'pitching_vs_NYM'},\n",
    "    {'type': 'NYY',                      'desc': 'pitching_vs_NYY'},\n",
    "    {'type': 'OAK',                      'desc': 'pitching_vs_OAK'},\n",
    "    {'type': 'PHI',                      'desc': 'pitching_vs_PHI'},\n",
    "    {'type': 'PIT',                      'desc': 'pitching_vs_PIT'},\n",
    "    {'type': 'SDP',                      'desc': 'pitching_vs_SDP'},\n",
    "    {'type': 'SEA',                      'desc': 'pitching_vs_SEA'},\n",
    "    {'type': 'SFG',                      'desc': 'pitching_vs_SFG'},\n",
    "    {'type': 'STL',                      'desc': 'pitching_vs_STL'},\n",
    "    {'type': 'TBD',                      'desc': 'pitching_vs_TBD'},\n",
    "    {'type': 'TEX',                      'desc': 'pitching_vs_TEX'},\n",
    "    {'type': 'TOR',                      'desc': 'pitching_vs_TOR'},\n",
    "    {'type': 'WSN',                      'desc': 'pitching_vs_WSN'},\n",
    "    {'type': 'Day',                      'desc': 'pitching_Day_Games'},\n",
    "    {'type': 'Night',                    'desc': 'pitching_Night_Games'},\n",
    "    {'type': 'Grass',                    'desc': 'pitching_Grass_Field_Games'},\n",
    "    {'type': 'Artif.%20Turf',            'desc': 'pitching_Artificial_Turf_Games'}\n",
    "]\n",
    "\n",
    "# Helper function to initialize driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a new Selenium WebDriver instance.\"\"\"\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") \n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\") # Use a recent, common User-Agent\n",
    "    # NOTE: Keep the path correct my Brave installation\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\" \n",
    "    \n",
    "    # Attempt to start the driver with a timeout\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options) \n",
    "        driver.set_page_load_timeout(60)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize Chrome driver. Check Brave path and driver version. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  pitcher_split function\n",
    "def pitcher_split(driver, split_type, team_abv, year, datatable_id, description):\n",
    "\n",
    "    # --- URL CONSTRUCTION --- \n",
    "    if split_type == 'LHB' or split_type == 'RHB': # matchups vs LHB and RHB hitters\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '1st%20Batter': # first batter of the game\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Leadoff%20Inn.': # first batter of the inning\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Batting%201st' or split_type == 'Batting%202nd' or split_type == 'Batting%203rd' \\\n",
    "        or split_type == 'Batting%204th' or split_type == 'Batting%205th' or split_type == 'Batting%206th' \\\n",
    "        or split_type == 'Batting%207th' or split_type == 'Batting%208th' or split_type == 'Batting%209th': \n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=lineu%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Starter' or split_type == 'Reliever':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=sprel%7Cas%20{split_type}%7CT{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '0-2%20Runs' or split_type == '3-5%20Runs' or split_type == '6%2B%20Runs':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=rs%7C{split_type}%20Scored%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Swung%20at%201st%20Pitch' or split_type == 'Took%201st%20Pitch':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=tkswg%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '0' or split_type == '1' or split_type == '2':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=outs%7C{split_type}%20outs%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'innng%7C1st' or split_type == 'innng%7C2nd' or split_type == 'innng%7C3rd' \\\n",
    "        or split_type == 'innng%7C4th' or split_type == 'innng%7C5th' or split_type == 'innng%7C6th' \\\n",
    "        or split_type == 'innng%7C7th' or split_type == 'innng%7C8th' or split_type == 'innng%7C9th':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params={split_type}%20inning%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=oppon%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Day' or split_type == 'Night' or split_type == 'Grass' or split_type == 'Artif.%20Turf':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=stad%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    else:\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(f\"[{team_abv} - {description}]: Page load timed out (60s). Skipping or retrying...\")\n",
    "        return None # Let the main loop handle the retry/skip\n",
    "        \n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    \n",
    "    # --- SCRAPING LOGIC --- \n",
    "    try:\n",
    "        # Wait up to 30 seconds for the table\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        print(f\"[{team_abv} - {description}]: Table loaded successfully.\")\n",
    "    except Exception:\n",
    "        # This catches both TimeoutException and NoSuchElementException\n",
    "        print(f\"[{team_abv} - {description}]: Table element not found after 30s. Check site content.\")\n",
    "        return None \n",
    "\n",
    "    # Extract the full HTML, wrap in StringIO, read with pandas\n",
    "    table_html = table_element.get_attribute('outerHTML')\n",
    "    html_string = StringIO(table_html)\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(html_string, flavor='lxml') \n",
    "    except Exception as e:\n",
    "        print(f\"[{team_abv} - {description}]: Error parsing HTML with pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"[{team_abv} - {description}]: No tables found.\")\n",
    "        return None\n",
    "\n",
    "    # Create an explicit copy\n",
    "    df = tables[0].copy() \n",
    "    \n",
    "    # --- CLEANING LOGIC --- \n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns]\n",
    "\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df[df['Rk'] != 'Rk']\n",
    "        \n",
    "    df = df.iloc[:-1] # Remove last row (Totals)\n",
    "    \n",
    "    df['description'] = description\n",
    "    df['team'] = team_abv\n",
    "    df['year'] = YEAR\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Master loop with driver reuse and retry logic\n",
    "\n",
    "pitching_splits = pd.DataFrame()\n",
    "driver = initialize_driver()\n",
    "\n",
    "if driver is None:\n",
    "    exit() # Stop if the driver failed to initialize\n",
    "\n",
    "print(\"Starting Scrape Job with Driver Reuse and Retry Logic...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # Outer loop for teams\n",
    "    for team_abv in team_abbreviations:\n",
    "        # Inner loop for splits\n",
    "        for split in split_parameters:\n",
    "            \n",
    "            # Retry loop for failed connection/table load\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Check if the driver is still alive (by checking its current URL)\n",
    "                    driver.current_url \n",
    "                    \n",
    "                    new_df = pitcher_split(\n",
    "                        driver=driver,\n",
    "                        split_type=split['type'], \n",
    "                        team_abv=team_abv, \n",
    "                        year=YEAR, \n",
    "                        datatable_id=DATATABLE_ID, \n",
    "                        description=split['desc']\n",
    "                    )\n",
    "                    \n",
    "                    if new_df is not None and not new_df.empty:\n",
    "                        pitching_splits = pd.concat([pitching_splits, new_df], ignore_index=True)\n",
    "                        print(f\"SUCCESS: Appended {len(new_df)} rows. Master DF size: {len(pitching_splits)}\")\n",
    "                        break # Break the retry loop on success\n",
    "                    \n",
    "                    # If new_df is None (due to TimeoutException/Table not found), retry\n",
    "                    print(f\"RETRYING: Attempt {attempt + 1}/{MAX_RETRIES} for {team_abv} - {split['desc']}...\")\n",
    "                    time.sleep(2) # Short wait before retry\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    # CRITICAL: Driver died (Connection refused/lost)\n",
    "                    print(f\"\\n[FATAL ERROR] Driver connection lost for {team_abv} - {split['desc']}. Restarting driver...\")\n",
    "                    \n",
    "                    # Clean up the old session\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass # Ignore errors on quitting a dead driver\n",
    "                    \n",
    "                    # Restart the driver\n",
    "                    driver = initialize_driver()\n",
    "                    if driver is None:\n",
    "                        # If restart fails, stop the whole script\n",
    "                        raise SystemExit(\"Driver restart failed. Terminating.\")\n",
    "                        \n",
    "                    time.sleep(5) # Longer wait after a fatal crash\n",
    "                    print(\"Driver successfully restarted. Retrying scrape.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[{team_abv} - {split['desc']}]: Unhandled error: {e}\")\n",
    "                    break # Break retry loop on unexpected failure\n",
    "\n",
    "            # Check if retry failed all attempts and the split was not appended\n",
    "            else: \n",
    "                print(f\"Skipping {team_abv} - {split['desc']} after {MAX_RETRIES} failed attempts.\")\n",
    "                \n",
    "finally:\n",
    "    # 3. CLEANUP: Quit the driver ONCE after all loops are finished\n",
    "    print(\"-\" * 30)\n",
    "    print(\"All tasks finished. Quitting driver.\")\n",
    "    if 'driver' in locals() and driver:\n",
    "        driver.quit() \n",
    "    \n",
    "print(\"Scraping Complete.\")\n",
    "print(f\"Final DataFrame Shape: {pitching_splits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce458d",
   "metadata": {},
   "source": [
    "### Pitching splits - Game Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "YEAR = 2025\n",
    "DATATABLE_ID = 'team_split1' \n",
    "MAX_RETRIES = 3 \n",
    "\n",
    "# 2. Define the lists for iteration\n",
    "team_abbreviations = ['BAL']\n",
    "split_parameters = [\n",
    "    {'type': '7',                        'desc': 'last_7_days'},\n",
    "    {'type': '14',                       'desc': 'last_14_days'},\n",
    "    {'type': '28',                       'desc': 'last_28_days'},\n",
    "    {'type': 'Home',                     'desc': 'home_games'},\n",
    "    {'type': 'Away',                     'desc': 'away_games'},\n",
    "    {'type': '1st',                      'desc': '1st_half'},\n",
    "    {'type': '2nd',                      'desc': '2nd_half'},\n",
    "    {'type': 'April%2FMarch',            'desc': 'april_march'},\n",
    "    {'type': 'June',                     'desc': 'june_splits'},\n",
    "    {'type': 'July',                     'desc': 'july_splits'},\n",
    "    {'type': 'August',                   'desc': 'august_splits'},\n",
    "    {'type': 'Sept%2FOct',               'desc': 'sept_oct_Splits'},\n",
    "    {'type': 'Starter',                  'desc': 'as_starter'},\n",
    "    {'type': 'Reliever',                 'desc': 'as_reliever'},\n",
    "    {'type': '0-2%20Runs',               'desc': 'run_support_0_2'},\n",
    "    {'type': '3-5%20Runs',               'desc': 'run_support_3_5'},\n",
    "    {'type': '6%2B%20Runs',              'desc': 'run_support_6_plus'},\n",
    "    {'type': 'ANA',                      'desc': 'pitching_vs_ANA'},\n",
    "    {'type': 'ARI',                      'desc': 'pitching_vs_ARI'},\n",
    "    {'type': 'ATL',                      'desc': 'pitching_vs_ATL'},\n",
    "    {'type': 'BAL',                      'desc': 'pitching_vs_BAL'},\n",
    "    {'type': 'BOS',                      'desc': 'pitching_vs_BOS'},\n",
    "    {'type': 'CHC',                      'desc': 'pitching_vs_CHC'},\n",
    "    {'type': 'CHW',                      'desc': 'pitching_vs_CHW'},\n",
    "    {'type': 'CIN',                      'desc': 'pitching_vs_CIN'},\n",
    "    {'type': 'CLE',                      'desc': 'pitching_vs_CLE'},\n",
    "    {'type': 'COL',                      'desc': 'pitching_vs_COL'},\n",
    "    {'type': 'DET',                      'desc': 'pitching_vs_DET'},\n",
    "    {'type': 'HOU',                      'desc': 'pitching_vs_HOU'},\n",
    "    {'type': 'KCR',                      'desc': 'pitching_vs_KCR'},\n",
    "    {'type': 'LAD',                      'desc': 'pitching_vs_LAD'},\n",
    "    {'type': 'FLA',                      'desc': 'pitching_vs_FLA'},\n",
    "    {'type': 'MIL',                      'desc': 'pitching_vs_MIL'},\n",
    "    {'type': 'MIN',                      'desc': 'pitching_vs_MIN'},\n",
    "    {'type': 'NYM',                      'desc': 'pitching_vs_NYM'},\n",
    "    {'type': 'NYY',                      'desc': 'pitching_vs_NYY'},\n",
    "    {'type': 'OAK',                      'desc': 'pitching_vs_OAK'},\n",
    "    {'type': 'PHI',                      'desc': 'pitching_vs_PHI'},\n",
    "    {'type': 'PIT',                      'desc': 'pitching_vs_PIT'},\n",
    "    {'type': 'SDP',                      'desc': 'pitching_vs_SDP'},\n",
    "    {'type': 'SEA',                      'desc': 'pitching_vs_SEA'},\n",
    "    {'type': 'SFG',                      'desc': 'pitching_vs_SFG'},\n",
    "    {'type': 'STL',                      'desc': 'pitching_vs_STL'},\n",
    "    {'type': 'TBD',                      'desc': 'pitching_vs_TBD'},\n",
    "    {'type': 'TEX',                      'desc': 'pitching_vs_TEX'},\n",
    "    {'type': 'TOR',                      'desc': 'pitching_vs_TOR'},\n",
    "    {'type': 'WSN',                      'desc': 'pitching_vs_WSN'},\n",
    "    {'type': 'Day',                      'desc': 'pitching_Day_Games'},\n",
    "    {'type': 'Night',                    'desc': 'pitching_Night_Games'},\n",
    "    {'type': 'Grass',                    'desc': 'pitching_Grass_Field_Games'},\n",
    "    {'type': 'Artif.%20Turf',            'desc': 'pitching_Artificial_Turf_Games'}\n",
    "]\n",
    "\n",
    "# Helper function to initialize driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a new Selenium WebDriver instance.\"\"\"\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") \n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\") # Use a recent, common User-Agent\n",
    "    # NOTE: Keep the path correct my Brave installation\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\" \n",
    "    \n",
    "    # Attempt to start the driver with a timeout\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options) \n",
    "        driver.set_page_load_timeout(60)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize Chrome driver. Check Brave path and driver version. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  pitcher_split function\n",
    "def pitcher_split_game_level(driver, split_type, team_abv, year, datatable_id, description):\n",
    "\n",
    "    # --- URL CONSTRUCTION --- \n",
    "    if split_type == 'LHB' or split_type == 'RHB': # matchups vs LHB and RHB hitters\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '1st%20Batter': # first batter of the game\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Leadoff%20Inn.': # first batter of the inning\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Batting%201st' or split_type == 'Batting%202nd' or split_type == 'Batting%203rd' \\\n",
    "        or split_type == 'Batting%204th' or split_type == 'Batting%205th' or split_type == 'Batting%206th' \\\n",
    "        or split_type == 'Batting%207th' or split_type == 'Batting%208th' or split_type == 'Batting%209th': \n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=lineu%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Starter' or split_type == 'Reliever':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=sprel%7Cas%20{split_type}%7CT{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '0-2%20Runs' or split_type == '3-5%20Runs' or split_type == '6%2B%20Runs':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=rs%7C{split_type}%20Scored%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Swung%20at%201st%20Pitch' or split_type == 'Took%201st%20Pitch':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=tkswg%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '0' or split_type == '1' or split_type == '2':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=outs%7C{split_type}%20outs%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'innng%7C1st' or split_type == 'innng%7C2nd' or split_type == 'innng%7C3rd' \\\n",
    "        or split_type == 'innng%7C4th' or split_type == 'innng%7C5th' or split_type == 'innng%7C6th' \\\n",
    "        or split_type == 'innng%7C7th' or split_type == 'innng%7C8th' or split_type == 'innng%7C9th':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params={split_type}%20inning%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=oppon%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Day' or split_type == 'Night' or split_type == 'Grass' or split_type == 'Artif.%20Turf':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=stad%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    else:\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(f\"[{team_abv} - {description}]: Page load timed out (60s). Skipping or retrying...\")\n",
    "        return None # Let the main loop handle the retry/skip\n",
    "        \n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    \n",
    "    # --- SCRAPING LOGIC --- \n",
    "    try:\n",
    "        # Wait up to 30 seconds for the table\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        print(f\"[{team_abv} - {description}]: Table loaded successfully.\")\n",
    "    except Exception:\n",
    "        # This catches both TimeoutException and NoSuchElementException\n",
    "        print(f\"[{team_abv} - {description}]: Table element not found after 30s. Check site content.\")\n",
    "        return None \n",
    "\n",
    "    # Extract the full HTML, wrap in StringIO, read with pandas\n",
    "    table_html = table_element.get_attribute('outerHTML')\n",
    "    html_string = StringIO(table_html)\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(html_string, flavor='lxml') \n",
    "    except Exception as e:\n",
    "        print(f\"[{team_abv} - {description}]: Error parsing HTML with pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"[{team_abv} - {description}]: No tables found.\")\n",
    "        return None\n",
    "\n",
    "    # Create an explicit copy\n",
    "    df = tables[0].copy() \n",
    "    \n",
    "    # --- CLEANING LOGIC --- \n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns]\n",
    "\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df[df['Rk'] != 'Rk']\n",
    "        \n",
    "    df = df.iloc[:-1] # Remove last row (Totals)\n",
    "    \n",
    "    df['description'] = description\n",
    "    df['team'] = team_abv\n",
    "    df['year'] = YEAR\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Master loop with driver reuse and retry logic\n",
    "\n",
    "pitching_splits_game_level = pd.DataFrame()\n",
    "driver = initialize_driver()\n",
    "\n",
    "if driver is None:\n",
    "    exit() # Stop if the driver failed to initialize\n",
    "\n",
    "print(\"Starting Scrape Job with Driver Reuse and Retry Logic...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # Outer loop for teams\n",
    "    for team_abv in team_abbreviations:\n",
    "        # Inner loop for splits\n",
    "        for split in split_parameters:\n",
    "            \n",
    "            # Retry loop for failed connection/table load\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Check if the driver is still alive (by checking its current URL)\n",
    "                    driver.current_url \n",
    "                    \n",
    "                    new_df = pitcher_split_game_level(\n",
    "                        driver=driver,\n",
    "                        split_type=split['type'], \n",
    "                        team_abv=team_abv, \n",
    "                        year=YEAR, \n",
    "                        datatable_id=DATATABLE_ID, \n",
    "                        description=split['desc']\n",
    "                    )\n",
    "                    \n",
    "                    if new_df is not None and not new_df.empty:\n",
    "                        pitching_splits_game_level = pd.concat([pitching_splits_game_level, new_df], ignore_index=True)\n",
    "                        print(f\"SUCCESS: Appended {len(new_df)} rows. Master DF size: {len(pitching_splits_game_level)}\")\n",
    "                        break # Break the retry loop on success\n",
    "                    \n",
    "                    # If new_df is None (due to TimeoutException/Table not found), retry\n",
    "                    print(f\"RETRYING: Attempt {attempt + 1}/{MAX_RETRIES} for {team_abv} - {split['desc']}...\")\n",
    "                    time.sleep(2) # Short wait before retry\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    # CRITICAL: Driver died (Connection refused/lost)\n",
    "                    print(f\"\\n[FATAL ERROR] Driver connection lost for {team_abv} - {split['desc']}. Restarting driver...\")\n",
    "                    \n",
    "                    # Clean up the old session\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass # Ignore errors on quitting a dead driver\n",
    "                    \n",
    "                    # Restart the driver\n",
    "                    driver = initialize_driver()\n",
    "                    if driver is None:\n",
    "                        # If restart fails, stop the whole script\n",
    "                        raise SystemExit(\"Driver restart failed. Terminating.\")\n",
    "                        \n",
    "                    time.sleep(5) # Longer wait after a fatal crash\n",
    "                    print(\"Driver successfully restarted. Retrying scrape.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[{team_abv} - {split['desc']}]: Unhandled error: {e}\")\n",
    "                    break # Break retry loop on unexpected failure\n",
    "\n",
    "            # Check if retry failed all attempts and the split was not appended\n",
    "            else: \n",
    "                print(f\"Skipping {team_abv} - {split['desc']} after {MAX_RETRIES} failed attempts.\")\n",
    "                \n",
    "finally:\n",
    "    # 3. CLEANUP: Quit the driver ONCE after all loops are finished\n",
    "    print(\"-\" * 30)\n",
    "    print(\"All tasks finished. Quitting driver.\")\n",
    "    if 'driver' in locals() and driver:\n",
    "        driver.quit() \n",
    "    \n",
    "print(\"Scraping Complete.\")\n",
    "print(f\"Final DataFrame Shape: {pitching_splits_game_level.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405a5f6",
   "metadata": {},
   "source": [
    "### Update the table statcast_pitches in PostgreSQL\n",
    "#### This table shows the events pitch-by-pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23114b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybaseball.cache.enable() # Enable caching for reliability\n",
    "\n",
    "def update_statcast_data(engine: Engine):\n",
    "    \"\"\"\n",
    "    Pulls Statcast data starting from the day AFTER the last record in the database\n",
    "    to ensure only new events are downloaded and appended.\n",
    "    \"\"\"\n",
    "    \n",
    "    today = date.today()\n",
    "    \n",
    "    # --- STEP 1: FIND LAST DATE IN DB ---\n",
    "    try:\n",
    "        # Query the database to find the latest game_date currently stored\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(\n",
    "                text(\"SELECT MAX(game_date) FROM statcast_pitches;\")\n",
    "            ).scalar()\n",
    "        \n",
    "        # If the table is empty, start from 400 days ago (initial load range)\n",
    "        if result is None:\n",
    "            print(\"Database is empty. Starting full initial load (400 days)...\")\n",
    "            last_date = today - timedelta(days=400)\n",
    "        else:\n",
    "            # Start the new pull from the day AFTER the last record\n",
    "            last_date = result.date()\n",
    "            print(f\"Latest game_date found in DB: {last_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR querying database for last date: {e}. Defaulting to last 5 days.\")\n",
    "        last_date = today - timedelta(days=5)\n",
    "\n",
    "    \n",
    "    # --- STEP 2: DEFINE NEW EXTRACTION RANGE ---\n",
    "    start_date = last_date + timedelta(days=1)\n",
    "    end_date = today - timedelta(days=1) # Pull up to yesterday, as today's games aren't finished\n",
    "\n",
    "    start_dt_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_dt_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    if start_date >= end_date:\n",
    "        print(f\"Data is up to date as of {end_dt_str}. No new extraction needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting DAILY Statcast ETL: Pulling data from {start_dt_str} to {end_dt_str}\")\n",
    "    \n",
    "    # --- STEP 3: EXTRACTION ---\n",
    "    try:\n",
    "        df = pyb.statcast(start_dt=start_dt_str, end_dt=end_dt_str)\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            print(\"No new Statcast data retrieved for this date range. Exiting.\")\n",
    "            return\n",
    "\n",
    "        #  --- STEP 4: TRANSFORMATION ---        \n",
    "        # Handle data types before loading (optional, but good practice)\n",
    "        df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "        \n",
    "        # # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(df)} new rows into 'statcast_pitches'...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            'statcast_pitches', \n",
    "            engine, \n",
    "            if_exists='replace', # CRITICAL: Append new data to the existing table\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully appended {len(df)} new rows of Statcast data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Statcast ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n",
    "# Execute the daily update\n",
    "update_statcast_data(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_statcast_data(start_date, end_date):\n",
    "    \"\"\"Pulls granular, pitch-by-pitch data for a specified date range.\"\"\"\n",
    "    print(f\"-> Pulling Statcast data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # pybaseball statcast function is designed to handle this extraction\n",
    "    raw_statcast_df = pyb.statcast(start_dt=start_date, end_dt=end_date)\n",
    "    \n",
    "    if raw_statcast_df is None or raw_statcast_df.empty:\n",
    "        print(\"Warning: No Statcast data returned for this date range.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return raw_statcast_df\n",
    "\n",
    "\n",
    "# Example\n",
    "test_start_date = '2025-10-28'\n",
    "test_end_date = '2025-10-30' \n",
    "\n",
    "daily_data = extract_statcast_data(test_start_date, test_end_date)\n",
    "print(f\"Successfully extracted {len(daily_data)} individual pitches/events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_pk: Integer. Game id provided by MLB Advanced Media.\n",
    "# get statcast data for game_pk \n",
    "game_log = pyb.statcast_single_game(813024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b679c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_lahman = pylahman.People()\n",
    "player_chadwick = pyb.chadwick_register()\n",
    "\n",
    "# Join lahman and chadwick on key identifiers and bring all the columns from lahman\n",
    "# Ignore if key_bbref is empty in chadwick\n",
    "players_chadwick_clean = player_chadwick[player_chadwick['key_retro'].notna()]\n",
    "players_lahman_clean   = players_lahman[players_lahman['retroID'].notna()]\n",
    "\n",
    "players_df = pd.merge(\n",
    "    players_chadwick_clean,\n",
    "    players_lahman_clean,\n",
    "    left_on=['key_retro'],\n",
    "    right_on=['retroID'],\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "# Remove unnecesary columns and drop them from the dataframe\n",
    "cols_to_remove = ['retroID', 'bbrefID', 'mlb_played_first', 'mlb_played_last']\n",
    "players_df = players_df.drop(columns= cols_to_remove)\n",
    "\n",
    "# Rename the fields\n",
    "rename_map = {\n",
    "    # IDs\n",
    "    \"key_mlbam\":     \"key_mlbam\",\n",
    "    \"key_retro\":     \"key_retro\",\n",
    "    \"key_bbref\":     \"key_bbref\",\n",
    "    \"key_fangraphs\": \"key_fangraphs\",\n",
    "    \"ID\":            \"id_lahman\",\n",
    "    \"playerID\":      \"player_id_lahman\",\n",
    "\n",
    "    # Names\n",
    "    \"name_last\":     \"last_name_chadwick\",\n",
    "    \"name_first\":    \"first_name_chadwick\",\n",
    "    \"nameLast\":      \"last_name_lahman\",\n",
    "    \"nameFirst\":     \"first_name_lahman\",\n",
    "    \"nameGiven\":     \"first_and_second_name_lahman\",\n",
    "\n",
    "    # Debut/Final game\n",
    "    \"debut\":         \"debut\",\n",
    "    \"finalGame\":     \"final_game\",\n",
    "\n",
    "    # Info\n",
    "    \"weight\":        \"weight\",\n",
    "    \"height\":        \"height\",\n",
    "    \"bats\":          \"bats\",\n",
    "    \"throws\":        \"throws\",\n",
    "\n",
    "    # Birth/Death\n",
    "    \"birthYear\":     \"birth_year\",\n",
    "    \"birthMonth\":    \"birth_month\",\n",
    "    \"birthDay\":      \"birth_day\",\n",
    "    \"birthCity\":     \"birth_city\",\n",
    "    \"birthCountry\":  \"birth_country\",\n",
    "    \"birthState\":    \"birth_state\",\n",
    "    \"deathYear\":     \"death_year\",\n",
    "    \"deathMonth\":    \"death_month\",\n",
    "    \"deathDay\":      \"death_day\",\n",
    "    \"deathCountry\":  \"death_country\",\n",
    "    \"deathState\":    \"death_state\",\n",
    "    \"deathCity\":     \"death_city\",\n",
    "}\n",
    "\n",
    "# Apply the rename\n",
    "players_df = players_df.rename(columns= rename_map)\n",
    "\n",
    "# Order the new columns\n",
    "ordered_cols = [\n",
    "    \"key_mlbam\",\n",
    "    \"key_retro\",\n",
    "    \"key_bbref\",\n",
    "    \"key_fangraphs\",\n",
    "    \"id_lahman\",\n",
    "    \"player_id_lahman\",\n",
    "    \"last_name_chadwick\",\n",
    "    \"first_name_chadwick\",\n",
    "    \"last_name_lahman\",\n",
    "    \"first_name_lahman\",\n",
    "    \"first_and_second_name_lahman\",\n",
    "    \"debut\",\n",
    "    \"final_game\",\n",
    "    \"weight\",\n",
    "    \"height\",\n",
    "    \"bats\",\n",
    "    \"throws\",\n",
    "    \"birth_year\",\n",
    "    \"birth_month\",\n",
    "    \"birth_day\",\n",
    "    \"birth_city\",\n",
    "    \"birth_country\",\n",
    "    \"birth_state\",\n",
    "    \"death_year\",\n",
    "    \"death_month\",\n",
    "    \"death_day\",\n",
    "    \"death_country\",\n",
    "    \"death_state\",\n",
    "    \"death_city\"\n",
    "]\n",
    "\n",
    "# Apply the order\n",
    "players_df = players_df[ordered_cols]\n",
    "\n",
    "# This selects only columns with numbers and fills their nulls with -1\n",
    "numeric_cols = players_df.select_dtypes(include=['number']).columns\n",
    "players_df[numeric_cols] = players_df[numeric_cols].fillna(-1)\n",
    "\n",
    "# Replace nulls in the text columns\n",
    "text_cols = [\n",
    "    \"key_retro\",\n",
    "    \"key_bbref\",\n",
    "    \"player_id_lahman\",\n",
    "    \"last_name_chadwick\",\n",
    "    \"first_name_chadwick\",\n",
    "    \"last_name_lahman\",\n",
    "    \"first_name_lahman\",\n",
    "    \"first_and_second_name_lahman\",\n",
    "    \"bats\",\n",
    "    \"throws\",\n",
    "    \"birth_city\",\n",
    "    \"birth_country\",\n",
    "    \"birth_state\",\n",
    "    \"death_country\",\n",
    "    \"death_state\",\n",
    "    \"death_city\"\n",
    "]\n",
    "\n",
    "# Convert to a standard object type first and then fill the nulls with N/A\n",
    "for col in text_cols:\n",
    "    players_df[col] = players_df[col].astype(object).fillna('N/A')\n",
    "    \n",
    "\n",
    "# List the date columns\n",
    "date_cols = [\n",
    "    \"debut\",\n",
    "    \"final_game\"\n",
    "]\n",
    "# Fill null dates with January 1st, 1700\n",
    "for col in date_cols:\n",
    "    players_df[col] = players_df[col].fillna(pd.Timestamp('1700-01-01'))\n",
    "\n",
    "# Check for nulls in my table - there shouldn't be any\n",
    "if (players_df.isnull().sum() == 0).all():\n",
    "    print(\"‚úÖ No nulls found.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING - There are nulls in some columns in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50721a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify all text columns\n",
    "text_cols = team_franchises.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "# 2. Convert to object FIRST, then fill\n",
    "for col in text_cols:\n",
    "    # Converting to object allows 'N/A' to be treated as a normal string\n",
    "    team_franchises[col] = team_franchises[col].astype(object).fillna('N/A')\n",
    "    \n",
    "    # Just in case some were literal 'nan' strings:\n",
    "    team_franchises[col] = team_franchises[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "# 3. Final Verification with Emojis\n",
    "null_count = team_franchises[text_cols].isnull().sum().sum()\n",
    "if null_count == 0:\n",
    "    print(\"‚úÖ All string columns are clean. No nulls found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {null_count} nulls still remain in text columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info = pylahman.Teams()\n",
    "\n",
    "# Identify all text columns\n",
    "text_cols = team_info.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "# Convert to object first, then fill with N/A\n",
    "for col in text_cols:\n",
    "    # Converting to object allows 'N/A' to be treated as a normal string\n",
    "    team_info[col] = team_info[col].astype(object).fillna('N/A')\n",
    "    \n",
    "    # Just in case some were literal 'nan' strings:\n",
    "    team_info[col] = team_info[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "# This selects only columns with numbers and fills their nulls with -1\n",
    "numeric_cols = team_info.select_dtypes(include=['number']).columns\n",
    "team_info[numeric_cols] = team_info[numeric_cols].fillna(-1)\n",
    "\n",
    "# Final verification\n",
    "null_count_text    = team_info[text_cols].isnull().sum().sum()\n",
    "null_count_numeric = team_info[numeric_cols].isnull().sum().sum()\n",
    "total_nulls        = null_count_text + null_count_numeric\n",
    "\n",
    "if total_nulls == 0:\n",
    "    print(\"‚úÖ All columns are clean. No nulls found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {total_nulls} nulls still remain some columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cff595",
   "metadata": {},
   "source": [
    "### Create model\n",
    "## dim_pitcher_archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dim_pitcher_archetypes updated! Primary Key was preserved.\n"
     ]
    }
   ],
   "source": [
    "#def update_dim_pitcher_archetypes(engine: Engine):\n",
    "\"\"\"\n",
    "Groups pitchers into 8 archetypes and updates the database.\n",
    "Now includes an 'updated_at' column to track the last run date.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Pull unique pitcher stats\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    pitcher,\n",
    "    AVG(release_speed) as avg_velo, \n",
    "    AVG(release_spin_rate) as avg_spin, -- The spin rate of a pitch measured in revolutions per minute (rpm) at the moment of release\n",
    "    AVG(pfx_x) as avg_horiz_mvmt, -- Horizontal movement in feet from the catcher's perspective\n",
    "    AVG(pfx_z) as avg_vert_mvmt -- Vertical movement from the catcher's perpsective.\n",
    "FROM fact_statcast_pitches\n",
    "WHERE release_speed IS NOT NULL \n",
    "    AND release_spin_rate IS NOT NULL\n",
    "    AND pfx_x IS NOT NULL \n",
    "    AND pfx_z IS NOT NULL\n",
    "GROUP BY pitcher\n",
    "HAVING COUNT(*) > 100 \n",
    "\"\"\"\n",
    "pitcher_stats = pd.read_sql(query, engine)\n",
    "\n",
    "# 2. Scale the data\n",
    "scaler = StandardScaler()\n",
    "features = ['avg_velo', 'avg_spin', 'avg_horiz_mvmt', 'avg_vert_mvmt']\n",
    "scaled_data = scaler.fit_transform(pitcher_stats[features])\n",
    "\n",
    "# 3. Create 8 Archetypes\n",
    "kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "pitcher_stats['archetype_id'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# 4. Map IDs and Add Timestamp\n",
    "archetype_map = {\n",
    "    0: \"Power Flamethrower\",\n",
    "    1: \"Sinker / Tail Specialist\",\n",
    "    2: \"Breaking Ball Specialist\",\n",
    "    3: \"Standard Control Righty\",\n",
    "    4: \"Position Player / Eephus\",\n",
    "    5: \"Deceptive Angle Specialist\",\n",
    "    6: \"Low-Spin / Heavy Sinker\",\n",
    "    7: \"Power Slider / Sweeper\"\n",
    "}\n",
    "pitcher_stats['archetype_name'] = pitcher_stats['archetype_id'].map(archetype_map)\n",
    "\n",
    "# Add the current timestamp to every row\n",
    "pitcher_stats['updated_at'] = datetime.now()\n",
    "\n",
    "# 5. Database Update (Truncate and Append)\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        conn.execute(text(\"TRUNCATE TABLE dim_pitcher_archetypes;\"))\n",
    "        conn.commit()\n",
    "        print(\"Refreshing existing dim_pitcher_archetypes table...\")\n",
    "    except Exception:\n",
    "        print(\"Table 'dim_pitcher_archetypes' not found. Creating it for the first time...\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Upload data including the new column\n",
    "pitcher_stats[['pitcher', 'archetype_id', 'archetype_name', 'updated_at']].to_sql(\n",
    "    'dim_pitcher_archetypes', \n",
    "    engine, \n",
    "    if_exists='append', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "# 6. Ensure the Primary Key is set\n",
    "pk_check = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM information_schema.table_constraints \n",
    "WHERE table_name='dim_pitcher_archetypes' AND constraint_type='PRIMARY KEY';\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    has_pk = conn.execute(text(pk_check)).scalar()\n",
    "    if has_pk == 0:\n",
    "        conn.execute(text(\"ALTER TABLE dim_pitcher_archetypes ADD PRIMARY KEY (pitcher);\"))\n",
    "        conn.commit()\n",
    "        print(\"‚úÖ Primary Key (pitcher) established.\")\n",
    "\n",
    "print(f\"‚úÖ Successfully categorized {len(pitcher_stats)} pitchers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970acd7",
   "metadata": {},
   "source": [
    "### \"Luck Score\" ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a807f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_luck_scores(engine, min_ab= 25):\n",
    "    query = \"SELECT * FROM vw_batter_vs_pitcher_archetype\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    #? Filter by At-Bats first to ensure statistical significance\n",
    "    # Even though the SQL view has a filter, we can tighten it here if needed\n",
    "    df = df[df['at_bats'] >= min_ab].copy()\n",
    "    \n",
    "    #? Calculate Ranks (Percentiles)\n",
    "    df['ev_rank'] = df['avg_exit_velo'].rank(pct=True)\n",
    "    df['speed_rank'] = df['avg_sprint_speed'].rank(pct=True)\n",
    "    \n",
    "    # Lower time is BETTER for home_to_first, so we rank descending\n",
    "    df['h1_rank'] = df['avg_home_to_first'].rank(pct=True, ascending=False)\n",
    "    df['ba_rank'] = df['batting_avg'].rank(pct=True)\n",
    "    \n",
    "    # 3. Weighting the \"Potential\" (Expected Performance)\n",
    "    # 60% Exit Velo, 20% Sprint, 20% Home-to-1st\n",
    "    df['potential_score'] = (df['ev_rank'] * 0.6) + (df['speed_rank'] * 0.2) + (df['h1_rank'] * 0.2)\n",
    "    \n",
    "    #? Luck Score Calculation\n",
    "    # A positive score means their physical tools exceed their actual results\n",
    "    # If High Positive (Unlucky) e.g. 0.40\n",
    "    # The player‚Äôs tools are elite (e.g., 90th percentile), but their results are \n",
    "    # poor (e.g., 50th percentile). They are likely hitting into the \"loudest outs\" in the league.\n",
    "\n",
    "    # If Near Zero (Fair) e.g. 0.05\n",
    "    # The player is getting exactly what they deserve. \n",
    "    # Their speed and power perfectly explain their batting average.\n",
    "\n",
    "    # If High Negative (Lucky) e.g. -0.4:\n",
    "    # The player has weak tools (e.g., 30th percentile) but a high batting average (e.g., 70th percentile). \n",
    "    # They are likely benefiting from \"bloop\" hits, defensive errors.\n",
    "    df['luck_score'] = df['potential_score'] - df['ba_rank']\n",
    "    \n",
    "    #? Sample Size Adjustment\n",
    "    # This 'confidence' metric tells you how much you should trust the luck score\n",
    "    \n",
    "    # 0.90 to 1.0 -> High Confidence. This player has faced this archetype many times. \n",
    "    # The luck score is likely a true reflection of their performance.\n",
    "    \n",
    "    # 0.5 -> Moderate. There is enough data to see a trend, \n",
    "    # but it could still be swayed by a single lucky or unlucky game.\n",
    "    \n",
    "    # 0.1 or lower -> Low Certainty. The player has very few At-Bats against this archetype. \n",
    "    # The high luck score might just be \"small-sample noise\".\n",
    "    df['luck_confidence'] = df['at_bats'] / df['at_bats'].max()\n",
    "    \n",
    "    # Add the current timestamp to every row\n",
    "    df['calculation_date'] = datetime.now()\n",
    "    \n",
    "    # Sort by the luckiest (most underperforming) players first\n",
    "    return df.sort_values('luck_score', ascending=False)\n",
    "\n",
    "luck_df = calculate_luck_scores(engine, min_ab= 25)\n",
    "luck_df = luck_df[['batter', 'archetype_name', 'luck_score', 'luck_confidence', 'at_bats', 'calculation_date']]\n",
    "# Example usage:\n",
    "# luck_df = calculate_luck_scores(engine, min_ab=30)\n",
    "# print(luck_df[['batter', 'archetype_name', 'at_bats', 'luck_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297d241",
   "metadata": {},
   "source": [
    "# Get probable starters for the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   game_id              home_team           away_team    home_pitcher  \\\n",
      "0   777505         Detroit Tigers     Cincinnati Reds    Tyler Holton   \n",
      "1   777501      Baltimore Orioles  Los Angeles Angels   Scott Blewett   \n",
      "2   777499  Philadelphia Phillies   Toronto Blue Jays    Zack Wheeler   \n",
      "3   777504         Atlanta Braves    Colorado Rockies    Grant Holmes   \n",
      "4   777503   Washington Nationals       Miami Marlins  MacKenzie Gore   \n",
      "\n",
      "    away_pitcher status                        venue  \n",
      "0     Wade Miley  Final                Comerica Park  \n",
      "1  Yusei Kikuchi  Final  Oriole Park at Camden Yards  \n",
      "2   Jos√© Berr√≠os  Final           Citizens Bank Park  \n",
      "3  Austin Gomber  Final                  Truist Park  \n",
      "4     Eury P√©rez  Final               Nationals Park  \n"
     ]
    }
   ],
   "source": [
    "import statsapi\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get today's date automatically\n",
    "today_str = datetime.now().strftime('%m/%d/%Y')\n",
    "\n",
    "def get_daily_starters(date_str):\n",
    "    # 1. Fetch all games for the specified date\n",
    "    # No team ID means it pulls all 30 teams\n",
    "    schedule = statsapi.schedule(date=date_str)\n",
    "    \n",
    "    games_list = []\n",
    "    \n",
    "    for game in schedule:\n",
    "        # Extract the core info for your Matchup Predictor\n",
    "        game_data = {\n",
    "            \"game_id\": game.get(\"game_id\"),\n",
    "            \"home_team\": game.get(\"home_name\"),\n",
    "            \"away_team\": game.get(\"away_name\"),\n",
    "            \"home_pitcher\": game.get(\"home_probable_pitcher\", \"TBD\"),\n",
    "            \"away_pitcher\": game.get(\"away_probable_pitcher\", \"TBD\"),\n",
    "            \"status\": game.get(\"status\"),\n",
    "            \"venue\": game.get(\"venue_name\")\n",
    "        }\n",
    "        games_list.append(game_data)\n",
    "    \n",
    "    # 2. Convert to DataFrame\n",
    "    df = pd.DataFrame(games_list)\n",
    "    return df\n",
    "\n",
    "# Test with June 15, 2025\n",
    "df_starters = get_daily_starters('6/15/2025')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_starters.head())\n",
    "\n",
    "# Integrating with your \"Matchup Predictor\"\n",
    "# Now that you have this DataFrame, you can loop through the home_pitcher and away_pitcher columns.\n",
    "\n",
    "# For each name:\n",
    "\n",
    "# Look up their Archetype in your database.\n",
    "\n",
    "# Highlight the \"Best Matchups\" for that day \n",
    "# (e.g., \"Today, 3 teams are facing 'Soft Tossers'‚Äîcheck your O's hitters' \n",
    "# luck scores against that archetype\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad2c38",
   "metadata": {},
   "source": [
    "### Get today's lineup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5579ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   game_id            team  batting_order        player_name  player_id  \\\n",
      "0   777505  Detroit Tigers              1    Kerry Carpenter     681481   \n",
      "1   777505  Detroit Tigers              2     Gleyber Torres     650402   \n",
      "2   777505  Detroit Tigers              3       Riley Greene     682985   \n",
      "3   777505  Detroit Tigers              4     Dillon Dingler     693307   \n",
      "4   777505  Detroit Tigers              5  Spencer Torkelson     679529   \n",
      "5   777505  Detroit Tigers              6     Zach McKinstry     656716   \n",
      "6   777505  Detroit Tigers              7      Wenceel P√©rez     672761   \n",
      "7   777505  Detroit Tigers              8        Javier B√°ez     595879   \n",
      "8   777505  Detroit Tigers              9         Colt Keith     690993   \n",
      "\n",
      "  position  \n",
      "0       DH  \n",
      "1       2B  \n",
      "2       LF  \n",
      "3        C  \n",
      "4       1B  \n",
      "5       3B  \n",
      "6       RF  \n",
      "7       SS  \n",
      "8       PH  \n"
     ]
    }
   ],
   "source": [
    "import statsapi\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get today's date automatically\n",
    "today_str = datetime.now().strftime('%m/%d/%Y')\n",
    "\n",
    "def get_lineups_to_df(date_str):\n",
    "    # 1. Get all games for the day to find game_ids\n",
    "    schedule = statsapi.schedule(date=date_str)\n",
    "    \n",
    "    lineup_records = []\n",
    "\n",
    "    for game in schedule:\n",
    "        game_id = game['game_id']\n",
    "        team_names = {\n",
    "            'home': game['home_name'],\n",
    "            'away': game['away_name']\n",
    "        }\n",
    "        \n",
    "        # 2. Pull boxscore data which contains the lineup\n",
    "        try:\n",
    "            box = statsapi.boxscore_data(game_id)\n",
    "            \n",
    "            for side in ['home', 'away']:\n",
    "                # The 'battingOrder' list contains player IDs in order (1-9)\n",
    "                order_ids = box[side].get('battingOrder', [])\n",
    "                \n",
    "                for slot, p_id in enumerate(order_ids, start=1):\n",
    "                    player_info = box[side]['players'][f\"ID{p_id}\"]\n",
    "                    \n",
    "                    lineup_records.append({\n",
    "                        \"game_id\": game_id,\n",
    "                        \"team\": team_names[side],\n",
    "                        \"batting_order\": slot,\n",
    "                        \"player_name\": player_info['person']['fullName'],\n",
    "                        \"player_id\": p_id,\n",
    "                        \"position\": player_info['position']['abbreviation']\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Lineup not yet available for {team_names['away']} @ {team_names['home']}\")\n",
    "\n",
    "    return pd.DataFrame(lineup_records)\n",
    "\n",
    "# Test for a game day (using your June 15 example)\n",
    "df_lineups = get_lineups_to_df('06/15/2025')\n",
    "print(df_lineups.head(9)) # Show the lead-off through 9th hitter for the first game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "739bea17",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['player_id', 'avg_exit_velo'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Player's in the lineup that are due for a hit based on their luck score\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming your Luck Score table is loaded as df_luck\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_today_luck \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m      5\u001b[0m     df_lineups, \n\u001b[1;32m----> 6\u001b[0m     \u001b[43mluck_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplayer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mluck_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_exit_velo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, \n\u001b[0;32m      7\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Display the unluckiest players in today's lineup (due for a hit!)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m due_for_hit \u001b[38;5;241m=\u001b[39m df_today_luck\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mluck_score\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4118\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4119\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4121\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['player_id', 'avg_exit_velo'] not in index\""
     ]
    }
   ],
   "source": [
    "# Player's in the lineup that are due for a hit based on their luck score\n",
    "\n",
    "# Assuming your Luck Score table is loaded as df_luck\n",
    "df_today_luck = pd.merge(\n",
    "    df_lineups, \n",
    "    luck_df[['player_id', 'luck_score', 'avg_exit_velo']], \n",
    "    on='player_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Display the unluckiest players in today's lineup (due for a hit!)\n",
    "due_for_hit = df_today_luck.sort_values('luck_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db14998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your local archetype data\n",
    "# (In a real app, this comes from your SQL database)\n",
    "df_archetypes = pd.read_csv('pitcher_archetypes.csv') \n",
    "\n",
    "# 2. Get the daily starters from the previous step\n",
    "df_starters = get_daily_starters('06/15/2025')\n",
    "\n",
    "# 3. Merge for Away Pitchers\n",
    "# We rename columns so we know which archetype belongs to which team\n",
    "df_final = pd.merge(\n",
    "    df_starters, \n",
    "    df_archetypes[['player_name', 'archetype']], \n",
    "    left_on='away_pitcher', \n",
    "    right_on='player_name', \n",
    "    how='left'\n",
    ").rename(columns={'archetype': 'away_pitcher_type'}).drop(columns=['player_name'])\n",
    "\n",
    "# 4. Merge for Home Pitchers\n",
    "df_final = pd.merge(\n",
    "    df_final, \n",
    "    df_archetypes[['player_name', 'archetype']], \n",
    "    left_on='home_pitcher', \n",
    "    right_on='player_name', \n",
    "    how='left'\n",
    ").rename(columns={'archetype': 'home_pitcher_type'}).drop(columns=['player_name'])\n",
    "\n",
    "# Display the result\n",
    "print(df_final[['away_team', 'away_pitcher', 'away_pitcher_type', 'home_team', 'home_pitcher', 'home_pitcher_type']])\n",
    "\n",
    "# What this enables in your Dashboard\n",
    "# Now that you have the pitcher_type in your DataFrame, you can add \"Smart Alerts\" to your UI:\n",
    "\n",
    "# Filter Logic: if row['home_pitcher_type'] == 'Breaking Ball Specialist':\n",
    "\n",
    "# UI Trigger: Display a message: \"Gunnar Henderson struggles against this pitcher type. Watch out for low-away sliders today.\"\n",
    "\n",
    "# Heatmap Update: Automatically filter your Matchup Heatmap to only show the column for that specific archetype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7220e",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d7f04",
   "metadata": {},
   "source": [
    "# Pitcher archetype model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51108be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_pitcher_clusters(df):\n",
    "    \"\"\"\n",
    "    Creates a 2D scatter plot of the clusters and a distribution breakdown.\n",
    "    \"\"\"\n",
    "    # 1. Prepare Data (Same features used in the GMM)\n",
    "    model_features = [\n",
    "        'ffour_usage', 'sinker_usage', 'bb_usage', 'offspeed_usage',\n",
    "        'ffour_vaa_pct', 'velo_gap_pct', 'command_pct', 'neutrality_pct'\n",
    "    ]\n",
    "    \n",
    "    # Scale and Reduce Dimensions\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[model_features].fillna(0))\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_results = pca.fit_transform(scaled_data)\n",
    "    df['pca_1'] = pca_results[:, 0]\n",
    "    df['pca_2'] = pca_results[:, 1]\n",
    "\n",
    "    # 2. Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    scatter = sns.scatterplot(\n",
    "        x='pca_1', y='pca_2', \n",
    "        hue='style_cluster', \n",
    "        style='is_unicorn',\n",
    "        data=df, \n",
    "        palette='viridis', \n",
    "        s=100, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add labels for specific \"Unicorns\" or top pitchers\n",
    "    top_pitchers = df.nlargest(5, 'stuff_score')\n",
    "    for i, row in top_pitchers.iterrows():\n",
    "        plt.text(row['pca_1'] + 0.02, row['pca_2'], row['full_name'], fontsize=9)\n",
    "\n",
    "    plt.title('MLB Pitcher Archetype Clusters (PCA Projection)', fontsize=15)\n",
    "    plt.xlabel('Principal Component 1 (Style Variance)')\n",
    "    plt.ylabel('Principal Component 2 (Mechanical Variance)')\n",
    "    plt.legend(title='Cluster ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Print Cluster Summary\n",
    "    print(\"### Cluster Archetype Breakdown ###\")\n",
    "    summary = df.groupby('style_cluster')[model_features].mean().round(1)\n",
    "    print(summary)\n",
    "\n",
    "# Call the function\n",
    "visualize_pitcher_clusters(pitcher_archetypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95215c7b",
   "metadata": {},
   "source": [
    "### Test new archetype for pitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f61468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_scouting_model(df):\n",
    "    \"\"\"\n",
    "    Synthesizes pitching identity (GMM) with performance outcomes (Whiff/Barrel),\n",
    "    Perceived Power (Extension), and Vertical Separation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identity Features (Physical & Tactical only)\n",
    "    identity_features = [\n",
    "        'ffour_usage', 'sinker_usage', 'bb_usage', 'offspeed_usage',\n",
    "        'ffour_vaa_pct', 'sinker_vaa_pct', 'bb_vaa_pct', 'offspeed_vaa_pct',\n",
    "        'velo_gap_pct', 'command_pct', 'paint_pct'\n",
    "    ]\n",
    "    \n",
    "    # 2. Effectiveness Scores\n",
    "    df['lethality_score'] = (\n",
    "        (df['whiff_pct'] * 0.75) + \n",
    "        (df['suppression_pct'] * 0.20) + \n",
    "        (df['velo_pct'] * 0.05)\n",
    "    ).round(1)\n",
    "\n",
    "    # 3. Clustering (Archetype Definition)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_identity = scaler.fit_transform(df[identity_features].fillna(0))\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=7, random_state=42)\n",
    "    df['style_cluster'] = gmm.fit_predict(scaled_identity)\n",
    "\n",
    "    # 4. Outlier Detection (Unicorns)\n",
    "    iso = IsolationForest(contamination=0.04, random_state=42)\n",
    "    df['is_unicorn'] = iso.fit_predict(scaled_identity)\n",
    "    \n",
    "    \n",
    "    def calculate_pitcher_grade(row):\n",
    "        # 1. STUFF+ (Physicality)\n",
    "        # This is the \"Weapon\" - 60% of the overall grade\n",
    "        stuff_plus = row['stuff_plus_pct']\n",
    "        \n",
    "        # 2. LOCATION+ (Surgicality)\n",
    "        # This is the \"Aim\" - 40% of the overall grade\n",
    "        location_plus = row['location_plus_pct']\n",
    "        \n",
    "        # 3. PITCHING+ (The Master Score)\n",
    "        # Note: We weigh Stuff higher because it's harder to find/teach\n",
    "        base_score = (stuff_plus * 0.60) + (location_plus * 0.40)\n",
    "        \n",
    "        # 4. VOLUME/STARTER ADJUSTMENTS\n",
    "        if row['is_starter'] == 1:\n",
    "            base_score += 5  # The \"Skubal Boost\"\n",
    "        \n",
    "        if row['total_appearances'] < 5:\n",
    "            base_score -= 10 # The \"Sample Size Penalty\"\n",
    "\n",
    "        # 5. FINAL LETTER GRADE\n",
    "        if base_score >= 85: \n",
    "            grade = 'A+'\n",
    "        elif base_score >= 75: \n",
    "            grade = 'A'\n",
    "        elif base_score >= 60: \n",
    "            grade = 'B'\n",
    "        elif base_score >= 45: \n",
    "            grade = 'C'\n",
    "        else: \n",
    "            grade = 'F'\n",
    "            \n",
    "        return grade, stuff_plus, location_plus\n",
    "\n",
    "    #df['overall_grade'] = df.apply(calculate_pitcher_grade, axis=1)\n",
    "    df[['overall_grade', 'stuff_plus_final', 'location_plus_final']] = df.apply(\n",
    "        lambda x: pd.Series(calculate_pitcher_grade(x)), axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    def generate_scouting_report(row):\n",
    "        tags = []\n",
    "        # Start the summary with the Grade and Handedness\n",
    "        summary_header = f\"[{row['overall_grade']} GRADE] ({row['hand']})\"\n",
    "        \n",
    "        # 1. CORE IDENTITY TAGS (Physicality)\n",
    "        s_plus = row['stuff_plus_pct']\n",
    "        l_plus = row['location_plus_pct']\n",
    "        \n",
    "        if s_plus >= 90: tags.append(\"üí£ PURE FILTH\")\n",
    "        elif s_plus <= 20: tags.append(\"üìâ LACKS BITE\")\n",
    "\n",
    "        if l_plus >= 90: tags.append(\"üéØ SURGEON\")\n",
    "        elif l_plus <= 20: tags.append(\"üèπ WILD THING\")\n",
    "\n",
    "        # 2. MATCHUP TACTICS (New Logic)\n",
    "        # We use the columns we just built in SQL\n",
    "        profile = row['attack_profile']\n",
    "        role = row['matchup_role']\n",
    "        platoon = row['platoon_identity']\n",
    "        \n",
    "        # Build the Narrative Summary\n",
    "        analysis = f\"Identified as a {role}. \"\n",
    "        \n",
    "        if \"NORTH-SOUTH\" in profile:\n",
    "            analysis += \"Wins vertically with high-carry fastballs; elite matchup against low-ball hitters. \"\n",
    "        elif \"EAST-WEST\" in profile:\n",
    "            analysis += \"Heavy horizontal movement profile; ideal for inducing double plays. \"\n",
    "        \n",
    "        if platoon == \"MATCHUP PROOF\":\n",
    "            tags.append(\"üõ°Ô∏è PLATOON NEUTRAL\")\n",
    "            analysis += \"Maintains effectiveness regardless of batter handedness. \"\n",
    "        elif platoon == \"PLATOON SENSITIVE\":\n",
    "            tags.append(\"‚ö†Ô∏è SPLIT RISK\")\n",
    "            analysis += \"Performance drops significantly against opposite-handed hitters. \"\n",
    "\n",
    "        # 3. SPECIAL TRAITS\n",
    "        if row['tunnel_pct'] >= 90: tags.append(\"üß¨ TUNNELER\")\n",
    "        if row['is_unicorn'] == -1: tags.append(\"ü¶Ñ UNICORN\")\n",
    "        if row['breakout_potential'] != 'OPTIMIZED':\n",
    "            tags.append(\"üöÄ BREAKOUT\")\n",
    "            analysis += f\"Tactical Alert: {row['breakout_potential']}. \"\n",
    "\n",
    "        # Create the final string\n",
    "        tag_str = \" | \".join(list(set(tags)))\n",
    "        final_summary = f\"{summary_header} {tag_str} ‚Äî {analysis.strip()}\"\n",
    "        \n",
    "        return tag_str, final_summary\n",
    "\n",
    "    # Apply to your DataFrame\n",
    "    results = df.apply(generate_scouting_report, axis=1)\n",
    "    df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "\n",
    "    # Apply and split into two columns\n",
    "    results = df.apply(generate_scouting_report, axis=1)\n",
    "    df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_dim_pitcher_archetypes(engine):\n",
    "    \"\"\"\n",
    "    SQL to extract the necessary metrics for the Python model.\n",
    "    \"\"\"\n",
    "    query = text(\"\"\"\n",
    "    WITH attack_zone_stats AS (\n",
    "    SELECT \n",
    "        p.*,\n",
    "        -- Define Command/Paint Zones\n",
    "        CASE \n",
    "            WHEN ABS(p.plate_x) <= 0.67 AND p.plate_z BETWEEN (p.sz_bot + 0.33) AND (p.sz_top - 0.33) THEN 'heart'\n",
    "            WHEN ABS(p.plate_x) <= 1.1 AND p.plate_z BETWEEN (p.sz_bot - 0.33) AND (p.sz_top + 0.33) THEN 'shadow'\n",
    "            WHEN ABS(p.plate_x) <= 1.5 AND p.plate_z BETWEEN (p.sz_bot - 0.75) AND (p.sz_top + 0.75) THEN 'chase'\n",
    "            ELSE 'waste'\n",
    "        END as attack_zone,\n",
    "        CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked', 'missed_bunt') THEN 1 ELSE 0 END as is_whiff,\n",
    "        CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked', 'missed_bunt', 'foul', 'foul_tip', 'hit_into_play') THEN 1 ELSE 0 END as is_swing\n",
    "    FROM fact_statcast_pitches p\n",
    "    ),\n",
    "    vaa_base_calc AS (\n",
    "        SELECT \n",
    "            az.*,\n",
    "            CASE WHEN az.pitch_type IN ('FA', 'FF', 'FC') THEN \n",
    "                -ATAN((az.vz0 + (az.az * ((-az.vy0 - SQRT(az.vy0^2 - (2 * az.ay * (50 - (17/12))))) / az.ay))) / \n",
    "                (-SQRT(az.vy0^2 - (2 * az.ay * (50 - (17/12)))))) * (180/3.14159) \n",
    "            END as individual_ff_vaa\n",
    "        FROM attack_zone_stats az\n",
    "    ),\n",
    "    aggregated_stats AS (\n",
    "        SELECT \n",
    "            p.pitcher,\n",
    "            p.p_throws,\n",
    "            COUNT(*) as total_pitches,        \n",
    "            AVG(p.release_extension) as avg_extension,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed + ((p.release_extension - 6.2) * 2) END)::numeric, 1), 0) as perceived_fb_velo,\n",
    "            (AVG(CASE WHEN p.pitch_type IN ('FA', 'FF') THEN p.pfx_z * 12 END) - \n",
    "            AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'SI') THEN p.pfx_z * 12 END)) as v_break_gap_raw,\n",
    "            \n",
    "            ROUND(100.0 * SUM(p.is_whiff) / NULLIF(SUM(p.is_swing), 0), 2) as whiff_rate_raw,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.launch_speed_angle = 6 THEN 1 ELSE 0 END) / \n",
    "                NULLIF(SUM(CASE WHEN p.type = 'X' THEN 1 ELSE 0 END), 0), 2) as barrel_rate_raw,        \n",
    "            \n",
    "            ROUND(100.0 * SUM(CASE WHEN p.attack_zone = 'shadow' THEN 1 ELSE 0 END) / COUNT(*), 1) as paint_raw,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.attack_zone IN ('shadow', 'chase') THEN 1 ELSE 0 END) / COUNT(*), 1) as command_raw,        \n",
    "            \n",
    "            AVG(CASE WHEN p.stand = 'L' THEN p.estimated_woba_using_speedangle END) as xwoba_vs_lhb,\n",
    "            AVG(CASE WHEN p.stand = 'R' THEN p.estimated_woba_using_speedangle END) as xwoba_vs_rhb,           \n",
    "            \n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed END)::numeric, 1), 0) as fb_velo,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN p.release_speed END)::numeric, 1), 0) as offspeed_velo,                                                                                                                                                                    \n",
    "            \n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FC') THEN 1 ELSE 0 END) / COUNT(*), 1) as ffour_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('SI', 'FT') THEN 1 ELSE 0 END) / COUNT(*), 1) as sinker_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('CU', 'SL', 'KC', 'ST', 'SV', 'CS', 'KN') THEN 1 ELSE 0 END) / COUNT(*), 1) as bb_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN 1 ELSE 0 END) / COUNT(*), 1) as offspeed_usage,        \n",
    "            \n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed END)::numeric - \n",
    "                        AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN p.release_speed END)::numeric, 1), 0) as velo_gap,        \n",
    "            COALESCE(ROUND(AVG(p.individual_ff_vaa)::numeric, 2), 0) as ffour_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('SI', 'FT') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as sinker_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CU', 'SL', 'KC', 'ST', 'SV', 'CS', 'KN') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as bb_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'EP') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as offspeed_vaa,\n",
    "\n",
    "            COUNT(DISTINCT game_pk) as total_appearances,\n",
    "            (COUNT(*) / COUNT(DISTINCT game_pk)) as avg_pitches_per_app,\n",
    "            CASE WHEN (COUNT(*) / COUNT(DISTINCT game_pk)) >= 40 AND COUNT(DISTINCT game_pk) >= 3 THEN 1 ELSE 0 END as is_starter,\n",
    "            \n",
    "            STDDEV(p.release_pos_x) as release_x_std,\n",
    "            STDDEV(p.release_pos_z) as release_z_std,\n",
    "            (STDDEV(p.release_pos_x) + STDDEV(p.release_pos_z)) as tunnel_raw,\n",
    "            AVG(p.individual_ff_vaa - ((-0.68 * p.plate_z) - 3.8)) as vaa_above_expected_raw,       \n",
    "            -- RAW STUFF+ (Process)\n",
    "            ( (AVG(p.release_speed) * 0.4) + (AVG(p.release_extension) * 0.2) + (AVG(ABS(p.pfx_x)) * 12 * 0.2) + (AVG(p.pfx_z) * 12 * 0.2) ) as stuff_raw,\n",
    "            -- RAW LOCATION+ (Process)\n",
    "            ( (SUM(CASE WHEN p.attack_zone = 'shadow' THEN 1 ELSE 0 END)::float / COUNT(*)) * 0.6 + (SUM(CASE WHEN p.attack_zone = 'heart' THEN 0 ELSE 1 END)::float / COUNT(*)) * 0.4 ) as location_raw\n",
    "\n",
    "        FROM vaa_base_calc p\n",
    "        GROUP BY p.pitcher, p.p_throws\n",
    "        HAVING COUNT(*) > 100 AND AVG(p.release_speed) > 84\n",
    "    ),\n",
    "    ranked_stats AS (\n",
    "        SELECT \n",
    "            ast.*,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY fb_velo))::numeric, 2) * 100 as velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY offspeed_velo))::numeric, 2) * 100, 0) as offspeed_velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY velo_gap))::numeric, 2) * 100, 0) as velo_gap_pct,                   \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY whiff_rate_raw))::numeric, 2) * 100 as whiff_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY barrel_rate_raw DESC))::numeric, 2) * 100 as suppression_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY command_raw))::numeric, 2) * 100 as command_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY paint_raw))::numeric, 2) * 100 as paint_pct,            \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY perceived_fb_velo))::numeric, 2) * 100 as perceived_velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0 OR sinker_usage > 0) ORDER BY v_break_gap_raw))::numeric, 2) * 100, 0) as movement_gap_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY avg_extension))::numeric, 2) * 100 as extension_pct,           \n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (ffour_usage > 0) ORDER BY ffour_vaa))::numeric, 2) * 100, 0) as ffour_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (sinker_usage > 0) ORDER BY sinker_vaa DESC))::numeric, 2) * 100, 0) as sinker_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (bb_usage > 0) ORDER BY bb_vaa DESC))::numeric, 2) * 100, 0) as bb_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY offspeed_vaa DESC))::numeric, 2) * 100, 0) as offspeed_vaa_pct,\n",
    "            ROUND((100 - (ABS(COALESCE(xwoba_vs_lhb, 0.320) - COALESCE(xwoba_vs_rhb, 0.320)) * 100))::numeric, 2) as neutrality_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY tunnel_raw DESC))::numeric, 2) * 100 as tunnel_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY vaa_above_expected_raw))::numeric, 2) * 100 as vaa_plus_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY stuff_raw))::numeric, 2) * 100 as stuff_plus_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY location_raw))::numeric, 2) * 100 as location_plus_pct\n",
    "        FROM aggregated_stats ast\n",
    "    )\n",
    "    SELECT \n",
    "        CONCAT(pn.first_name_chadwick, ' ', pn.last_name_chadwick) as full_name,\n",
    "        rs.p_throws as hand,\n",
    "        rs.*,\n",
    "        -- MATCHUP COLUMN 1: ATTACK PROFILE (Rise vs Run)\n",
    "        CASE \n",
    "            WHEN vaa_plus_pct > 75 THEN 'NORTH-SOUTH (High Rise)'\n",
    "            WHEN sinker_usage > 25 THEN 'EAST-WEST (Sinker/Run)'\n",
    "            WHEN movement_gap_pct > 75 THEN 'DECEPTIVE (High Break)'\n",
    "            ELSE 'BALANCED'\n",
    "        END as attack_profile,\n",
    "        -- MATCHUP COLUMN 2: ROLE IDENTITY\n",
    "        CASE \n",
    "            WHEN rs.whiff_pct > 75 AND rs.location_plus_pct > 75 THEN 'DOMINANT ACE'\n",
    "            WHEN rs.whiff_pct > 75 AND rs.location_plus_pct < 40 THEN 'POWER ARMS (High Risk)'\n",
    "            WHEN rs.location_plus_pct > 75 AND rs.whiff_pct < 45 THEN 'PITCH TO CONTACT SURGEON'\n",
    "            ELSE 'ROTATION STABILIZER'\n",
    "        END as matchup_role,\n",
    "        -- MATCHUP COLUMN 3: PLATOON RESISTANCE\n",
    "        CASE \n",
    "            WHEN rs.neutrality_pct > 75 THEN 'MATCHUP PROOF'\n",
    "            WHEN rs.neutrality_pct < 35 THEN 'PLATOON SENSITIVE'\n",
    "            ELSE 'STANDARD SPLITS'\n",
    "        END as platoon_identity,\n",
    "        ROUND((perceived_velo_pct * 0.25 + ffour_vaa_pct * 0.25 + whiff_pct * 0.5), 0) as ffour_quality_score,\n",
    "        ROUND((movement_gap_pct * 0.25 + offspeed_vaa_pct * 0.25 + whiff_pct * 0.5), 0) as offspeed_quality_score,   \n",
    "        CASE \n",
    "            WHEN (ffour_vaa_pct > 80 AND ffour_usage < 20) THEN 'UNDERUSED ELITE FASTBALL'\n",
    "            WHEN (bb_vaa_pct > 80 AND bb_usage < 15) THEN 'UNDERUSED ELITE BREAKING'\n",
    "            WHEN (offspeed_vaa_pct > 80 AND offspeed_usage < 15) THEN 'UNDERUSED ELITE OFFSPD'\n",
    "            ELSE 'OPTIMIZED'\n",
    "        END as breakout_potential\n",
    "    FROM ranked_stats rs\n",
    "    JOIN dim_player pn ON rs.pitcher = pn.key_mlbam\n",
    "    ORDER BY stuff_plus_pct DESC;\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    return run_scouting_model(df)\n",
    "\n",
    "# Execute\n",
    "pitcher_archetypes = update_dim_pitcher_archetypes(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdcda8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Drivers for your Clusters:\n",
      "offspeed_vaa_pct    0.200762\n",
      "sinker_usage        0.177802\n",
      "bb_vaa_pct          0.146355\n",
      "sinker_vaa_pct      0.108379\n",
      "ffour_usage         0.074102\n",
      "offspeed_usage      0.073449\n",
      "ffour_vaa_pct       0.042655\n",
      "bb_usage            0.019695\n",
      "paint_pct           0.017737\n",
      "velo_gap_pct        0.011099\n",
      "command_pct         0.005114\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def explain_clusters_stable(df, model_features):\n",
    "    # 1. Clean Data\n",
    "    X = df[model_features].values\n",
    "    y = df['style_cluster'].values.astype(int)\n",
    "\n",
    "    # 2. Use Random Forest (More stable with various NumPy/SciPy versions)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # 3. Calculate Permutation Importance\n",
    "    # This identifies which features define the clusters\n",
    "    result = permutation_importance(rf, X, y, n_repeats=10, random_state=42)\n",
    "    \n",
    "    # Map importance back to feature names\n",
    "    feature_importance = pd.Series(result.importances_mean, index=model_features)\n",
    "    \n",
    "    return rf, feature_importance\n",
    "\n",
    "# Execute\n",
    "#model_features = ['fb_velo', 'avg_extension', 'vaa_proxy', 'raw_iz_whiff', 'k_bb_rate']\n",
    "model_features = ['ffour_usage', 'sinker_usage', 'bb_usage', 'offspeed_usage',\n",
    "    'ffour_vaa_pct', 'sinker_vaa_pct', 'bb_vaa_pct', 'offspeed_vaa_pct',\n",
    "    'velo_gap_pct', 'command_pct', 'paint_pct']\n",
    "rf_model, trait_importance = explain_clusters_stable(pitcher_archetypes, model_features)\n",
    "\n",
    "print(\"Top Drivers for your Clusters:\")\n",
    "print(trait_importance.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66e5bd",
   "metadata": {},
   "source": [
    "# Hitter archetype model\n",
    "## Needs to be in a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67a87dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_hitter_scouting_model(df):\n",
    "    \"\"\"\n",
    "    Stabilized Hitter Model: Uses Bayesian-weighted metrics to define \n",
    "    archetypes and calculate grades.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identity Features (The DNA)\n",
    "    # We use 'stabilized_ev' and 'neutrality_raw' to ensure cluster stability\n",
    "    identity_features = [\n",
    "        'chase_pct_raw', 'zone_swing_raw', 'first_pitch_swing_raw',\n",
    "        'avg_la', 'pull_pct_raw', 'two_strike_contact_raw',\n",
    "        'woba_vs_hard', 'woba_vs_break', 'woba_vs_offspeed',\n",
    "        'neutrality_raw', 'stabilized_ev'\n",
    "    ]\n",
    "    \n",
    "    # 2. Effectiveness Scores (Grade Components)\n",
    "    # Power is now grounded in stabilized EV\n",
    "    df['power_score'] = (\n",
    "        (df['ev_pct'] * 0.50) + \n",
    "        (df['barrel_pct'] * 0.50)\n",
    "    ).round(1)\n",
    "    \n",
    "    # Eye combines discipline with 'Battle' (2-strike) ability\n",
    "    df['discipline_score'] = (\n",
    "        (df['discipline_pct'] * 0.60) + \n",
    "        (df['battle_pct'] * 0.40)\n",
    "    ).round(1)\n",
    "\n",
    "    # 3. Clustering (Archetype Definition)\n",
    "    scaler = StandardScaler()\n",
    "    # Filling NaNs with league average proxies for safety\n",
    "    scaled_data = scaler.fit_transform(df[identity_features].fillna(df[identity_features].median()))\n",
    "    \n",
    "    # 7 Clusters allows for enough nuance (Sluggers, Pests, Specialists, etc.)\n",
    "    gmm = GaussianMixture(n_components=7, random_state=42, n_init=10)\n",
    "    df['hitter_cluster'] = gmm.fit_predict(scaled_data)\n",
    "\n",
    "    # 4. Outlier Detection (Unicorns)\n",
    "    iso = IsolationForest(contamination=0.03, random_state=42)\n",
    "    df['is_hitter_unicorn'] = iso.fit_predict(scaled_data)\n",
    "    \n",
    "    # # 5. Grading Logic with Sample-Size Penalties\n",
    "    # def calculate_hitter_grade(row):\n",
    "    #     # We use woba_reliability_pct as the 'anchor' for the grade\n",
    "    #     base_score = (\n",
    "    #         (row['power_score'] * 0.40) + \n",
    "    #         (row['woba_reliability_pct'] * 0.40) + \n",
    "    #         (row['discipline_score'] * 0.20)\n",
    "    #     )\n",
    "        \n",
    "    #     # Provisional Penalty: If the sample isn't verified, we cap the upside\n",
    "    #     if \"PROVISIONAL\" in row['data_confidence']:\n",
    "    #         base_score -= 8\n",
    "        \n",
    "    #     if base_score >= 88: return 'A+'\n",
    "    #     elif base_score >= 78: return 'A'\n",
    "    #     elif base_score >= 65: return 'B'\n",
    "    #     elif base_score >= 50: return 'C'\n",
    "    #     else: return 'D/F'\n",
    "    \n",
    "    def calculate_scouting_grade(row, mode='hitter'):\n",
    "        \"\"\"\n",
    "        Applies the 20-80 Scouting Scale (Standardized).\n",
    "        A+ = 80 Grade (Generational)\n",
    "        A  = 70 Grade (Elite All-Star)\n",
    "        B  = 60 Grade (Plus)\n",
    "        C  = 50 Grade (Average)\n",
    "        \"\"\"\n",
    "        # 1. Get the combined Z-score from SQL\n",
    "        z_score = row.get('combined_scouting_z', 0)\n",
    "        \n",
    "        # 2. Determine sample size based on player type\n",
    "        if mode == 'hitter':\n",
    "            sample = row.get('total_pitches_faced', 0)\n",
    "        else:\n",
    "            sample = row.get('total_pitches_thrown', 0)\n",
    "            \n",
    "        # 3. Apply Reliability Penalty (Prevents small-sample \"fluke\" A's)\n",
    "        if sample < 500:\n",
    "            z_score -= 1.0  # Moves a lucky 'A' down to a 'B' or 'C'\n",
    "        elif sample < 1000:\n",
    "            z_score -= 0.4  # Slight penalty for unproven consistency\n",
    "            \n",
    "        # 4. Universal Grade Thresholds\n",
    "        if z_score >= 2.4:\n",
    "            return 'A+'\n",
    "        elif z_score >= 1.5:\n",
    "            return 'A'\n",
    "        elif z_score >= 0.6:\n",
    "            return 'B'\n",
    "        elif z_score >= -0.5:\n",
    "            return 'C'\n",
    "        else:\n",
    "            return 'D/F'\n",
    "\n",
    "    # Usage:\n",
    "    df['overall_grade'] = df.apply(lambda x: calculate_scouting_grade(x, mode='hitter'), axis=1)\n",
    "\n",
    "\n",
    "    # 6. Scouting Report Generation\n",
    "    def generate_hitter_report(row):\n",
    "        tags = []\n",
    "        \n",
    "        # Logic for tags\n",
    "        if row['power_score'] >= 90: tags.append(\"üî• ELITE POWER\")\n",
    "        if row['discipline_score'] >= 90: tags.append(\"üéØ DISCIPLINE MASTER\")\n",
    "        if row['two_strike_identity'] == 'ELITE SPOILER': tags.append(\"ü¶ü PEST\")\n",
    "        if row['neutrality_pct'] > 85: tags.append(\"üõ°Ô∏è MATCHUP PROOF\")\n",
    "        if row['is_hitter_unicorn'] == -1: tags.append(\"ü¶Ñ UNICORN\")\n",
    "\n",
    "        # Vertical Profile Analysis\n",
    "        v_desc = f\"Attacks the {row['vertical_profile']}.\"\n",
    "        \n",
    "        # Build Summary\n",
    "        conf_prefix = \"PROVISIONAL: \" if \"PROVISIONAL\" in row['data_confidence'] else \"\"\n",
    "        header = f\"[{row['overall_grade']}] {conf_prefix}{row['full_name']} ({row['hand']})\"\n",
    "        \n",
    "        tag_str = \" | \".join(tags)\n",
    "        body = f\"{v_desc} Handles {row['swing_plane']} path. \"\n",
    "        \n",
    "        # Platoon Insight\n",
    "        if row['neutrality_pct'] < 30:\n",
    "            body += f\"Extreme platoon splits detected; high risk against {row['hand']}HP. \"\n",
    "        else:\n",
    "            body += \"Balanced splits make them difficult to platoon against. \"\n",
    "\n",
    "        return tag_str, f\"{header}\\nTAGS: {tag_str}\\nSUMMARY: {body}\"\n",
    "\n",
    "    df['hitter_tags'], df['hitter_summary'] = zip(*df.apply(generate_hitter_report, axis=1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_dim_hitter_archetypes(engine):\n",
    "    \"\"\"\n",
    "    SQL to extract the necessary metrics for the Python model.\n",
    "    \"\"\"\n",
    "    query = text(\"\"\"\n",
    "    WITH constants AS (\n",
    "        SELECT \n",
    "            0.312 as lg_woba,    -- Actual 2024-25 league average\n",
    "            88.4  as lg_ev,      -- Current league avg exit velocity\n",
    "            0.045 as woba_sd,    -- Fixed Standard Deviation for wOBA\n",
    "            3.8   as ev_sd,      -- Fixed Standard Deviation for Exit Velocity\n",
    "            0.040 as barrel_sd,  -- Fixed Standard Deviation for Barrel Rate\n",
    "            500   as m_woba,     -- Reliability threshold for wOBA\n",
    "            150   as m_ev        -- Reliability threshold for EV\n",
    "    ),\n",
    "    attack_zone_stats AS (\n",
    "        SELECT \n",
    "            p.*,\n",
    "            CASE WHEN p.strikes = 2 THEN 1 ELSE 0 END as is_two_strike,\n",
    "            CASE WHEN p.inning >= 7 AND ABS(p.bat_score - p.fld_score) <= 2 THEN 1 ELSE 0 END as is_clutch,\n",
    "            CASE WHEN p.zone > 9 AND p.description IN ('swinging_strike', 'foul', 'hit_into_play', 'swinging_strike_blocked') THEN 1 ELSE 0 END as is_chase,\n",
    "            CASE WHEN p.zone <= 9 AND p.description IN ('swinging_strike', 'foul', 'hit_into_play', 'swinging_strike_blocked') THEN 1 ELSE 0 END as is_zone_swing,\n",
    "            CASE \n",
    "                WHEN (p.stand = 'R' AND p.hc_x < 125) OR (p.stand = 'L' AND p.hc_x > 125) THEN 1 \n",
    "                ELSE 0 \n",
    "            END as is_pull\n",
    "        FROM fact_statcast_pitches p\n",
    "    ),\n",
    "    hitter_base_stats AS (\n",
    "        SELECT \n",
    "            p.batter,\n",
    "            p.stand AS bat_side,\n",
    "            COUNT(*) AS total_pitches_faced,\n",
    "            COUNT(CASE WHEN p.type = 'X' THEN 1 END) as balls_in_play,\n",
    "            -- Bayesian Stabilized Metrics\n",
    "            ((SUM(p.woba_value) + (MAX(c.m_woba) * MAX(c.lg_woba))) / (COUNT(*) + MAX(c.m_woba))) as stabilized_woba,\n",
    "            ((SUM(p.launch_speed) + (MAX(c.m_ev) * MAX(c.lg_ev))) / (NULLIF(COUNT(CASE WHEN p.type = 'X' THEN 1 END), 0) + MAX(c.m_ev))) as stabilized_ev,\n",
    "            -- Raw Stats for Clustering & Context\n",
    "            AVG(CASE WHEN p.pitch_type IN ('FF', 'FA', 'FT', 'SI', 'FC') THEN p.woba_value END) AS woba_vs_hard,\n",
    "            AVG(CASE WHEN p.pitch_type IN ('SL', 'ST', 'CU', 'KC', 'SV', 'CS', 'GY', 'KN') THEN p.woba_value END) AS woba_vs_break,\n",
    "            AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'EP') THEN p.woba_value END) AS woba_vs_offspeed,            \n",
    "            ROUND(SUM(p.is_chase)::numeric / NULLIF(SUM(CASE WHEN p.zone > 9 THEN 1 ELSE 0 END), 0) * 100, 1) as chase_pct_raw,\n",
    "            ROUND(SUM(p.is_zone_swing)::numeric / NULLIF(SUM(CASE WHEN p.zone <= 9 THEN 1 ELSE 0 END), 0) * 100, 1) as zone_swing_raw,\n",
    "            ROUND(SUM(CASE WHEN p.balls = 0 AND p.strikes = 0 AND p.description IN ('swinging_strike', 'foul', 'hit_into_play') THEN 1 ELSE 0 END)::numeric / \n",
    "                NULLIF(SUM(CASE WHEN p.balls = 0 AND p.strikes = 0 THEN 1 ELSE 0 END), 0) * 100, 1) as first_pitch_swing_raw,\n",
    "            ROUND(SUM(CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked') THEN 1 ELSE 0 END)::numeric / \n",
    "                NULLIF(SUM(CASE WHEN p.description IN ('swinging_strike', 'foul', 'hit_into_play') THEN 1 ELSE 0 END), 0) * 100, 1) as whiff_pct_hitter,      \n",
    "            AVG(p.launch_speed) AS avg_ev,\n",
    "            AVG(p.launch_angle) AS avg_la,\n",
    "            SUM(CASE WHEN p.launch_speed_angle = 6 THEN 1 ELSE 0 END)::float / NULLIF(SUM(CASE WHEN p.type = 'X' THEN 1 ELSE 0 END), 0) AS barrel_rate_raw,\n",
    "            AVG(CASE WHEN p.plate_z > (p.sz_top + p.sz_bot)/2 THEN p.woba_value END) as woba_high_raw,\n",
    "            AVG(CASE WHEN p.plate_z <= (p.sz_top + p.sz_bot)/2 THEN p.woba_value END) as woba_low_raw,      \n",
    "            ROUND(SUM(CASE WHEN p.is_two_strike = 1 AND p.description IN ('foul', 'hit_into_play') THEN 1 ELSE 0 END)::numeric / \n",
    "                NULLIF(SUM(CASE WHEN p.is_two_strike = 1 AND p.description IN ('swinging_strike', 'foul', 'hit_into_play') THEN 1 ELSE 0 END), 0) * 100, 1) as two_strike_contact_raw,       \n",
    "            AVG(CASE WHEN p.is_clutch = 1 THEN p.woba_value END) as clutch_woba_raw,\n",
    "            100 - (ABS(COALESCE(AVG(CASE WHEN p.p_throws = 'L' THEN p.woba_value END), 0.320) - \n",
    "                    COALESCE(AVG(CASE WHEN p.p_throws = 'R' THEN p.woba_value END), 0.320)) * 100) as neutrality_raw,\n",
    "            ROUND(SUM(p.is_pull)::numeric / NULLIF(SUM(CASE WHEN p.type = 'X' THEN 1 ELSE 0 END), 0) * 100, 1) as pull_pct_raw\n",
    "        FROM attack_zone_stats p, constants c\n",
    "        GROUP BY p.batter, p.stand\n",
    "        HAVING COUNT(*) > 150\n",
    "    ),\n",
    "    hitter_ranked AS (\n",
    "        SELECT \n",
    "            hb.*,\n",
    "            -- UNIVERSAL Z-SCORE CALCULATION\n",
    "            (hb.stabilized_ev - 88.5) / 3.5 as ev_z,\n",
    "            (hb.stabilized_woba - 0.320) / 0.045 as woba_z,\n",
    "            (hb.barrel_rate_raw - 0.075) / 0.040 as barrel_z,       \n",
    "            -- Percentiles (kept for existing columns)\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY avg_ev))::numeric, 2) * 100 AS ev_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY barrel_rate_raw))::numeric, 2) * 100 AS barrel_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY stabilized_woba))::numeric, 2) * 100 AS woba_reliability_pct,\n",
    "            ROUND((1 - PERCENT_RANK() OVER (ORDER BY chase_pct_raw))::numeric, 2) * 100 AS discipline_pct,     \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY first_pitch_swing_raw))::numeric, 2) * 100 AS aggression_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY woba_high_raw))::numeric, 2) * 100 AS high_ball_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY woba_low_raw))::numeric, 2) * 100 AS low_ball_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY pull_pct_raw))::numeric, 2) * 100 AS pull_pct,      \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY two_strike_contact_raw))::numeric, 2) * 100 AS battle_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY clutch_woba_raw))::numeric, 2) * 100 AS clutch_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY neutrality_raw))::numeric, 2) * 100 as neutrality_pct,       \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY woba_vs_hard))::numeric, 2) * 100 as woba_hard_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY woba_vs_break))::numeric, 2) * 100 as woba_break_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY woba_vs_offspeed))::numeric, 2) * 100 as woba_offspeed_pct\n",
    "        FROM hitter_base_stats hb\n",
    "    )\n",
    "    SELECT \n",
    "        CONCAT(pn.first_name_chadwick, ' ', pn.last_name_chadwick) AS full_name,\n",
    "        hr.bat_side as hand,\n",
    "        hr.*,\n",
    "        -- UNIVERSAL ANCHOR: Matches Pitcher Model Scale\n",
    "        ((hr.woba_z * 0.5) + (hr.barrel_z * 0.3) + (hr.ev_z * 0.2)) as combined_scouting_z,\n",
    "        CASE \n",
    "            WHEN total_pitches_faced > 1500 THEN 'VERIFIED ELITE SAMPLE'\n",
    "            WHEN total_pitches_faced > 600 THEN 'STABILIZED'\n",
    "            ELSE 'PROVISIONAL (Small Sample)'\n",
    "        END as data_confidence,\n",
    "        CASE \n",
    "            WHEN avg_la > 18 THEN 'UPPERCUT (Flyball)'\n",
    "            WHEN avg_la < 8 THEN 'DOWNWARD (Groundball)'\n",
    "            ELSE 'LEVEL (Line Drive)'\n",
    "        END AS swing_plane,\n",
    "        CASE \n",
    "            WHEN battle_pct > 80 THEN 'ELITE SPOILER'\n",
    "            WHEN battle_pct < 25 THEN 'FREE SWINGER'\n",
    "            ELSE 'STANDARD'\n",
    "        END AS two_strike_identity,\n",
    "        CASE\n",
    "            WHEN high_ball_pct > 75 AND low_ball_pct < 40 THEN 'HIGH-BALL HUNTER'\n",
    "            WHEN low_ball_pct > 75 AND high_ball_pct < 40 THEN 'LOW-BALL GOLFER'\n",
    "            ELSE 'ALL-ZONE THREAT'\n",
    "        END AS vertical_profile\n",
    "    FROM hitter_ranked hr\n",
    "    JOIN dim_player pn ON hr.batter = pn.key_mlbam\n",
    "    ORDER BY combined_scouting_z DESC;\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    return run_hitter_scouting_model(df)\n",
    "\n",
    "# Execute\n",
    "hitter_archetypes = update_dim_hitter_archetypes(engine)\n",
    "\n",
    "# Add the calculation_date\n",
    "hitter_archetypes['calculation_date'] = datetime.now()\n",
    "\n",
    "# Load to SQL\n",
    "hitter_archetypes.to_sql(\n",
    "'dim_hitter_archetypes', \n",
    "engine, \n",
    "if_exists='replace',\n",
    "index=False, \n",
    "chunksize=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fcccf",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4e57ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. THE BRAIN (The Matrix)\n",
    "# Values represent the 'Shift' in win probability for the pitcher.\n",
    "# Positive = Pitcher Adv | Negative = Hitter Adv\n",
    "MATCHUP_MATRIX = {\n",
    "    \"path_geometry\": {\n",
    "        (\"NORTH-SOUTH (High Rise)\", \"LOW-BALL GOLFER\"): 1.5,\n",
    "        (\"NORTH-SOUTH (High Rise)\", \"HIGH-BALL HUNTER\"): -1.5,\n",
    "        (\"EAST-WEST (Sinker/Run)\", \"DOWNWARD (Groundball)\"): 2.0,\n",
    "        (\"EAST-WEST (Sinker/Run)\", \"LEVEL (Line Drive)\"): -1.0,\n",
    "        (\"DECEPTIVE (High Break)\", \"UPPERCUT (Flyball)\"): 1.0,\n",
    "    },\n",
    "    \"skill_clash\": {\n",
    "        (\"DOMINANT ACE\", \"FREE SWINGER\"): 2.0,\n",
    "        (\"PITCH TO CONTACT SURGEON\", \"ELITE SPOILER\"): -2.5,\n",
    "        (\"POWER ARMS (High Risk)\", \"FREE SWINGER\"): 1.5,\n",
    "        (\"ROTATION STABILIZER\", \"STANDARD\"): 0.5,\n",
    "    },\n",
    "    \"platoon_rules\": {\n",
    "        (\"MATCHUP PROOF\", \"STANDARD SPLITS\"): 1.0,\n",
    "        (\"STANDARD SPLITS\", \"MATCHUP PROOF\"): -1.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. THE LOGIC ENGINE (Replaces get_tactical_advantage)\n",
    "def calculate_matchup_edge(pitcher, hitter):\n",
    "    edge_score = 0\n",
    "    reasons = []\n",
    "\n",
    "    # Check Geometry (Attack Profile vs Vertical Profile)\n",
    "    geo_key = (pitcher['attack_profile'], hitter['vertical_profile'])\n",
    "    if geo_key in MATCHUP_MATRIX[\"path_geometry\"]:\n",
    "        score = MATCHUP_MATRIX[\"path_geometry\"][geo_key]\n",
    "        edge_score += score\n",
    "        reasons.append(f\"Geometry: {score}\")\n",
    "\n",
    "    # Check Skill Clash (Role vs 2-Strike Identity)\n",
    "    skill_key = (pitcher['matchup_role'], hitter['two_strike_identity'])\n",
    "    if skill_key in MATCHUP_MATRIX[\"skill_clash\"]:\n",
    "        score = MATCHUP_MATRIX[\"skill_clash\"][skill_key]\n",
    "        edge_score += score\n",
    "        reasons.append(f\"Skill: {score}\")\n",
    "\n",
    "    # Check Platoon Identity\n",
    "    plat_key = (pitcher['platoon_identity'], hitter['platoon_identity'])\n",
    "    if plat_key in MATCHUP_MATRIX[\"platoon_rules\"]:\n",
    "        score = MATCHUP_MATRIX[\"platoon_rules\"][plat_key]\n",
    "        edge_score += score\n",
    "        reasons.append(f\"Platoon: {score}\")\n",
    "\n",
    "    return edge_score, reasons\n",
    "\n",
    "# 3. THE PREDICTOR\n",
    "def predict_lineup_performance(pitcher, lineup_df):\n",
    "    \"\"\"\n",
    "    Simulates a pitcher going through a 9-man lineup.\n",
    "    \"\"\"\n",
    "    total_lineup_edge = 0\n",
    "    \n",
    "    for _, hitter in lineup_df.iterrows():\n",
    "        # Get the tactical edge\n",
    "        edge, _ = calculate_matchup_edge(pitcher, hitter)\n",
    "        \n",
    "        # Factor in the raw talent (Overall Grade)\n",
    "        # We convert grades to numerical values (A+ = 4, F = 0)\n",
    "        grade_map = {'A+': 4, 'A': 3, 'B': 2, 'C': 1, 'D/F': 0}\n",
    "        p_quality = grade_map.get(pitcher['overall_grade'], 1)\n",
    "        h_quality = grade_map.get(hitter['overall_grade'], 1)\n",
    "        \n",
    "        # Talent Gap + Tactical Edge\n",
    "        matchup_final = (p_quality - h_quality) + edge\n",
    "        total_lineup_edge += matchup_final\n",
    "\n",
    "    # Convert to Win Probability (50% is baseline)\n",
    "    # We divide by 9 to get the average edge per hitter, then scale\n",
    "    avg_edge = total_lineup_edge / 9\n",
    "    win_prob = 50 + (avg_edge * 7) # Scaling factor of 7 for variability\n",
    "    \n",
    "    return round(max(min(win_prob, 99), 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c14008da",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'platoon_identity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'platoon_identity'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m my_pitcher \u001b[38;5;241m=\u001b[39m pitcher_archetypes\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Pick Chris Sale / Skubal / etc.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m opposing_lineup \u001b[38;5;241m=\u001b[39m hitter_archetypes\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m9\u001b[39m) \u001b[38;5;66;03m# Or filter for a specific team\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_lineup_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_pitcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopposing_lineup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProjected Win Probability for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_pitcher[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 61\u001b[0m, in \u001b[0;36mpredict_lineup_performance\u001b[1;34m(pitcher, lineup_df)\u001b[0m\n\u001b[0;32m     57\u001b[0m total_lineup_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, hitter \u001b[38;5;129;01min\u001b[39;00m lineup_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Get the tactical edge\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     edge, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_matchup_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpitcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Factor in the raw talent (Overall Grade)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# We convert grades to numerical values (A+ = 4, F = 0)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     grade_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA+\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD/F\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "Cell \u001b[1;32mIn[59], line 44\u001b[0m, in \u001b[0;36mcalculate_matchup_edge\u001b[1;34m(pitcher, hitter)\u001b[0m\n\u001b[0;32m     41\u001b[0m     reasons\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkill: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Check Platoon Identity\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m plat_key \u001b[38;5;241m=\u001b[39m (pitcher[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplatoon_identity\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mhitter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplatoon_identity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plat_key \u001b[38;5;129;01min\u001b[39;00m MATCHUP_MATRIX[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplatoon_rules\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     46\u001b[0m     score \u001b[38;5;241m=\u001b[39m MATCHUP_MATRIX[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplatoon_rules\u001b[39m\u001b[38;5;124m\"\u001b[39m][plat_key]\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\series.py:1133\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\series.py:1249\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1249\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'platoon_identity'"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "my_pitcher = pitcher_archetypes.iloc[0] # Pick Chris Sale / Skubal / etc.\n",
    "opposing_lineup = hitter_archetypes.sample(9) # Or filter for a specific team\n",
    "\n",
    "prob = predict_lineup_performance(my_pitcher, opposing_lineup)\n",
    "print(f\"Projected Win Probability for {my_pitcher['full_name']}: {prob}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
