{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01078c0e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b6a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pybaseball as pyb\n",
    "import pybaseball.cache # Ensure caching is imported\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy import text\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "from rapidfuzz import process\n",
    "import re\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import pylahman\n",
    "import statsapi\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891da24a",
   "metadata": {},
   "source": [
    "### Load environment and connect to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f549c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Build the PostgreSQL connection string\n",
    "DB_URL = f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASS']}@{os.environ['DB_HOST']}:5432/{os.environ['DB_NAME']}\"\n",
    "\n",
    "# Create the engine object for connecting\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "print(\"Database connection established.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036bfa2",
   "metadata": {},
   "source": [
    "### Create dim_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_players(engine: Engine):   \n",
    "    try:\n",
    "        players_lahman = pylahman.People()\n",
    "        player_chadwick = pyb.chadwick_register()\n",
    "\n",
    "        # Join lahman and chadwick on key identifiers and bring all the columns from lahman\n",
    "        # Ignore if key_bbref is empty in chadwick\n",
    "        players_chadwick_clean = player_chadwick[player_chadwick['key_retro'].notna()]\n",
    "        players_lahman_clean   = players_lahman[players_lahman['retroID'].notna()]\n",
    "\n",
    "        players_df = pd.merge(\n",
    "            players_chadwick_clean,\n",
    "            players_lahman_clean,\n",
    "            left_on=['key_retro'],\n",
    "            right_on=['retroID'],\n",
    "            how='left',\n",
    "        )\n",
    "\n",
    "        # Remove unnecesary columns and drop them from the dataframe\n",
    "        cols_to_remove = ['retroID', 'bbrefID', 'mlb_played_first', 'mlb_played_last']\n",
    "        players_df = players_df.drop(columns= cols_to_remove)\n",
    "\n",
    "        # Rename the fields\n",
    "        rename_map = {\n",
    "            # IDs\n",
    "            \"key_mlbam\":     \"key_mlbam\",\n",
    "            \"key_retro\":     \"key_retro\",\n",
    "            \"key_bbref\":     \"key_bbref\",\n",
    "            \"key_fangraphs\": \"key_fangraphs\",\n",
    "            \"ID\":            \"id_lahman\",\n",
    "            \"playerID\":      \"player_id_lahman\",\n",
    "\n",
    "            # Names\n",
    "            \"name_last\":     \"last_name_chadwick\",\n",
    "            \"name_first\":    \"first_name_chadwick\",\n",
    "            \"nameLast\":      \"last_name_lahman\",\n",
    "            \"nameFirst\":     \"first_name_lahman\",\n",
    "            \"nameGiven\":     \"first_and_second_name_lahman\",\n",
    "\n",
    "            # Debut/Final game\n",
    "            \"debut\":         \"debut\",\n",
    "            \"finalGame\":     \"final_game\",\n",
    "\n",
    "            # Info\n",
    "            \"weight\":        \"weight\",\n",
    "            \"height\":        \"height\",\n",
    "            \"bats\":          \"bats\",\n",
    "            \"throws\":        \"throws\",\n",
    "\n",
    "            # Birth/Death\n",
    "            \"birthYear\":     \"birth_year\",\n",
    "            \"birthMonth\":    \"birth_month\",\n",
    "            \"birthDay\":      \"birth_day\",\n",
    "            \"birthCity\":     \"birth_city\",\n",
    "            \"birthCountry\":  \"birth_country\",\n",
    "            \"birthState\":    \"birth_state\",\n",
    "            \"deathYear\":     \"death_year\",\n",
    "            \"deathMonth\":    \"death_month\",\n",
    "            \"deathDay\":      \"death_day\",\n",
    "            \"deathCountry\":  \"death_country\",\n",
    "            \"deathState\":    \"death_state\",\n",
    "            \"deathCity\":     \"death_city\",\n",
    "        }\n",
    "\n",
    "        # Apply the rename\n",
    "        players_df = players_df.rename(columns= rename_map)\n",
    "\n",
    "        # Order the new columns\n",
    "        ordered_cols = [\n",
    "            \"key_mlbam\",\n",
    "            \"key_retro\",\n",
    "            \"key_bbref\",\n",
    "            \"key_fangraphs\",\n",
    "            \"id_lahman\",\n",
    "            \"player_id_lahman\",\n",
    "            \"last_name_chadwick\",\n",
    "            \"first_name_chadwick\",\n",
    "            \"last_name_lahman\",\n",
    "            \"first_name_lahman\",\n",
    "            \"first_and_second_name_lahman\",\n",
    "            \"debut\",\n",
    "            \"final_game\",\n",
    "            \"weight\",\n",
    "            \"height\",\n",
    "            \"bats\",\n",
    "            \"throws\",\n",
    "            \"birth_year\",\n",
    "            \"birth_month\",\n",
    "            \"birth_day\",\n",
    "            \"birth_city\",\n",
    "            \"birth_country\",\n",
    "            \"birth_state\",\n",
    "            \"death_year\",\n",
    "            \"death_month\",\n",
    "            \"death_day\",\n",
    "            \"death_country\",\n",
    "            \"death_state\",\n",
    "            \"death_city\"\n",
    "        ]\n",
    "\n",
    "        # Apply the order\n",
    "        players_df = players_df[ordered_cols]\n",
    "\n",
    "        # This selects only columns with numbers and fills their nulls with -1\n",
    "        numeric_cols = players_df.select_dtypes(include=['number']).columns\n",
    "        players_df[numeric_cols] = players_df[numeric_cols].fillna(-1)\n",
    "\n",
    "        # Replace nulls in the text columns\n",
    "        text_cols = [\n",
    "            \"key_retro\",\n",
    "            \"key_bbref\",\n",
    "            \"player_id_lahman\",\n",
    "            \"last_name_chadwick\",\n",
    "            \"first_name_chadwick\",\n",
    "            \"last_name_lahman\",\n",
    "            \"first_name_lahman\",\n",
    "            \"first_and_second_name_lahman\",\n",
    "            \"bats\",\n",
    "            \"throws\",\n",
    "            \"birth_city\",\n",
    "            \"birth_country\",\n",
    "            \"birth_state\",\n",
    "            \"death_country\",\n",
    "            \"death_state\",\n",
    "            \"death_city\"\n",
    "        ]\n",
    "\n",
    "        # Convert to a standard object type first and then fill the nulls with N/A\n",
    "        for col in text_cols:\n",
    "            players_df[col] = players_df[col].astype(object).fillna('N/A')\n",
    "            \n",
    "\n",
    "        # List the date columns\n",
    "        date_cols = [\n",
    "            \"debut\",\n",
    "            \"final_game\"\n",
    "        ]\n",
    "        # Fill null dates with January 1st, 1700\n",
    "        for col in date_cols:\n",
    "            players_df[col] = players_df[col].fillna(pd.Timestamp('1700-01-01'))\n",
    "\n",
    "        # Check for nulls in my table - there shouldn't be any\n",
    "        if (players_df.isnull().sum() == 0).all():\n",
    "            print(\"‚úÖ No nulls found.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è WARNING - There are nulls in some columns in the dataframe.\")\n",
    "\n",
    "        # # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(players_df)} new rows into 'players'...\")\n",
    "        \n",
    "        players_df.to_sql(\n",
    "            'players', \n",
    "            engine, \n",
    "            if_exists='replace',\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Players successfully added {len(players_df)} new rows of players data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n",
    "# Execute the players function\n",
    "update_players(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1195f56",
   "metadata": {},
   "source": [
    "### Create dim_franchises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_team_franchises(engine: Engine):\n",
    "    try:\n",
    "        # Import the franchises\n",
    "        #? Note: As of 2025-12-18 there is only data up to the 2024 season\n",
    "        team_franchises = pylahman.TeamsFranchises()\n",
    "        \n",
    "        # Data cleaning\n",
    "        # Identify all text columns\n",
    "        text_cols = team_franchises.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "        # Convert to object first, then fill (since the columns are literal strings)\n",
    "        for col in text_cols:\n",
    "            # Converting to object allows 'N/A' to be treated as a normal string\n",
    "            team_franchises[col] = team_franchises[col].astype(object).fillna('N/A')\n",
    "            \n",
    "            # Just in case some were literal 'nan' strings:\n",
    "            team_franchises[col] = team_franchises[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "        # Final verification\n",
    "        null_count = team_franchises[text_cols].isnull().sum().sum()\n",
    "        if null_count == 0:\n",
    "            print(\"‚úÖ All string columns are clean. No nulls found!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: {null_count} nulls still remain in text columns.\")\n",
    "            \n",
    "        \n",
    "        # Loading\n",
    "        print(f\"Loading {len(team_franchises)} new rows into 'team_franchises'...\")\n",
    "        \n",
    "        team_franchises.to_sql(\n",
    "            'team_franchises', \n",
    "            engine, \n",
    "            if_exists='replace',\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Team franchises successfully added {len(team_franchises)} new rows of data.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "        \n",
    "# Apply the function\n",
    "update_team_franchises(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b52b44",
   "metadata": {},
   "source": [
    "### Teams info ***NOT IN USE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe940a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_team_info(engine: Engine):\n",
    "#     try:\n",
    "#         team_info = pylahman.Teams()\n",
    "\n",
    "#         # Identify all text columns\n",
    "#         text_cols = team_info.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "#         # Convert to object first, then fill with N/A\n",
    "#         for col in text_cols:\n",
    "#             # Converting to object allows 'N/A' to be treated as a normal string\n",
    "#             team_info[col] = team_info[col].astype(object).fillna('N/A')\n",
    "            \n",
    "#             # Just in case some were literal 'nan' strings:\n",
    "#             team_info[col] = team_info[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "#         # This selects only columns with numbers and fills their nulls with -1\n",
    "#         numeric_cols = team_info.select_dtypes(include=['number']).columns\n",
    "#         team_info[numeric_cols] = team_info[numeric_cols].fillna(-1)\n",
    "\n",
    "#         # Final verification\n",
    "#         null_count_text    = team_info[text_cols].isnull().sum().sum()\n",
    "#         null_count_numeric = team_info[numeric_cols].isnull().sum().sum()\n",
    "#         total_nulls        = null_count_text + null_count_numeric\n",
    "\n",
    "#         if total_nulls == 0:\n",
    "#             print(\"‚úÖ All columns are clean. No nulls found!\")\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è Warning: {total_nulls} nulls still remain some columns.\")\n",
    "\n",
    "#         # Loading\n",
    "#         print(f\"Loading {len(team_info)} new rows into 'team_info'...\")\n",
    "        \n",
    "#         team_info.to_sql(\n",
    "#             'team_info', \n",
    "#             engine, \n",
    "#             if_exists='replace',\n",
    "#             index=False, \n",
    "#             chunksize=5000\n",
    "#         )\n",
    "        \n",
    "#         print(f\"‚úÖ Team information successfully added {len(team_info)} new rows of data.\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "\n",
    "# # Apply the function\n",
    "# update_team_info(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6532c",
   "metadata": {},
   "source": [
    "### Create fact_team_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 330 new rows into 'team_info'...\n",
      "‚úÖ Team information successfully added 330 new rows of data.\n",
      "Loading 330 new rows into 'team_info'...\n",
      "‚úÖ Team information successfully added 330 new rows of data.\n",
      "Loading 330 new rows into 'team_info'...\n",
      "‚úÖ Team information successfully added 330 new rows of data.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_team_tables(engine: Engine):    \n",
    "    def load_fact_team_tables(engine: Engine, df, category):\n",
    "        try:\n",
    "            table_name = 'fact_team_' + category\n",
    "            print(f\"üíæ Creating {table_name}...\")\n",
    "            \n",
    "            # Loading\n",
    "            print(f\"   üîÉ Loading {len(df)} rows...\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace',\n",
    "                index=False, \n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully added {len(df)} new rows of data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "    # Declare the years\n",
    "    current_year  = date.today().year\n",
    "    ten_years_ago = current_year - 10\n",
    "\n",
    "    # Import the team data for the last 10 years\n",
    "    fact_team_batting  = pyb.team_batting(ten_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_team_pitching = pyb.team_pitching(ten_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_team_fielding = pyb.team_fielding(ten_years_ago, current_year,  ind= 1, qual= 0)\n",
    "\n",
    "    # Apply the function\n",
    "    load_fact_team_tables(engine, fact_team_batting,  'batting')\n",
    "    load_fact_team_tables(engine, fact_team_pitching, 'pitching')\n",
    "    load_fact_team_tables(engine, fact_team_fielding, 'fielding')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f0b3a",
   "metadata": {},
   "source": [
    "### Create fact_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "      ‚¨áÔ∏è  IMPORTING BASEBALL STATS      \n",
      "             Please wait...             \n",
      "========================================\n",
      "\n",
      "üíæ Creating fact_player_batting...\n",
      "   üîÉ Loading 8673 rows...\n",
      "   ‚úÖ Successfully added 8673 new rows of data.\n",
      "üíæ Creating fact_player_pitching...\n",
      "   üîÉ Loading 5106 rows...\n",
      "   ‚úÖ Successfully added 5106 new rows of data.\n",
      "üíæ Creating fact_player_fielding...\n",
      "   üîÉ Loading 13553 rows...\n",
      "   ‚úÖ Successfully added 13553 new rows of data.\n",
      "üíæ Creating fact_player_running...\n",
      "   üîÉ Loading 3830 rows...\n",
      "   ‚úÖ Successfully added 3830 new rows of data.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_player_tables(engine: Engine):    \n",
    "    def load_fact_player_tables(engine: Engine, df, category):\n",
    "        try:\n",
    "            table_name = 'fact_player_' + category\n",
    "            print(f\"üíæ Creating {table_name}...\")\n",
    "            \n",
    "            # Loading\n",
    "            print(f\"   üîÉ Loading {len(df)} rows...\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace',\n",
    "                index=False, \n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully added {len(df)} new rows of data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "\n",
    "    # Declare the years\n",
    "    current_year   = date.today().year\n",
    "    five_years_ago = current_year - 5\n",
    "\n",
    "    # Import the team data for the last 10 years\n",
    "    # print(\"\\n\" + \"=\"*40)\n",
    "    # print(f\"{'‚¨áÔ∏è  Importing player stats':^40}\")\n",
    "    # print(f\"{'Please wait...':^40}\")\n",
    "    # print(\"=\"*40 + \"\\n\")\n",
    "    print(\"‚¨áÔ∏è  Importing player stats... please wait\")\n",
    "    \n",
    "    fact_player_batting  = pyb.batting_stats(five_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_player_pitching = pyb.pitching_stats(five_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    fact_player_fielding = pyb.fielding_stats(five_years_ago, current_year,  ind= 1, qual= 0)\n",
    "    \n",
    "    # Speed tables are by year - they do not include range\n",
    "    # Setup year range\n",
    "    #current_year = datetime.now().year\n",
    "    years = range(current_year - 9, current_year + 1) # Last 10 years including current\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for year in years:\n",
    "        #print(f\"Fetching sprint speed for {year}...\")\n",
    "        try:\n",
    "            # Fetch data\n",
    "            df = pyb.statcast_sprint_speed(year, 50)\n",
    "            \n",
    "            # Adding the years\n",
    "            df['Season'] = year\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch data for {year}: {e}\")\n",
    "\n",
    "    # Combine everything into one fact table\n",
    "    fact_player_running = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Apply the function\n",
    "    load_fact_player_tables(engine, fact_player_batting,  'batting')\n",
    "    load_fact_player_tables(engine, fact_player_pitching, 'pitching')\n",
    "    load_fact_player_tables(engine, fact_player_fielding, 'fielding')\n",
    "    load_fact_player_tables(engine, fact_player_running, 'running')\n",
    "    \n",
    "create_fact_player_tables(engine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical player stats - It has data from 1871 but it doesn't have last year (2025)\n",
    "player_batting_historical     = pylahman.Batting()\n",
    "player_pitching_historical    = pylahman.Pitching()\n",
    "player_fielding_historical    = pylahman.Fielding()\n",
    "player_appearances_historical = pylahman.Appearances()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c652b",
   "metadata": {},
   "source": [
    "### Get scores last n days *NOT IN USE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for all games played from 2025-12-17 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-17 and 2025-12-17.\n",
      "Searching for all games played from 2025-12-11 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-11 and 2025-12-17.\n",
      "Searching for all games played from 2025-12-03 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-12-03 and 2025-12-17.\n",
      "Searching for all games played from 2025-11-18 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No games found between 2025-11-18 and 2025-12-17.\n",
      "Searching for all games played from 2025-10-19 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:03<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 2970 pitch events.\n",
      "Searching for all games played from 2025-09-19 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:04<00:00, 14.14it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 53951 pitch events.\n"
     ]
    }
   ],
   "source": [
    "# def get_game_results_last_n_days(n_days=90):\n",
    "#     \"\"\"\n",
    "#     Pulls raw pitch-by-pitch data for all games played in the last 'n_days' \n",
    "#     and then extracts the final score for each game.\n",
    "#     \"\"\"\n",
    "#     today = date.today()\n",
    "    \n",
    "#     # 1. Calculate the start and end dates for the 90-day range\n",
    "#     end_date = today - timedelta(days=1)  # Search up to yesterday\n",
    "#     start_date = today - timedelta(days=n_days)\n",
    "    \n",
    "#     start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "#     end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     print(f\"Searching for all games played from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "#     try:\n",
    "#         # 2. Pull all pitch-by-pitch data in that range\n",
    "#         all_data_in_range = pyb.statcast(start_dt=start_date_str, end_dt=end_date_str)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error retrieving Statcast data: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     if all_data_in_range.empty:\n",
    "#         print(f\"No games found between {start_date_str} and {end_date_str}.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     print(f\"Successfully pulled {len(all_data_in_range)} pitch events.\")\n",
    "\n",
    "#     # 3. Sort the data chronologically by game_pk, inning, etc.\n",
    "#     data_sorted = all_data_in_range.sort_values(\n",
    "#         by=['game_pk', 'inning', 'inning_topbot', 'at_bat_number', 'pitch_number'],\n",
    "#         ascending=True\n",
    "#     )\n",
    "\n",
    "#     # 4. Group by game_pk and take the last row (which contains the final score)\n",
    "#     final_events = data_sorted.groupby('game_pk').tail(1).reset_index(drop=True)\n",
    "    \n",
    "#     # 5. Extract and rename the relevant columns for the final scoreboard\n",
    "#     scoreboard = final_events[[\n",
    "#         'game_date', \n",
    "#         'home_team', \n",
    "#         'away_team', \n",
    "#         'home_score', \n",
    "#         'away_score'\n",
    "#     ]].copy()\n",
    "    \n",
    "#     scoreboard.rename(columns={\n",
    "#         'home_score': 'Home_Final_Score',\n",
    "#         'away_score': 'Away_Final_Score',\n",
    "#         'game_date': 'Date'\n",
    "#     }, inplace=True)\n",
    "    \n",
    "#     # 6. Determine the Winner\n",
    "#     scoreboard['Winner'] = scoreboard.apply(\n",
    "#         lambda row: row['home_team'] if row['Home_Final_Score'] > row['Away_Final_Score'] else row['away_team'],\n",
    "#         axis=1\n",
    "#     )\n",
    "#     scoreboard['Result'] = (\n",
    "#         scoreboard['Winner'] + ' wins ' + \n",
    "#         scoreboard['Home_Final_Score'].astype(str) + '-' + \n",
    "#         scoreboard['Away_Final_Score'].astype(str)\n",
    "#     )\n",
    "    \n",
    "#     return scoreboard[['Date', 'away_team', 'home_team', 'Away_Final_Score', 'Home_Final_Score', 'Winner', 'Result']]\n",
    "\n",
    "# # --- EXECUTION ---\n",
    "# results_yesterday_df    = get_game_results_last_n_days(n_days= 1)\n",
    "# results_last_7_days_df  = get_game_results_last_n_days(n_days= 7)\n",
    "# results_last_15_days_df = get_game_results_last_n_days(n_days= 15)\n",
    "# results_last_30_days_df = get_game_results_last_n_days(n_days= 30)\n",
    "# results_last_60_days_df = get_game_results_last_n_days(n_days= 60)\n",
    "# results_last_90_days_df = get_game_results_last_n_days(n_days= 90)\n",
    "\n",
    "\n",
    "# # if not results_last_90_days_df.empty:\n",
    "# #     print(f\"\\n--- Game Results from the Last 90 Days ({len(results_last_90_days_df)} Games Found) ---\")\n",
    "# #     print(results_last_90_days_df.tail(10)) # Print the last 10 games found\n",
    "# # else:\n",
    "# #     print(\"\\nNo games were found in the last 90 days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17fde4",
   "metadata": {},
   "source": [
    "### Create fact_statcast_pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab62df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for all games played from 2025-09-19 to 2025-12-17...\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:04<00:00, 13.99it/s]\n",
      "c:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled 53951 pitch events.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_statcast_events_pitch_by_pitch(engine: Engine, n_days= 90):\n",
    "    \"\"\"\n",
    "    Pulls raw pitch-by-pitch data for all games played in the last 'n_days' \n",
    "    and then extracts the final score for each game.\n",
    "    \"\"\"\n",
    "    def load_fact_statcast_events(engine: Engine, df):\n",
    "        try:\n",
    "            table_name = 'fact_statcast_pitches'\n",
    "            print(f\"üíæ Creating {table_name}...\")\n",
    "            \n",
    "            # Loading\n",
    "            print(f\"   üîÉ Loading {len(df)} rows...\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace',\n",
    "                index=False, \n",
    "                chunksize=5000\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully added {len(df)} new rows of data.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ETL Failed during extraction or loading: {e}\")\n",
    "    \n",
    "    # Get today's date\n",
    "    today = date.today()\n",
    "\n",
    "    # Calculate the start and end dates for the n-day range\n",
    "    end_date = today - timedelta(days= 1)  # Search up to yesterday\n",
    "    start_date = today - timedelta(days= n_days)\n",
    "\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Searching for all games played from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "    try:\n",
    "        # Pull all pitch-by-pitch data in that range\n",
    "        fact_statcast_pitches_last_n_days = pyb.statcast(start_dt= start_date_str, end_dt= end_date_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving Statcast data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if fact_statcast_pitches_last_n_days.empty:\n",
    "        print(f\"No games found between {start_date_str} and {end_date_str}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Successfully pulled {len(fact_statcast_pitches_last_n_days)} pitch events for the last {n_days} days.\")\n",
    "\n",
    "    # def filter_days(df, days):\n",
    "    #     cutoff = today - timedelta(days=days)\n",
    "    #     # Convert 'Date' column to datetime objects if they aren't already\n",
    "    #     df['game_date'] = pd.to_datetime(df['game_date']).dt.date\n",
    "    #     return df[df['game_date'] >= cutoff]\n",
    "\n",
    "    # # Sub-df from the main one\n",
    "    # results_1_day_df   = filter_days(fact_statcast_pitches_last_90_days, 1)\n",
    "    # results_7_days_df  = filter_days(fact_statcast_pitches_last_90_days, 7)\n",
    "    # results_30_days_df = filter_days(fact_statcast_pitches_last_90_days, 30)\n",
    "\n",
    "    # Apply the function\n",
    "    load_fact_statcast_events(engine, fact_statcast_pitches_last_n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a30db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max date in the fact_statcast_pitches\n",
    "def get_latest_date_from_db():\n",
    "    query = text(\"SELECT MAX(game_date) FROM fact_statcast_pitches\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query).scalar()\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Execute and calculate fetch window\n",
    "last_date = get_latest_date_from_db()\n",
    "\n",
    "if last_date:\n",
    "    # I want to start fetching from the day AFTER the last recorded date\n",
    "    fetch_start = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    # Fetch up to yesterday\n",
    "    fetch_end = (date.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    if fetch_start <= fetch_end:\n",
    "        print(f\"üîÑ Last data was {last_date}. Fetching from {fetch_start} to {fetch_end}...\")\n",
    "        new_data = pyb.statcast(start_dt=fetch_start, end_dt=fetch_end)\n",
    "        new_data.to_sql('fact_statcast_pitches', engine, if_exists='append', index=False)\n",
    "    else:\n",
    "        print(\"‚úÖ Database is already up to date.\")\n",
    "else:\n",
    "    print(\"Empty table. You need to run an initial seed fetch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f7f00",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d72a7b",
   "metadata": {},
   "source": [
    "### Splits by team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams_split(split_type, clean_mode):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Define year\n",
    "    year = datetime.now().year\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)  \n",
    "\n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=total%7CLast%20{split_type}%20days%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=hmvis%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'first_batter_game':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=leado%7C1st%20Batter%20G%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_power_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Power%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_weak_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Finesse%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    # For each team:\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "            driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_less_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3C%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_greater_or_equal_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3E%3D%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    \n",
    "    # Name of the table\n",
    "    datatable_id = 'split1'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} ({split_type}) table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()    \n",
    "    \n",
    "    if clean_mode == 1:\n",
    "        # Remove 'Roe' exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "\n",
    "        # Rename last 3 columns\n",
    "        new_column_names = [\"BAbip\", \"tOPS+\", \"sOPS+\"]  # New names for last 3 columns\n",
    "        df.columns.values[-3:] = new_column_names  # Assign new names\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "    else:\n",
    "        # Remove 'Roe' and GS exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "        df[0] = df[0].str.replace('GS', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Remove rows where column 'A' contains 'Rk', but keep the first row\n",
    "        df = df[~((df.index > 0) & (df[0].str.contains('Rk', na=False)))]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "\n",
    "        # Remove the last 2 columns\n",
    "        df = df.iloc[:, :-2]\n",
    "\n",
    "        # New column names\n",
    "        new_column_names = ['Team', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'RBI', 'SB',\n",
    "                            'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDP', 'HBP', 'SH',\n",
    "                            'SF', 'IBB', 'ROE', 'BAbip', 'tOPS+', 'sOPS+']\n",
    "\n",
    "        # Rename all columns\n",
    "        df.columns = new_column_names\n",
    "\n",
    "    return df\n",
    "\n",
    "# Call the function to get the teams split data\n",
    "team_vs_lhp             = teams_split(split_type= 'LHP',  clean_mode= 0) # GS empty\n",
    "team_vs_rhp             = teams_split(split_type= 'RHP',  clean_mode= 0) # GS empty\n",
    "team_vs_lh_starters     = teams_split(split_type= 'LH',   clean_mode= 1)\n",
    "team_vs_rh_starters     = teams_split(split_type= 'RH',   clean_mode= 1)\n",
    "team_last_seven_days    = teams_split(split_type= '7',    clean_mode= 1)\n",
    "team_last_fourteen_days = teams_split(split_type= '14',   clean_mode= 1)\n",
    "team_last_28_days       = teams_split(split_type= '28',   clean_mode= 1)\n",
    "team_home_games         = teams_split(split_type= 'Home', clean_mode= 1)\n",
    "team_away_games         = teams_split(split_type= 'Away', clean_mode= 1)\n",
    "team_first_batter_game  = teams_split(split_type= 'first_batter_game', clean_mode= 0) # GS empty\n",
    "team_vs_power_pitcher   = teams_split(split_type= 'vs_power_pitcher',  clean_mode= 0) # GS empty\n",
    "team_vs_weak_pitcher    = teams_split(split_type= 'vs_weak_pitcher',   clean_mode= 0) # GS empty\n",
    "team_vs_power_team      = teams_split(split_type= 'vs_greater_or_equal_than_500_WP', clean_mode= 1)\n",
    "team_vs_weak_team       = teams_split(split_type= 'vs_less_than_500_WP',             clean_mode= 1)\n",
    "\n",
    "# # Direct matchups\n",
    "team_laa = teams_split(split_type= 'ANA', clean_mode= 1)\n",
    "team_ari = teams_split(split_type= 'ARI', clean_mode= 1)\n",
    "team_atl = teams_split(split_type= 'ATL', clean_mode= 1)\n",
    "team_bal = teams_split(split_type= 'BAL', clean_mode= 1)\n",
    "team_bos = teams_split(split_type= 'BOS', clean_mode= 1)\n",
    "team_chc = teams_split(split_type= 'CHC', clean_mode= 1)\n",
    "team_chw = teams_split(split_type= 'CHW', clean_mode= 1)\n",
    "team_cin = teams_split(split_type= 'CIN', clean_mode= 1)\n",
    "team_cle = teams_split(split_type= 'CLE', clean_mode= 1)\n",
    "team_col = teams_split(split_type= 'COL', clean_mode= 1)\n",
    "team_det = teams_split(split_type= 'DET', clean_mode= 1)\n",
    "team_hou = teams_split(split_type= 'HOU', clean_mode= 1)\n",
    "team_kcr = teams_split(split_type= 'KCR', clean_mode= 1)\n",
    "team_lad = teams_split(split_type= 'LAD', clean_mode= 1)\n",
    "team_mia = teams_split(split_type= 'FLA', clean_mode= 1) \n",
    "team_mil = teams_split(split_type= 'MIL', clean_mode= 1)\n",
    "team_min = teams_split(split_type= 'MIN', clean_mode= 1)\n",
    "team_nym = teams_split(split_type= 'NYM', clean_mode= 1)\n",
    "team_nyy = teams_split(split_type= 'NYY', clean_mode= 1)\n",
    "team_oak = teams_split(split_type= 'OAK', clean_mode= 1)\n",
    "team_phi = teams_split(split_type= 'PHI', clean_mode= 1)\n",
    "team_pit = teams_split(split_type= 'PIT', clean_mode= 1)\n",
    "team_sdp = teams_split(split_type= 'SDP', clean_mode= 1)\n",
    "team_sea = teams_split(split_type= 'SEA', clean_mode= 1)\n",
    "team_sfg = teams_split(split_type= 'SFG', clean_mode= 1)\n",
    "team_stl = teams_split(split_type= 'STL', clean_mode= 1)\n",
    "team_tbr = teams_split(split_type= 'TBD', clean_mode= 1)\n",
    "team_tex = teams_split(split_type= 'TEX', clean_mode= 1)\n",
    "team_tor = teams_split(split_type= 'TOR', clean_mode= 1)\n",
    "team_wsn = teams_split(split_type= 'WSN', clean_mode= 1)\n",
    "\n",
    "# Dictionary of dataframes for the teams\n",
    "dic_team = {\n",
    "    'LAA': team_laa,\n",
    "    'AZ':  team_ari,\n",
    "    'ATL': team_atl,\n",
    "    'BAL': team_bal,\n",
    "    'BOS': team_bos,\n",
    "    'CHC': team_chc,\n",
    "    'CHW': team_chw,\n",
    "    'CIN': team_cin,\n",
    "    'CLE': team_cle,\n",
    "    'COL': team_col,\n",
    "    'DET': team_det,\n",
    "    'HOU': team_hou,\n",
    "    'KC':  team_kcr,\n",
    "    'LAD': team_lad,\n",
    "    'MIA': team_mia,\n",
    "    'MIL': team_mil,\n",
    "    'MIN': team_min,\n",
    "    'NYM': team_nym,\n",
    "    'NYY': team_nyy,\n",
    "    'ATH': team_oak,\n",
    "    'PHI': team_phi,\n",
    "    'PIT': team_pit,\n",
    "    'SD':  team_sdp,\n",
    "    'SEA': team_sea,\n",
    "    'SF':  team_sfg,\n",
    "    'STL': team_stl,\n",
    "    'TB':  team_tbr,\n",
    "    'TEX': team_tex,\n",
    "    'TOR': team_tor,\n",
    "    'WSH': team_wsn   \n",
    "    }\n",
    "\n",
    "# Add an ID column with the dictionary key as the identifier\n",
    "for key, df in dic_team.items():\n",
    "    df['ID'] = key  # Assign the dictionary key as the ID\n",
    "\n",
    "# Concatenate all dataFrames in the dictionary\n",
    "direct_matches = pd.concat(dic_team.values(), ignore_index=True)  # Resets index\n",
    "\n",
    "dic_splits = {\n",
    "    'team_vs_lhp'        :team_vs_lhp,        \n",
    "    'team_vs_rhp'        :team_vs_rhp,\n",
    "    'team_vs_lh_starters':team_vs_lh_starters,\n",
    "    'team_vs_rh_starters':team_vs_rh_starters,\n",
    "    'team_last_seven_days':team_last_seven_days,\n",
    "    'team_last_fourteen_days':team_last_fourteen_days,\n",
    "    'team_last_28_days':team_last_28_days,\n",
    "    'team_home_games':team_home_games,\n",
    "    'team_away_games':team_away_games,\n",
    "    'team_first_batter_game':team_first_batter_game,\n",
    "    'team_vs_power_pitcher':team_vs_power_pitcher,\n",
    "    'team_vs_weak_pitcher':team_vs_weak_pitcher,\n",
    "    'team_vs_power_team':team_vs_power_team,\n",
    "    'team_vs_weak_team':team_vs_weak_team      \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d645a94",
   "metadata": {},
   "source": [
    "### Create batting_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "537925e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Scrape Job with Driver Reuse and Retry Logic...\n",
      "------------------------------\n",
      "[BAL - vs_LHP]: Table loaded successfully.\n",
      "SUCCESS: Appended 30 rows. Master DF size: 30\n",
      "[BAL - vs_RHP]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 64\n",
      "[BAL - last_7_days]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 79\n",
      "[BAL - last_14_days]: Table loaded successfully.\n",
      "SUCCESS: Appended 16 rows. Master DF size: 95\n",
      "[BAL - last_28_days]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 113\n",
      "[BAL - home_games]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 151\n",
      "[BAL - away_games]: Table loaded successfully.\n",
      "SUCCESS: Appended 37 rows. Master DF size: 188\n",
      "[BAL - vs_RH_Starters]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 226\n",
      "[BAL - vs_LH_Starters]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 264\n",
      "[BAL - 1st_Half]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 298\n",
      "[BAL - 2nd_Half]: Table loaded successfully.\n",
      "SUCCESS: Appended 31 rows. Master DF size: 329\n",
      "[BAL - April_March]: Table loaded successfully.\n",
      "SUCCESS: Appended 17 rows. Master DF size: 346\n",
      "[BAL - June_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 371\n",
      "[BAL - July_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 389\n",
      "[BAL - August_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 23 rows. Master DF size: 412\n",
      "[BAL - Sept_Oct_Splits]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 430\n",
      "[BAL - C_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 7 rows. Master DF size: 437\n",
      "[BAL - 1B_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 6 rows. Master DF size: 443\n",
      "[BAL - 2B_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 6 rows. Master DF size: 449\n",
      "[BAL - 3B_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 7 rows. Master DF size: 456\n",
      "[BAL - SS_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 4 rows. Master DF size: 460\n",
      "[BAL - LF_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 10 rows. Master DF size: 470\n",
      "[BAL - CF_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 9 rows. Master DF size: 479\n",
      "[BAL - RF_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 491\n",
      "[BAL - DH_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 21 rows. Master DF size: 512\n",
      "[BAL - PH_Position]: Table loaded successfully.\n",
      "SUCCESS: Appended 28 rows. Master DF size: 540\n",
      "[BAL - First_Batter_Game]: Table loaded successfully.\n",
      "SUCCESS: Appended 10 rows. Master DF size: 550\n",
      "[BAL - First_Batter_Inning]: Table loaded successfully.\n",
      "SUCCESS: Appended 32 rows. Master DF size: 582\n",
      "[BAL - Batting_1st]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 595\n",
      "[BAL - Batting_2nd]: Table loaded successfully.\n",
      "SUCCESS: Appended 20 rows. Master DF size: 615\n",
      "[BAL - Batting_3rd]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 627\n",
      "[BAL - Batting_4th]: Table loaded successfully.\n",
      "SUCCESS: Appended 20 rows. Master DF size: 647\n",
      "[BAL - Batting_5th]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 672\n",
      "[BAL - Batting_6th]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 697\n",
      "[BAL - Batting_7th]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 722\n",
      "[BAL - Batting_8th]: Table loaded successfully.\n",
      "SUCCESS: Appended 27 rows. Master DF size: 749\n",
      "[BAL - Batting_9th]: Table loaded successfully.\n",
      "SUCCESS: Appended 38 rows. Master DF size: 787\n",
      "[BAL - in_the_lineup_1-3rd]: Table loaded successfully.\n",
      "SUCCESS: Appended 30 rows. Master DF size: 817\n",
      "[BAL - in_the_lineup_4-6th]: Table loaded successfully.\n",
      "SUCCESS: Appended 31 rows. Master DF size: 848\n",
      "[BAL - in_the_lineup_7-9th]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 882\n",
      "[BAL - vs_SP]: Table loaded successfully.\n",
      "SUCCESS: Appended 32 rows. Master DF size: 914\n",
      "[BAL - vs_RP]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 948\n",
      "[BAL - vs_Power_Pitchers]: Table loaded successfully.\n",
      "SUCCESS: Appended 31 rows. Master DF size: 979\n",
      "[BAL - vs_Finesse_Pitchers]: Table loaded successfully.\n",
      "SUCCESS: Appended 34 rows. Master DF size: 1013\n",
      "[BAL - batting_vs_ANA]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1031\n",
      "[BAL - batting_vs_ARI]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1044\n",
      "[BAL - batting_vs_ATL]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 1056\n",
      "[BAL - batting_vs_BAL]: Table loaded successfully.\n",
      "RETRYING: Attempt 1/3 for BAL - batting_vs_BAL...\n",
      "[BAL - batting_vs_BAL]: Table loaded successfully.\n",
      "RETRYING: Attempt 2/3 for BAL - batting_vs_BAL...\n",
      "[BAL - batting_vs_BAL]: Table loaded successfully.\n",
      "RETRYING: Attempt 3/3 for BAL - batting_vs_BAL...\n",
      "Skipping BAL - batting_vs_BAL after 3 failed attempts.\n",
      "[BAL - batting_vs_BOS]: Table loaded successfully.\n",
      "SUCCESS: Appended 25 rows. Master DF size: 1081\n",
      "[BAL - batting_vs_CHC]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1096\n",
      "[BAL - batting_vs_CHW]: Table loaded successfully.\n",
      "SUCCESS: Appended 22 rows. Master DF size: 1118\n",
      "[BAL - batting_vs_CIN]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1131\n",
      "[BAL - batting_vs_CLE]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1149\n",
      "[BAL - batting_vs_COL]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1162\n",
      "[BAL - batting_vs_DET]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1177\n",
      "[BAL - batting_vs_HOU]: Table loaded successfully.\n",
      "SUCCESS: Appended 16 rows. Master DF size: 1193\n",
      "[BAL - batting_vs_KCR]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1211\n",
      "[BAL - batting_vs_LAD]: Table loaded successfully.\n",
      "SUCCESS: Appended 14 rows. Master DF size: 1225\n",
      "[BAL - batting_vs_FLA]: Table loaded successfully.\n",
      "SUCCESS: Appended 14 rows. Master DF size: 1239\n",
      "[BAL - batting_vs_MIL]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1252\n",
      "[BAL - batting_vs_MIN]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1267\n",
      "[BAL - batting_vs_NYM]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1280\n",
      "[BAL - batting_vs_NYY]: Table loaded successfully.\n",
      "SUCCESS: Appended 23 rows. Master DF size: 1303\n",
      "[BAL - batting_vs_OAK]: Table loaded successfully.\n",
      "SUCCESS: Appended 21 rows. Master DF size: 1324\n",
      "[BAL - batting_vs_PHI]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1337\n",
      "[BAL - batting_vs_PIT]: Table loaded successfully.\n",
      "SUCCESS: Appended 15 rows. Master DF size: 1352\n",
      "[BAL - batting_vs_SDP]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1365\n",
      "[BAL - batting_vs_SEA]: Table loaded successfully.\n",
      "SUCCESS: Appended 20 rows. Master DF size: 1385\n",
      "[BAL - batting_vs_SFG]: Table loaded successfully.\n",
      "SUCCESS: Appended 12 rows. Master DF size: 1397\n",
      "[BAL - batting_vs_STL]: Table loaded successfully.\n",
      "SUCCESS: Appended 13 rows. Master DF size: 1410\n",
      "[BAL - batting_vs_TBD]: Table loaded successfully.\n",
      "SUCCESS: Appended 22 rows. Master DF size: 1432\n",
      "[BAL - batting_vs_TEX]: Table loaded successfully.\n",
      "SUCCESS: Appended 21 rows. Master DF size: 1453\n",
      "[BAL - batting_vs_TOR]: Table loaded successfully.\n",
      "SUCCESS: Appended 22 rows. Master DF size: 1475\n",
      "[BAL - batting_vs_WSN]: Table loaded successfully.\n",
      "SUCCESS: Appended 18 rows. Master DF size: 1493\n",
      "[BAL - batting_Day_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 40 rows. Master DF size: 1533\n",
      "[BAL - batting_Night_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 37 rows. Master DF size: 1570\n",
      "[BAL - batting_Grass_Field_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 41 rows. Master DF size: 1611\n",
      "[BAL - batting_Artificial_Turf_Games]: Table loaded successfully.\n",
      "SUCCESS: Appended 30 rows. Master DF size: 1641\n",
      "------------------------------\n",
      "All tasks finished. Quitting driver.\n",
      "Scraping Complete.\n",
      "Final DataFrame Shape: (1641, 33)\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2025\n",
    "DATATABLE_ID = 'team_split1' \n",
    "MAX_RETRIES = 3 \n",
    "\n",
    "# 2. Define the lists for iteration\n",
    "team_abbreviations = ['BAL']\n",
    "split_parameters = [\n",
    "    {'type': 'LHP',            'desc': 'vs_LHP'},\n",
    "    {'type': 'RHP',            'desc': 'vs_RHP'},\n",
    "    {'type': '7',              'desc': 'last_7_days'},\n",
    "    {'type': '14',             'desc': 'last_14_days'},\n",
    "    {'type': '28',             'desc': 'last_28_days'},\n",
    "    {'type': 'Home',           'desc': 'home_games'},\n",
    "    {'type': 'Away',           'desc': 'away_games'},\n",
    "    {'type': 'RH',             'desc': 'vs_RH_Starters'},\n",
    "    {'type': 'LH',             'desc': 'vs_LH_Starters'},\n",
    "    {'type': '1st',            'desc': '1st_Half'},\n",
    "    {'type': '2nd',            'desc': '2nd_Half'},\n",
    "    {'type': 'April%2FMarch',  'desc': 'April_March'},\n",
    "    {'type': 'June',           'desc': 'June_Splits'},\n",
    "    {'type': 'July',           'desc': 'July_Splits'},\n",
    "    {'type': 'August',         'desc': 'August_Splits'},\n",
    "    {'type': 'Sept%2FOct',     'desc': 'Sept_Oct_Splits'},\n",
    "    {'type': 'C',              'desc':'C_Position'},\n",
    "    {'type': '1B',             'desc': '1B_Position'},\n",
    "    {'type': '2B',             'desc': '2B_Position'},\n",
    "    {'type': '3B',             'desc': '3B_Position'},\n",
    "    {'type': 'SS',             'desc': 'SS_Position'},\n",
    "    {'type': 'LF',             'desc': 'LF_Position'},\n",
    "    {'type': 'CF',             'desc': 'CF_Position'},\n",
    "    {'type': 'RF',             'desc': 'RF_Position'},\n",
    "    {'type': 'DH',             'desc': 'DH_Position'},\n",
    "    {'type': 'PH',             'desc': 'PH_Position'},\n",
    "    {'type': '1st%20Batter',   'desc': 'First_Batter_Game'},\n",
    "    {'type': 'Leadoff%20Inn.', 'desc': 'First_Batter_Inning'},\n",
    "    {'type': 'Batting%201st',  'desc': 'Batting_1st'},\n",
    "    {'type': 'Batting%202nd',  'desc': 'Batting_2nd'},\n",
    "    {'type': 'Batting%203rd',  'desc': 'Batting_3rd'},\n",
    "    {'type': 'Batting%204th',  'desc': 'Batting_4th'},\n",
    "    {'type': 'Batting%205th',  'desc': 'Batting_5th'},\n",
    "    {'type': 'Batting%206th',  'desc': 'Batting_6th'},\n",
    "    {'type': 'Batting%207th',  'desc': 'Batting_7th'},\n",
    "    {'type': 'Batting%208th',  'desc': 'Batting_8th'},\n",
    "    {'type': 'Batting%209th',  'desc': 'Batting_9th'},\n",
    "    {'type': '1-3',            'desc': 'in_the_lineup_1-3rd'},\n",
    "    {'type': '4-6',            'desc': 'in_the_lineup_4-6th'},\n",
    "    {'type': '7-9',            'desc': 'in_the_lineup_7-9th'},\n",
    "    {'type': 'SP',             'desc': 'vs_SP'},\n",
    "    {'type': 'RP',             'desc': 'vs_RP'},\n",
    "    {'type': 'Power',          'desc': 'vs_Power_Pitchers'},\n",
    "    {'type': 'Finesse',        'desc': 'vs_Finesse_Pitchers'},\n",
    "    {'type': 'ANA',            'desc': 'batting_vs_ANA'},\n",
    "    {'type': 'ARI',            'desc': 'batting_vs_ARI'},\n",
    "    {'type': 'ATL',            'desc': 'batting_vs_ATL'},\n",
    "    {'type': 'BAL',            'desc': 'batting_vs_BAL'},\n",
    "    {'type': 'BOS',            'desc': 'batting_vs_BOS'},\n",
    "    {'type': 'CHC',            'desc': 'batting_vs_CHC'},\n",
    "    {'type': 'CHW',            'desc': 'batting_vs_CHW'},\n",
    "    {'type': 'CIN',            'desc': 'batting_vs_CIN'},\n",
    "    {'type': 'CLE',            'desc': 'batting_vs_CLE'},\n",
    "    {'type': 'COL',            'desc': 'batting_vs_COL'},\n",
    "    {'type': 'DET',            'desc': 'batting_vs_DET'},\n",
    "    {'type': 'HOU',            'desc': 'batting_vs_HOU'},\n",
    "    {'type': 'KCR',            'desc': 'batting_vs_KCR'},\n",
    "    {'type': 'LAD',            'desc': 'batting_vs_LAD'},\n",
    "    {'type': 'FLA',            'desc': 'batting_vs_FLA'},\n",
    "    {'type': 'MIL',            'desc': 'batting_vs_MIL'},\n",
    "    {'type': 'MIN',            'desc': 'batting_vs_MIN'},\n",
    "    {'type': 'NYM',            'desc': 'batting_vs_NYM'},\n",
    "    {'type': 'NYY',            'desc': 'batting_vs_NYY'},\n",
    "    {'type': 'OAK',            'desc': 'batting_vs_OAK'},\n",
    "    {'type': 'PHI',            'desc': 'batting_vs_PHI'},\n",
    "    {'type': 'PIT',            'desc': 'batting_vs_PIT'},\n",
    "    {'type': 'SDP',            'desc': 'batting_vs_SDP'},\n",
    "    {'type': 'SEA',            'desc': 'batting_vs_SEA'},\n",
    "    {'type': 'SFG',            'desc': 'batting_vs_SFG'},\n",
    "    {'type': 'STL',            'desc': 'batting_vs_STL'},\n",
    "    {'type': 'TBD',            'desc': 'batting_vs_TBD'},\n",
    "    {'type': 'TEX',            'desc': 'batting_vs_TEX'},\n",
    "    {'type': 'TOR',            'desc': 'batting_vs_TOR'},\n",
    "    {'type': 'WSN',            'desc': 'batting_vs_WSN'},\n",
    "    {'type': 'Day',            'desc': 'batting_Day_Games'},\n",
    "    {'type': 'Night',          'desc': 'batting_Night_Games'},\n",
    "    {'type': 'Grass',          'desc': 'batting_Grass_Field_Games'},\n",
    "    {'type': 'Artif.%20Turf',  'desc': 'batting_Artificial_Turf_Games'}\n",
    "]\n",
    "\n",
    "# Helper function to initialize driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a new Selenium WebDriver instance.\"\"\"\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") \n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\") # Use a recent, common User-Agent\n",
    "    # NOTE: Keep the path correct my Brave installation\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\" \n",
    "    \n",
    "    # Attempt to start the driver with a timeout\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options) \n",
    "        driver.set_page_load_timeout(60)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize Chrome driver. Check Brave path and driver version. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  batter_split function\n",
    "def batter_split(driver, split_type, team_abv, year, datatable_id, description):\n",
    "    \n",
    "    # --- URL CONSTRUCTION --- \n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'C' or split_type == '1B' or split_type == '2B' or split_type == '3B' \\\n",
    "        or split_type == 'SS' or split_type == 'LF' or split_type == 'CF' or split_type == 'RF' \\\n",
    "        or split_type == 'DH' or split_type == 'PH': # for each position\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=defp%7Cas%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '1st%20Batter': # first batter of the game\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Leadoff%20Inn.': # first batter of the inning\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Batting%201st' or split_type == 'Batting%202nd' or split_type == 'Batting%203rd' \\\n",
    "        or split_type == 'Batting%204th' or split_type == 'Batting%205th' or split_type == 'Batting%206th' \\\n",
    "        or split_type == 'Batting%207th' or split_type == 'Batting%208th' or split_type == 'Batting%209th': # for each spot in the lineup\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=lineu%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == '1-3' or split_type == '4-6' or split_type == '7-9': # for each third of the lineup\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=innng%7CInnings%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'SP' or split_type == 'RP': # vs SP or RP\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=times%7Cvs.%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Power' or split_type == 'avg.P%2FF' or split_type == 'Finesse':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=power%7Cvs.%20{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\" \n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=oppon%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "    elif split_type == 'Day' or split_type == 'Night' or split_type == 'Grass' or split_type == 'Artif.%20Turf':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=stad%7C{split_type}%7C{team_abv}%7C{year}%7Cbat%7CAB%7C\"\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(f\"[{team_abv} - {description}]: Page load timed out (60s). Skipping or retrying...\")\n",
    "        return None # Let the main loop handle the retry/skip\n",
    "        \n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    \n",
    "    # --- SCRAPING LOGIC --- \n",
    "    try:\n",
    "        # Wait up to 30 seconds for the table\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        print(f\"[{team_abv} - {description}]: Table loaded successfully.\")\n",
    "    except Exception:\n",
    "        # This catches both TimeoutException and NoSuchElementException\n",
    "        print(f\"[{team_abv} - {description}]: Table element not found after 30s. Check site content.\")\n",
    "        return None \n",
    "\n",
    "    # Extract the full HTML, wrap in StringIO, read with pandas\n",
    "    table_html = table_element.get_attribute('outerHTML')\n",
    "    html_string = StringIO(table_html)\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(html_string, flavor='lxml') \n",
    "    except Exception as e:\n",
    "        print(f\"[{team_abv} - {description}]: Error parsing HTML with pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"[{team_abv} - {description}]: No tables found.\")\n",
    "        return None\n",
    "\n",
    "    # Create an explicit copy\n",
    "    df = tables[0].copy() \n",
    "    \n",
    "    # --- CLEANING LOGIC --- \n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns]\n",
    "\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df[df['Rk'] != 'Rk']\n",
    "        \n",
    "    df = df.iloc[:-1] # Remove last row (Totals)\n",
    "    \n",
    "    df['description'] = description\n",
    "    df['team'] = team_abv\n",
    "    df['year'] = YEAR\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Master loop with driver reuse and retry logic\n",
    "\n",
    "batting_splits = pd.DataFrame()\n",
    "driver = initialize_driver()\n",
    "\n",
    "if driver is None:\n",
    "    exit() # Stop if the driver failed to initialize\n",
    "\n",
    "print(\"Starting Scrape Job with Driver Reuse and Retry Logic...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # Outer loop for teams\n",
    "    for team_abv in team_abbreviations:\n",
    "        # Inner loop for splits\n",
    "        for split in split_parameters:\n",
    "            \n",
    "            # Retry loop for failed connection/table load\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Check if the driver is still alive (by checking its current URL)\n",
    "                    driver.current_url \n",
    "                    \n",
    "                    new_df = batter_split(\n",
    "                        driver=driver,\n",
    "                        split_type=split['type'], \n",
    "                        team_abv=team_abv, \n",
    "                        year=YEAR, \n",
    "                        datatable_id=DATATABLE_ID, \n",
    "                        description=split['desc']\n",
    "                    )\n",
    "                    \n",
    "                    if new_df is not None and not new_df.empty:\n",
    "                        batting_splits = pd.concat([batting_splits, new_df], ignore_index=True)\n",
    "                        print(f\"SUCCESS: Appended {len(new_df)} rows. Master DF size: {len(batting_splits)}\")\n",
    "                        break # Break the retry loop on success\n",
    "                    \n",
    "                    # If new_df is None (due to TimeoutException/Table not found), retry\n",
    "                    print(f\"RETRYING: Attempt {attempt + 1}/{MAX_RETRIES} for {team_abv} - {split['desc']}...\")\n",
    "                    time.sleep(2) # Short wait before retry\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    # CRITICAL: Driver died (Connection refused/lost)\n",
    "                    print(f\"\\n[FATAL ERROR] Driver connection lost for {team_abv} - {split['desc']}. Restarting driver...\")\n",
    "                    \n",
    "                    # Clean up the old session\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass # Ignore errors on quitting a dead driver\n",
    "                    \n",
    "                    # Restart the driver\n",
    "                    driver = initialize_driver()\n",
    "                    if driver is None:\n",
    "                        # If restart fails, stop the whole script\n",
    "                        raise SystemExit(\"Driver restart failed. Terminating.\")\n",
    "                        \n",
    "                    time.sleep(5) # Longer wait after a fatal crash\n",
    "                    print(\"Driver successfully restarted. Retrying scrape.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[{team_abv} - {split['desc']}]: Unhandled error: {e}\")\n",
    "                    break # Break retry loop on unexpected failure\n",
    "\n",
    "            # Check if retry failed all attempts and the split was not appended\n",
    "            else: \n",
    "                print(f\"Skipping {team_abv} - {split['desc']} after {MAX_RETRIES} failed attempts.\")\n",
    "                \n",
    "finally:\n",
    "    # 3. CLEANUP: Quit the driver ONCE after all loops are finished\n",
    "    print(\"-\" * 30)\n",
    "    print(\"All tasks finished. Quitting driver.\")\n",
    "    if 'driver' in locals() and driver:\n",
    "        driver.quit() \n",
    "    \n",
    "print(\"Scraping Complete.\")\n",
    "print(f\"Final DataFrame Shape: {batting_splits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cd99b",
   "metadata": {},
   "source": [
    "### Pitching splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "YEAR = 2025\n",
    "DATATABLE_ID = 'team_split1' \n",
    "MAX_RETRIES = 3 \n",
    "\n",
    "# 2. Define the lists for iteration\n",
    "team_abbreviations = ['BAL']\n",
    "split_parameters = [\n",
    "    {'type': 'LHB',                      'desc': 'vs_LHB'},\n",
    "    {'type': 'RHB',                      'desc': 'vs_RHB'},\n",
    "    {'type': '7',                        'desc': 'last_7_days'},\n",
    "    {'type': '14',                       'desc': 'last_14_days'},\n",
    "    {'type': '28',                       'desc': 'last_28_days'},\n",
    "    {'type': 'Home',                     'desc': 'home_games'},\n",
    "    {'type': 'Away',                     'desc': 'away_games'},\n",
    "    {'type': '1st',                      'desc': '1st_half'},\n",
    "    {'type': '2nd',                      'desc': '2nd_half'},\n",
    "    {'type': 'April%2FMarch',            'desc': 'april_march'},\n",
    "    {'type': 'June',                     'desc': 'june_splits'},\n",
    "    {'type': 'July',                     'desc': 'july_splits'},\n",
    "    {'type': 'August',                   'desc': 'august_splits'},\n",
    "    {'type': 'Sept%2FOct',               'desc': 'sept_oct_Splits'},\n",
    "    {'type': '1st%20Batter',             'desc': 'first_batter_game'},\n",
    "    {'type': 'Leadoff%20Inn.',           'desc': 'first_batter_inning'},\n",
    "    {'type': 'Batting%201st',            'desc': 'pitching_vs_1st'},\n",
    "    {'type': 'Batting%202nd',            'desc': 'pitching_vs_2nd'},\n",
    "    {'type': 'Batting%203rd',            'desc': 'pitching_vs_3rd'},\n",
    "    {'type': 'Batting%204th',            'desc': 'pitching_vs_4th'},\n",
    "    {'type': 'Batting%205th',            'desc': 'pitching_vs_5th'},\n",
    "    {'type': 'Batting%206th',            'desc': 'pitching_vs_6th'},\n",
    "    {'type': 'Batting%207th',            'desc': 'pitching_vs_7th'},\n",
    "    {'type': 'Batting%208th',            'desc': 'pitching_vs_8th'},\n",
    "    {'type': 'Batting%209th',            'desc': 'pitching_vs_9th'},\n",
    "    {'type': 'Starter',                  'desc': 'as_starter'},\n",
    "    {'type': 'Reliever',                 'desc': 'as_reliever'},\n",
    "    {'type': '0-2%20Runs',               'desc': 'run_support_0_2'},\n",
    "    {'type': '3-5%20Runs',               'desc': 'run_support_3_5'},\n",
    "    {'type': '6%2B%20Runs',              'desc': 'run_support_6_plus'},\n",
    "    {'type': 'Swung%20at%201st%20Pitch', 'desc': 'outcome_of_at_bat_when_swung_at_first_pitch'},\n",
    "    {'type': 'Took%201st%20Pitch',       'desc': 'outcome_of_at_bat_when_took_first_pitch'},\n",
    "    {'type': '0',                        'desc': '0_outs_in_the_inning'},\n",
    "    {'type': '1',                        'desc': '1_outs_in_the_inning'},\n",
    "    {'type': '2',                        'desc': '2_outs_in_the_inning'},\n",
    "    {'type': 'innng%7C1st',             'desc': 'pitching_in_1st_inning'},\n",
    "    {'type': 'innng%7C2nd',             'desc': 'pitching_in_2nd_inning'},\n",
    "    {'type': 'innng%7C3rd',             'desc': 'pitching_in_3rd_inning'},\n",
    "    {'type': 'innng%7C4th',             'desc': 'pitching_in_4th_inning'},\n",
    "    {'type': 'innng%7C5th',             'desc': 'pitching_in_5th_inning'},\n",
    "    {'type': 'innng%7C6th',             'desc': 'pitching_in_6th_inning'},\n",
    "    {'type': 'innng%7C7th',             'desc': 'pitching_in_7th_inning'},\n",
    "    {'type': 'innng%7C8th',             'desc': 'pitching_in_8th_inning'},\n",
    "    {'type': 'innng%7C9th',             'desc': 'pitching_in_9th_inning'},\n",
    "    {'type': 'ANA',                      'desc': 'pitching_vs_ANA'},\n",
    "    {'type': 'ARI',                      'desc': 'pitching_vs_ARI'},\n",
    "    {'type': 'ATL',                      'desc': 'pitching_vs_ATL'},\n",
    "    {'type': 'BAL',                      'desc': 'pitching_vs_BAL'},\n",
    "    {'type': 'BOS',                      'desc': 'pitching_vs_BOS'},\n",
    "    {'type': 'CHC',                      'desc': 'pitching_vs_CHC'},\n",
    "    {'type': 'CHW',                      'desc': 'pitching_vs_CHW'},\n",
    "    {'type': 'CIN',                      'desc': 'pitching_vs_CIN'},\n",
    "    {'type': 'CLE',                      'desc': 'pitching_vs_CLE'},\n",
    "    {'type': 'COL',                      'desc': 'pitching_vs_COL'},\n",
    "    {'type': 'DET',                      'desc': 'pitching_vs_DET'},\n",
    "    {'type': 'HOU',                      'desc': 'pitching_vs_HOU'},\n",
    "    {'type': 'KCR',                      'desc': 'pitching_vs_KCR'},\n",
    "    {'type': 'LAD',                      'desc': 'pitching_vs_LAD'},\n",
    "    {'type': 'FLA',                      'desc': 'pitching_vs_FLA'},\n",
    "    {'type': 'MIL',                      'desc': 'pitching_vs_MIL'},\n",
    "    {'type': 'MIN',                      'desc': 'pitching_vs_MIN'},\n",
    "    {'type': 'NYM',                      'desc': 'pitching_vs_NYM'},\n",
    "    {'type': 'NYY',                      'desc': 'pitching_vs_NYY'},\n",
    "    {'type': 'OAK',                      'desc': 'pitching_vs_OAK'},\n",
    "    {'type': 'PHI',                      'desc': 'pitching_vs_PHI'},\n",
    "    {'type': 'PIT',                      'desc': 'pitching_vs_PIT'},\n",
    "    {'type': 'SDP',                      'desc': 'pitching_vs_SDP'},\n",
    "    {'type': 'SEA',                      'desc': 'pitching_vs_SEA'},\n",
    "    {'type': 'SFG',                      'desc': 'pitching_vs_SFG'},\n",
    "    {'type': 'STL',                      'desc': 'pitching_vs_STL'},\n",
    "    {'type': 'TBD',                      'desc': 'pitching_vs_TBD'},\n",
    "    {'type': 'TEX',                      'desc': 'pitching_vs_TEX'},\n",
    "    {'type': 'TOR',                      'desc': 'pitching_vs_TOR'},\n",
    "    {'type': 'WSN',                      'desc': 'pitching_vs_WSN'},\n",
    "    {'type': 'Day',                      'desc': 'pitching_Day_Games'},\n",
    "    {'type': 'Night',                    'desc': 'pitching_Night_Games'},\n",
    "    {'type': 'Grass',                    'desc': 'pitching_Grass_Field_Games'},\n",
    "    {'type': 'Artif.%20Turf',            'desc': 'pitching_Artificial_Turf_Games'}\n",
    "]\n",
    "\n",
    "# Helper function to initialize driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a new Selenium WebDriver instance.\"\"\"\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") \n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\") # Use a recent, common User-Agent\n",
    "    # NOTE: Keep the path correct my Brave installation\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\" \n",
    "    \n",
    "    # Attempt to start the driver with a timeout\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options) \n",
    "        driver.set_page_load_timeout(60)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize Chrome driver. Check Brave path and driver version. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  pitcher_split function\n",
    "def pitcher_split(driver, split_type, team_abv, year, datatable_id, description):\n",
    "\n",
    "    # --- URL CONSTRUCTION --- \n",
    "    if split_type == 'LHB' or split_type == 'RHB': # matchups vs LHB and RHB hitters\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '1st%20Batter': # first batter of the game\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Leadoff%20Inn.': # first batter of the inning\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Batting%201st' or split_type == 'Batting%202nd' or split_type == 'Batting%203rd' \\\n",
    "        or split_type == 'Batting%204th' or split_type == 'Batting%205th' or split_type == 'Batting%206th' \\\n",
    "        or split_type == 'Batting%207th' or split_type == 'Batting%208th' or split_type == 'Batting%209th': \n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=lineu%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Starter' or split_type == 'Reliever':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=sprel%7Cas%20{split_type}%7CT{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '0-2%20Runs' or split_type == '3-5%20Runs' or split_type == '6%2B%20Runs':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=rs%7C{split_type}%20Scored%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Swung%20at%201st%20Pitch' or split_type == 'Took%201st%20Pitch':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=tkswg%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == '0' or split_type == '1' or split_type == '2':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=outs%7C{split_type}%20outs%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'innng%7C1st' or split_type == 'innng%7C2nd' or split_type == 'innng%7C3rd' \\\n",
    "        or split_type == 'innng%7C4th' or split_type == 'innng%7C5th' or split_type == 'innng%7C6th' \\\n",
    "        or split_type == 'innng%7C7th' or split_type == 'innng%7C8th' or split_type == 'innng%7C9th':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params={split_type}%20inning%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=oppon%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    elif split_type == 'Day' or split_type == 'Night' or split_type == 'Grass' or split_type == 'Artif.%20Turf':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=stad%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CAB%7C\"\n",
    "    else:\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(f\"[{team_abv} - {description}]: Page load timed out (60s). Skipping or retrying...\")\n",
    "        return None # Let the main loop handle the retry/skip\n",
    "        \n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    \n",
    "    # --- SCRAPING LOGIC --- \n",
    "    try:\n",
    "        # Wait up to 30 seconds for the table\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        print(f\"[{team_abv} - {description}]: Table loaded successfully.\")\n",
    "    except Exception:\n",
    "        # This catches both TimeoutException and NoSuchElementException\n",
    "        print(f\"[{team_abv} - {description}]: Table element not found after 30s. Check site content.\")\n",
    "        return None \n",
    "\n",
    "    # Extract the full HTML, wrap in StringIO, read with pandas\n",
    "    table_html = table_element.get_attribute('outerHTML')\n",
    "    html_string = StringIO(table_html)\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(html_string, flavor='lxml') \n",
    "    except Exception as e:\n",
    "        print(f\"[{team_abv} - {description}]: Error parsing HTML with pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"[{team_abv} - {description}]: No tables found.\")\n",
    "        return None\n",
    "\n",
    "    # Create an explicit copy\n",
    "    df = tables[0].copy() \n",
    "    \n",
    "    # --- CLEANING LOGIC --- \n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns]\n",
    "\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df[df['Rk'] != 'Rk']\n",
    "        \n",
    "    df = df.iloc[:-1] # Remove last row (Totals)\n",
    "    \n",
    "    df['description'] = description\n",
    "    df['team'] = team_abv\n",
    "    df['year'] = YEAR\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Master loop with driver reuse and retry logic\n",
    "\n",
    "pitching_splits = pd.DataFrame()\n",
    "driver = initialize_driver()\n",
    "\n",
    "if driver is None:\n",
    "    exit() # Stop if the driver failed to initialize\n",
    "\n",
    "print(\"Starting Scrape Job with Driver Reuse and Retry Logic...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # Outer loop for teams\n",
    "    for team_abv in team_abbreviations:\n",
    "        # Inner loop for splits\n",
    "        for split in split_parameters:\n",
    "            \n",
    "            # Retry loop for failed connection/table load\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Check if the driver is still alive (by checking its current URL)\n",
    "                    driver.current_url \n",
    "                    \n",
    "                    new_df = pitcher_split(\n",
    "                        driver=driver,\n",
    "                        split_type=split['type'], \n",
    "                        team_abv=team_abv, \n",
    "                        year=YEAR, \n",
    "                        datatable_id=DATATABLE_ID, \n",
    "                        description=split['desc']\n",
    "                    )\n",
    "                    \n",
    "                    if new_df is not None and not new_df.empty:\n",
    "                        pitching_splits = pd.concat([pitching_splits, new_df], ignore_index=True)\n",
    "                        print(f\"SUCCESS: Appended {len(new_df)} rows. Master DF size: {len(pitching_splits)}\")\n",
    "                        break # Break the retry loop on success\n",
    "                    \n",
    "                    # If new_df is None (due to TimeoutException/Table not found), retry\n",
    "                    print(f\"RETRYING: Attempt {attempt + 1}/{MAX_RETRIES} for {team_abv} - {split['desc']}...\")\n",
    "                    time.sleep(2) # Short wait before retry\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    # CRITICAL: Driver died (Connection refused/lost)\n",
    "                    print(f\"\\n[FATAL ERROR] Driver connection lost for {team_abv} - {split['desc']}. Restarting driver...\")\n",
    "                    \n",
    "                    # Clean up the old session\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass # Ignore errors on quitting a dead driver\n",
    "                    \n",
    "                    # Restart the driver\n",
    "                    driver = initialize_driver()\n",
    "                    if driver is None:\n",
    "                        # If restart fails, stop the whole script\n",
    "                        raise SystemExit(\"Driver restart failed. Terminating.\")\n",
    "                        \n",
    "                    time.sleep(5) # Longer wait after a fatal crash\n",
    "                    print(\"Driver successfully restarted. Retrying scrape.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[{team_abv} - {split['desc']}]: Unhandled error: {e}\")\n",
    "                    break # Break retry loop on unexpected failure\n",
    "\n",
    "            # Check if retry failed all attempts and the split was not appended\n",
    "            else: \n",
    "                print(f\"Skipping {team_abv} - {split['desc']} after {MAX_RETRIES} failed attempts.\")\n",
    "                \n",
    "finally:\n",
    "    # 3. CLEANUP: Quit the driver ONCE after all loops are finished\n",
    "    print(\"-\" * 30)\n",
    "    print(\"All tasks finished. Quitting driver.\")\n",
    "    if 'driver' in locals() and driver:\n",
    "        driver.quit() \n",
    "    \n",
    "print(\"Scraping Complete.\")\n",
    "print(f\"Final DataFrame Shape: {pitching_splits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce458d",
   "metadata": {},
   "source": [
    "### Pitching splits - Game Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "YEAR = 2025\n",
    "DATATABLE_ID = 'team_split1' \n",
    "MAX_RETRIES = 3 \n",
    "\n",
    "# 2. Define the lists for iteration\n",
    "team_abbreviations = ['BAL']\n",
    "split_parameters = [\n",
    "    {'type': '7',                        'desc': 'last_7_days'},\n",
    "    {'type': '14',                       'desc': 'last_14_days'},\n",
    "    {'type': '28',                       'desc': 'last_28_days'},\n",
    "    {'type': 'Home',                     'desc': 'home_games'},\n",
    "    {'type': 'Away',                     'desc': 'away_games'},\n",
    "    {'type': '1st',                      'desc': '1st_half'},\n",
    "    {'type': '2nd',                      'desc': '2nd_half'},\n",
    "    {'type': 'April%2FMarch',            'desc': 'april_march'},\n",
    "    {'type': 'June',                     'desc': 'june_splits'},\n",
    "    {'type': 'July',                     'desc': 'july_splits'},\n",
    "    {'type': 'August',                   'desc': 'august_splits'},\n",
    "    {'type': 'Sept%2FOct',               'desc': 'sept_oct_Splits'},\n",
    "    {'type': 'Starter',                  'desc': 'as_starter'},\n",
    "    {'type': 'Reliever',                 'desc': 'as_reliever'},\n",
    "    {'type': '0-2%20Runs',               'desc': 'run_support_0_2'},\n",
    "    {'type': '3-5%20Runs',               'desc': 'run_support_3_5'},\n",
    "    {'type': '6%2B%20Runs',              'desc': 'run_support_6_plus'},\n",
    "    {'type': 'ANA',                      'desc': 'pitching_vs_ANA'},\n",
    "    {'type': 'ARI',                      'desc': 'pitching_vs_ARI'},\n",
    "    {'type': 'ATL',                      'desc': 'pitching_vs_ATL'},\n",
    "    {'type': 'BAL',                      'desc': 'pitching_vs_BAL'},\n",
    "    {'type': 'BOS',                      'desc': 'pitching_vs_BOS'},\n",
    "    {'type': 'CHC',                      'desc': 'pitching_vs_CHC'},\n",
    "    {'type': 'CHW',                      'desc': 'pitching_vs_CHW'},\n",
    "    {'type': 'CIN',                      'desc': 'pitching_vs_CIN'},\n",
    "    {'type': 'CLE',                      'desc': 'pitching_vs_CLE'},\n",
    "    {'type': 'COL',                      'desc': 'pitching_vs_COL'},\n",
    "    {'type': 'DET',                      'desc': 'pitching_vs_DET'},\n",
    "    {'type': 'HOU',                      'desc': 'pitching_vs_HOU'},\n",
    "    {'type': 'KCR',                      'desc': 'pitching_vs_KCR'},\n",
    "    {'type': 'LAD',                      'desc': 'pitching_vs_LAD'},\n",
    "    {'type': 'FLA',                      'desc': 'pitching_vs_FLA'},\n",
    "    {'type': 'MIL',                      'desc': 'pitching_vs_MIL'},\n",
    "    {'type': 'MIN',                      'desc': 'pitching_vs_MIN'},\n",
    "    {'type': 'NYM',                      'desc': 'pitching_vs_NYM'},\n",
    "    {'type': 'NYY',                      'desc': 'pitching_vs_NYY'},\n",
    "    {'type': 'OAK',                      'desc': 'pitching_vs_OAK'},\n",
    "    {'type': 'PHI',                      'desc': 'pitching_vs_PHI'},\n",
    "    {'type': 'PIT',                      'desc': 'pitching_vs_PIT'},\n",
    "    {'type': 'SDP',                      'desc': 'pitching_vs_SDP'},\n",
    "    {'type': 'SEA',                      'desc': 'pitching_vs_SEA'},\n",
    "    {'type': 'SFG',                      'desc': 'pitching_vs_SFG'},\n",
    "    {'type': 'STL',                      'desc': 'pitching_vs_STL'},\n",
    "    {'type': 'TBD',                      'desc': 'pitching_vs_TBD'},\n",
    "    {'type': 'TEX',                      'desc': 'pitching_vs_TEX'},\n",
    "    {'type': 'TOR',                      'desc': 'pitching_vs_TOR'},\n",
    "    {'type': 'WSN',                      'desc': 'pitching_vs_WSN'},\n",
    "    {'type': 'Day',                      'desc': 'pitching_Day_Games'},\n",
    "    {'type': 'Night',                    'desc': 'pitching_Night_Games'},\n",
    "    {'type': 'Grass',                    'desc': 'pitching_Grass_Field_Games'},\n",
    "    {'type': 'Artif.%20Turf',            'desc': 'pitching_Artificial_Turf_Games'}\n",
    "]\n",
    "\n",
    "# Helper function to initialize driver\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes and returns a new Selenium WebDriver instance.\"\"\"\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") \n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\") # Use a recent, common User-Agent\n",
    "    # NOTE: Keep the path correct my Brave installation\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\" \n",
    "    \n",
    "    # Attempt to start the driver with a timeout\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options) \n",
    "        driver.set_page_load_timeout(60)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize Chrome driver. Check Brave path and driver version. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  pitcher_split function\n",
    "def pitcher_split_game_level(driver, split_type, team_abv, year, datatable_id, description):\n",
    "\n",
    "    # --- URL CONSTRUCTION --- \n",
    "    if split_type == 'LHB' or split_type == 'RHB': # matchups vs LHB and RHB hitters\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=plato%7Cvs%20{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=total%7CLast%20{split_type}%20days%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=hmvis%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '1st' or split_type == '2nd': # for 1st and 2nd half of the season\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=half%7C{split_type}%20Half%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'April%2FMarch' or split_type == 'May' or split_type == 'June' \\\n",
    "        or split_type == 'July' or split_type == 'August' or split_type == 'Sept%2FOct': # for each month\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=month%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '1st%20Batter': # first batter of the game\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%20G%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Leadoff%20Inn.': # first batter of the inning\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=leado%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Batting%201st' or split_type == 'Batting%202nd' or split_type == 'Batting%203rd' \\\n",
    "        or split_type == 'Batting%204th' or split_type == 'Batting%205th' or split_type == 'Batting%206th' \\\n",
    "        or split_type == 'Batting%207th' or split_type == 'Batting%208th' or split_type == 'Batting%209th': \n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=lineu%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Starter' or split_type == 'Reliever':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=sprel%7Cas%20{split_type}%7CT{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '0-2%20Runs' or split_type == '3-5%20Runs' or split_type == '6%2B%20Runs':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=rs%7C{split_type}%20Scored%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Swung%20at%201st%20Pitch' or split_type == 'Took%201st%20Pitch':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=tkswg%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == '0' or split_type == '1' or split_type == '2':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=outs%7C{split_type}%20outs%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'innng%7C1st' or split_type == 'innng%7C2nd' or split_type == 'innng%7C3rd' \\\n",
    "        or split_type == 'innng%7C4th' or split_type == 'innng%7C5th' or split_type == 'innng%7C6th' \\\n",
    "        or split_type == 'innng%7C7th' or split_type == 'innng%7C8th' or split_type == 'innng%7C9th':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params={split_type}%20inning%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=oppon%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    elif split_type == 'Day' or split_type == 'Night' or split_type == 'Grass' or split_type == 'Artif.%20Turf':\n",
    "        url = f\"https://www.baseball-reference.com/tools/split_stats_team.cgi?full=1&params=stad%7C{split_type}%7C{team_abv}%7C{year}%7Cpitch%7CIP%7C\"\n",
    "    else:\n",
    "        print(f\"Error: Split type '{split_type}' not supported yet.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(f\"[{team_abv} - {description}]: Page load timed out (60s). Skipping or retrying...\")\n",
    "        return None # Let the main loop handle the retry/skip\n",
    "        \n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"\n",
    "    \n",
    "    # --- SCRAPING LOGIC --- \n",
    "    try:\n",
    "        # Wait up to 30 seconds for the table\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "        print(f\"[{team_abv} - {description}]: Table loaded successfully.\")\n",
    "    except Exception:\n",
    "        # This catches both TimeoutException and NoSuchElementException\n",
    "        print(f\"[{team_abv} - {description}]: Table element not found after 30s. Check site content.\")\n",
    "        return None \n",
    "\n",
    "    # Extract the full HTML, wrap in StringIO, read with pandas\n",
    "    table_html = table_element.get_attribute('outerHTML')\n",
    "    html_string = StringIO(table_html)\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(html_string, flavor='lxml') \n",
    "    except Exception as e:\n",
    "        print(f\"[{team_abv} - {description}]: Error parsing HTML with pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"[{team_abv} - {description}]: No tables found.\")\n",
    "        return None\n",
    "\n",
    "    # Create an explicit copy\n",
    "    df = tables[0].copy() \n",
    "    \n",
    "    # --- CLEANING LOGIC --- \n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in df.columns]\n",
    "\n",
    "    if 'Rk' in df.columns:\n",
    "        df = df[df['Rk'] != 'Rk']\n",
    "        \n",
    "    df = df.iloc[:-1] # Remove last row (Totals)\n",
    "    \n",
    "    df['description'] = description\n",
    "    df['team'] = team_abv\n",
    "    df['year'] = YEAR\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Master loop with driver reuse and retry logic\n",
    "\n",
    "pitching_splits_game_level = pd.DataFrame()\n",
    "driver = initialize_driver()\n",
    "\n",
    "if driver is None:\n",
    "    exit() # Stop if the driver failed to initialize\n",
    "\n",
    "print(\"Starting Scrape Job with Driver Reuse and Retry Logic...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # Outer loop for teams\n",
    "    for team_abv in team_abbreviations:\n",
    "        # Inner loop for splits\n",
    "        for split in split_parameters:\n",
    "            \n",
    "            # Retry loop for failed connection/table load\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Check if the driver is still alive (by checking its current URL)\n",
    "                    driver.current_url \n",
    "                    \n",
    "                    new_df = pitcher_split_game_level(\n",
    "                        driver=driver,\n",
    "                        split_type=split['type'], \n",
    "                        team_abv=team_abv, \n",
    "                        year=YEAR, \n",
    "                        datatable_id=DATATABLE_ID, \n",
    "                        description=split['desc']\n",
    "                    )\n",
    "                    \n",
    "                    if new_df is not None and not new_df.empty:\n",
    "                        pitching_splits_game_level = pd.concat([pitching_splits_game_level, new_df], ignore_index=True)\n",
    "                        print(f\"SUCCESS: Appended {len(new_df)} rows. Master DF size: {len(pitching_splits_game_level)}\")\n",
    "                        break # Break the retry loop on success\n",
    "                    \n",
    "                    # If new_df is None (due to TimeoutException/Table not found), retry\n",
    "                    print(f\"RETRYING: Attempt {attempt + 1}/{MAX_RETRIES} for {team_abv} - {split['desc']}...\")\n",
    "                    time.sleep(2) # Short wait before retry\n",
    "\n",
    "                except WebDriverException as e:\n",
    "                    # CRITICAL: Driver died (Connection refused/lost)\n",
    "                    print(f\"\\n[FATAL ERROR] Driver connection lost for {team_abv} - {split['desc']}. Restarting driver...\")\n",
    "                    \n",
    "                    # Clean up the old session\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass # Ignore errors on quitting a dead driver\n",
    "                    \n",
    "                    # Restart the driver\n",
    "                    driver = initialize_driver()\n",
    "                    if driver is None:\n",
    "                        # If restart fails, stop the whole script\n",
    "                        raise SystemExit(\"Driver restart failed. Terminating.\")\n",
    "                        \n",
    "                    time.sleep(5) # Longer wait after a fatal crash\n",
    "                    print(\"Driver successfully restarted. Retrying scrape.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"[{team_abv} - {split['desc']}]: Unhandled error: {e}\")\n",
    "                    break # Break retry loop on unexpected failure\n",
    "\n",
    "            # Check if retry failed all attempts and the split was not appended\n",
    "            else: \n",
    "                print(f\"Skipping {team_abv} - {split['desc']} after {MAX_RETRIES} failed attempts.\")\n",
    "                \n",
    "finally:\n",
    "    # 3. CLEANUP: Quit the driver ONCE after all loops are finished\n",
    "    print(\"-\" * 30)\n",
    "    print(\"All tasks finished. Quitting driver.\")\n",
    "    if 'driver' in locals() and driver:\n",
    "        driver.quit() \n",
    "    \n",
    "print(\"Scraping Complete.\")\n",
    "print(f\"Final DataFrame Shape: {pitching_splits_game_level.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405a5f6",
   "metadata": {},
   "source": [
    "### Update the table statcast_pitches in PostgreSQL\n",
    "#### This table shows the events pitch-by-pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23114b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybaseball.cache.enable() # Enable caching for reliability\n",
    "\n",
    "def update_statcast_data(engine: Engine):\n",
    "    \"\"\"\n",
    "    Pulls Statcast data starting from the day AFTER the last record in the database\n",
    "    to ensure only new events are downloaded and appended.\n",
    "    \"\"\"\n",
    "    \n",
    "    today = date.today()\n",
    "    \n",
    "    # --- STEP 1: FIND LAST DATE IN DB ---\n",
    "    try:\n",
    "        # Query the database to find the latest game_date currently stored\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(\n",
    "                text(\"SELECT MAX(game_date) FROM statcast_pitches;\")\n",
    "            ).scalar()\n",
    "        \n",
    "        # If the table is empty, start from 400 days ago (initial load range)\n",
    "        if result is None:\n",
    "            print(\"Database is empty. Starting full initial load (400 days)...\")\n",
    "            last_date = today - timedelta(days=400)\n",
    "        else:\n",
    "            # Start the new pull from the day AFTER the last record\n",
    "            last_date = result.date()\n",
    "            print(f\"Latest game_date found in DB: {last_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR querying database for last date: {e}. Defaulting to last 5 days.\")\n",
    "        last_date = today - timedelta(days=5)\n",
    "\n",
    "    \n",
    "    # --- STEP 2: DEFINE NEW EXTRACTION RANGE ---\n",
    "    start_date = last_date + timedelta(days=1)\n",
    "    end_date = today - timedelta(days=1) # Pull up to yesterday, as today's games aren't finished\n",
    "\n",
    "    start_dt_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_dt_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    if start_date >= end_date:\n",
    "        print(f\"Data is up to date as of {end_dt_str}. No new extraction needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting DAILY Statcast ETL: Pulling data from {start_dt_str} to {end_dt_str}\")\n",
    "    \n",
    "    # --- STEP 3: EXTRACTION ---\n",
    "    try:\n",
    "        df = pyb.statcast(start_dt=start_dt_str, end_dt=end_dt_str)\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            print(\"No new Statcast data retrieved for this date range. Exiting.\")\n",
    "            return\n",
    "\n",
    "        #  --- STEP 4: TRANSFORMATION ---        \n",
    "        # Handle data types before loading (optional, but good practice)\n",
    "        df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "        \n",
    "        # # --- STEP 5: LOADING ---\n",
    "        print(f\"Loading {len(df)} new rows into 'statcast_pitches'...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            'statcast_pitches', \n",
    "            engine, \n",
    "            if_exists='replace', # CRITICAL: Append new data to the existing table\n",
    "            index=False, \n",
    "            chunksize=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully appended {len(df)} new rows of Statcast data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Statcast ETL Failed during extraction or loading: {e}\")\n",
    "        \n",
    "\n",
    "# Execute the daily update\n",
    "update_statcast_data(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_statcast_data(start_date, end_date):\n",
    "    \"\"\"Pulls granular, pitch-by-pitch data for a specified date range.\"\"\"\n",
    "    print(f\"-> Pulling Statcast data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # pybaseball statcast function is designed to handle this extraction\n",
    "    raw_statcast_df = pyb.statcast(start_dt=start_date, end_dt=end_date)\n",
    "    \n",
    "    if raw_statcast_df is None or raw_statcast_df.empty:\n",
    "        print(\"Warning: No Statcast data returned for this date range.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return raw_statcast_df\n",
    "\n",
    "\n",
    "# Example\n",
    "test_start_date = '2025-10-28'\n",
    "test_end_date = '2025-10-30' \n",
    "\n",
    "daily_data = extract_statcast_data(test_start_date, test_end_date)\n",
    "print(f\"Successfully extracted {len(daily_data)} individual pitches/events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_pk: Integer. Game id provided by MLB Advanced Media.\n",
    "# get statcast data for game_pk \n",
    "game_log = pyb.statcast_single_game(813024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b679c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_lahman = pylahman.People()\n",
    "player_chadwick = pyb.chadwick_register()\n",
    "\n",
    "# Join lahman and chadwick on key identifiers and bring all the columns from lahman\n",
    "# Ignore if key_bbref is empty in chadwick\n",
    "players_chadwick_clean = player_chadwick[player_chadwick['key_retro'].notna()]\n",
    "players_lahman_clean   = players_lahman[players_lahman['retroID'].notna()]\n",
    "\n",
    "players_df = pd.merge(\n",
    "    players_chadwick_clean,\n",
    "    players_lahman_clean,\n",
    "    left_on=['key_retro'],\n",
    "    right_on=['retroID'],\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "# Remove unnecesary columns and drop them from the dataframe\n",
    "cols_to_remove = ['retroID', 'bbrefID', 'mlb_played_first', 'mlb_played_last']\n",
    "players_df = players_df.drop(columns= cols_to_remove)\n",
    "\n",
    "# Rename the fields\n",
    "rename_map = {\n",
    "    # IDs\n",
    "    \"key_mlbam\":     \"key_mlbam\",\n",
    "    \"key_retro\":     \"key_retro\",\n",
    "    \"key_bbref\":     \"key_bbref\",\n",
    "    \"key_fangraphs\": \"key_fangraphs\",\n",
    "    \"ID\":            \"id_lahman\",\n",
    "    \"playerID\":      \"player_id_lahman\",\n",
    "\n",
    "    # Names\n",
    "    \"name_last\":     \"last_name_chadwick\",\n",
    "    \"name_first\":    \"first_name_chadwick\",\n",
    "    \"nameLast\":      \"last_name_lahman\",\n",
    "    \"nameFirst\":     \"first_name_lahman\",\n",
    "    \"nameGiven\":     \"first_and_second_name_lahman\",\n",
    "\n",
    "    # Debut/Final game\n",
    "    \"debut\":         \"debut\",\n",
    "    \"finalGame\":     \"final_game\",\n",
    "\n",
    "    # Info\n",
    "    \"weight\":        \"weight\",\n",
    "    \"height\":        \"height\",\n",
    "    \"bats\":          \"bats\",\n",
    "    \"throws\":        \"throws\",\n",
    "\n",
    "    # Birth/Death\n",
    "    \"birthYear\":     \"birth_year\",\n",
    "    \"birthMonth\":    \"birth_month\",\n",
    "    \"birthDay\":      \"birth_day\",\n",
    "    \"birthCity\":     \"birth_city\",\n",
    "    \"birthCountry\":  \"birth_country\",\n",
    "    \"birthState\":    \"birth_state\",\n",
    "    \"deathYear\":     \"death_year\",\n",
    "    \"deathMonth\":    \"death_month\",\n",
    "    \"deathDay\":      \"death_day\",\n",
    "    \"deathCountry\":  \"death_country\",\n",
    "    \"deathState\":    \"death_state\",\n",
    "    \"deathCity\":     \"death_city\",\n",
    "}\n",
    "\n",
    "# Apply the rename\n",
    "players_df = players_df.rename(columns= rename_map)\n",
    "\n",
    "# Order the new columns\n",
    "ordered_cols = [\n",
    "    \"key_mlbam\",\n",
    "    \"key_retro\",\n",
    "    \"key_bbref\",\n",
    "    \"key_fangraphs\",\n",
    "    \"id_lahman\",\n",
    "    \"player_id_lahman\",\n",
    "    \"last_name_chadwick\",\n",
    "    \"first_name_chadwick\",\n",
    "    \"last_name_lahman\",\n",
    "    \"first_name_lahman\",\n",
    "    \"first_and_second_name_lahman\",\n",
    "    \"debut\",\n",
    "    \"final_game\",\n",
    "    \"weight\",\n",
    "    \"height\",\n",
    "    \"bats\",\n",
    "    \"throws\",\n",
    "    \"birth_year\",\n",
    "    \"birth_month\",\n",
    "    \"birth_day\",\n",
    "    \"birth_city\",\n",
    "    \"birth_country\",\n",
    "    \"birth_state\",\n",
    "    \"death_year\",\n",
    "    \"death_month\",\n",
    "    \"death_day\",\n",
    "    \"death_country\",\n",
    "    \"death_state\",\n",
    "    \"death_city\"\n",
    "]\n",
    "\n",
    "# Apply the order\n",
    "players_df = players_df[ordered_cols]\n",
    "\n",
    "# This selects only columns with numbers and fills their nulls with -1\n",
    "numeric_cols = players_df.select_dtypes(include=['number']).columns\n",
    "players_df[numeric_cols] = players_df[numeric_cols].fillna(-1)\n",
    "\n",
    "# Replace nulls in the text columns\n",
    "text_cols = [\n",
    "    \"key_retro\",\n",
    "    \"key_bbref\",\n",
    "    \"player_id_lahman\",\n",
    "    \"last_name_chadwick\",\n",
    "    \"first_name_chadwick\",\n",
    "    \"last_name_lahman\",\n",
    "    \"first_name_lahman\",\n",
    "    \"first_and_second_name_lahman\",\n",
    "    \"bats\",\n",
    "    \"throws\",\n",
    "    \"birth_city\",\n",
    "    \"birth_country\",\n",
    "    \"birth_state\",\n",
    "    \"death_country\",\n",
    "    \"death_state\",\n",
    "    \"death_city\"\n",
    "]\n",
    "\n",
    "# Convert to a standard object type first and then fill the nulls with N/A\n",
    "for col in text_cols:\n",
    "    players_df[col] = players_df[col].astype(object).fillna('N/A')\n",
    "    \n",
    "\n",
    "# List the date columns\n",
    "date_cols = [\n",
    "    \"debut\",\n",
    "    \"final_game\"\n",
    "]\n",
    "# Fill null dates with January 1st, 1700\n",
    "for col in date_cols:\n",
    "    players_df[col] = players_df[col].fillna(pd.Timestamp('1700-01-01'))\n",
    "\n",
    "# Check for nulls in my table - there shouldn't be any\n",
    "if (players_df.isnull().sum() == 0).all():\n",
    "    print(\"‚úÖ No nulls found.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING - There are nulls in some columns in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50721a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify all text columns\n",
    "text_cols = team_franchises.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "# 2. Convert to object FIRST, then fill\n",
    "for col in text_cols:\n",
    "    # Converting to object allows 'N/A' to be treated as a normal string\n",
    "    team_franchises[col] = team_franchises[col].astype(object).fillna('N/A')\n",
    "    \n",
    "    # Just in case some were literal 'nan' strings:\n",
    "    team_franchises[col] = team_franchises[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "# 3. Final Verification with Emojis\n",
    "null_count = team_franchises[text_cols].isnull().sum().sum()\n",
    "if null_count == 0:\n",
    "    print(\"‚úÖ All string columns are clean. No nulls found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {null_count} nulls still remain in text columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info = pylahman.Teams()\n",
    "\n",
    "# Identify all text columns\n",
    "text_cols = team_info.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "# Convert to object first, then fill with N/A\n",
    "for col in text_cols:\n",
    "    # Converting to object allows 'N/A' to be treated as a normal string\n",
    "    team_info[col] = team_info[col].astype(object).fillna('N/A')\n",
    "    \n",
    "    # Just in case some were literal 'nan' strings:\n",
    "    team_info[col] = team_info[col].replace(['nan', 'None', '<NA>'], 'N/A')\n",
    "\n",
    "# This selects only columns with numbers and fills their nulls with -1\n",
    "numeric_cols = team_info.select_dtypes(include=['number']).columns\n",
    "team_info[numeric_cols] = team_info[numeric_cols].fillna(-1)\n",
    "\n",
    "# Final verification\n",
    "null_count_text    = team_info[text_cols].isnull().sum().sum()\n",
    "null_count_numeric = team_info[numeric_cols].isnull().sum().sum()\n",
    "total_nulls        = null_count_text + null_count_numeric\n",
    "\n",
    "if total_nulls == 0:\n",
    "    print(\"‚úÖ All columns are clean. No nulls found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {total_nulls} nulls still remain some columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cff595",
   "metadata": {},
   "source": [
    "### Create model\n",
    "## dim_pitcher_archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dim_pitcher_archetypes updated! Primary Key was preserved.\n"
     ]
    }
   ],
   "source": [
    "#def update_dim_pitcher_archetypes(engine: Engine):\n",
    "\"\"\"\n",
    "Groups pitchers into 8 archetypes and updates the database.\n",
    "Now includes an 'updated_at' column to track the last run date.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Pull unique pitcher stats\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    pitcher,\n",
    "    AVG(release_speed) as avg_velo, \n",
    "    AVG(release_spin_rate) as avg_spin, -- The spin rate of a pitch measured in revolutions per minute (rpm) at the moment of release\n",
    "    AVG(pfx_x) as avg_horiz_mvmt, -- Horizontal movement in feet from the catcher's perspective\n",
    "    AVG(pfx_z) as avg_vert_mvmt -- Vertical movement from the catcher's perpsective.\n",
    "FROM fact_statcast_pitches\n",
    "WHERE release_speed IS NOT NULL \n",
    "    AND release_spin_rate IS NOT NULL\n",
    "    AND pfx_x IS NOT NULL \n",
    "    AND pfx_z IS NOT NULL\n",
    "GROUP BY pitcher\n",
    "HAVING COUNT(*) > 100 \n",
    "\"\"\"\n",
    "pitcher_stats = pd.read_sql(query, engine)\n",
    "\n",
    "# 2. Scale the data\n",
    "scaler = StandardScaler()\n",
    "features = ['avg_velo', 'avg_spin', 'avg_horiz_mvmt', 'avg_vert_mvmt']\n",
    "scaled_data = scaler.fit_transform(pitcher_stats[features])\n",
    "\n",
    "# 3. Create 8 Archetypes\n",
    "kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "pitcher_stats['archetype_id'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# 4. Map IDs and Add Timestamp\n",
    "archetype_map = {\n",
    "    0: \"Power Flamethrower\",\n",
    "    1: \"Sinker / Tail Specialist\",\n",
    "    2: \"Breaking Ball Specialist\",\n",
    "    3: \"Standard Control Righty\",\n",
    "    4: \"Position Player / Eephus\",\n",
    "    5: \"Deceptive Angle Specialist\",\n",
    "    6: \"Low-Spin / Heavy Sinker\",\n",
    "    7: \"Power Slider / Sweeper\"\n",
    "}\n",
    "pitcher_stats['archetype_name'] = pitcher_stats['archetype_id'].map(archetype_map)\n",
    "\n",
    "# Add the current timestamp to every row\n",
    "pitcher_stats['updated_at'] = datetime.now()\n",
    "\n",
    "# 5. Database Update (Truncate and Append)\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        conn.execute(text(\"TRUNCATE TABLE dim_pitcher_archetypes;\"))\n",
    "        conn.commit()\n",
    "        print(\"Refreshing existing dim_pitcher_archetypes table...\")\n",
    "    except Exception:\n",
    "        print(\"Table 'dim_pitcher_archetypes' not found. Creating it for the first time...\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Upload data including the new column\n",
    "pitcher_stats[['pitcher', 'archetype_id', 'archetype_name', 'updated_at']].to_sql(\n",
    "    'dim_pitcher_archetypes', \n",
    "    engine, \n",
    "    if_exists='append', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "# 6. Ensure the Primary Key is set\n",
    "pk_check = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM information_schema.table_constraints \n",
    "WHERE table_name='dim_pitcher_archetypes' AND constraint_type='PRIMARY KEY';\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    has_pk = conn.execute(text(pk_check)).scalar()\n",
    "    if has_pk == 0:\n",
    "        conn.execute(text(\"ALTER TABLE dim_pitcher_archetypes ADD PRIMARY KEY (pitcher);\"))\n",
    "        conn.commit()\n",
    "        print(\"‚úÖ Primary Key (pitcher) established.\")\n",
    "\n",
    "print(f\"‚úÖ Successfully categorized {len(pitcher_stats)} pitchers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970acd7",
   "metadata": {},
   "source": [
    "### \"Luck Score\" ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a807f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_luck_scores(engine, min_ab= 25):\n",
    "    query = \"SELECT * FROM vw_batter_vs_pitcher_archetype\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    #? Filter by At-Bats first to ensure statistical significance\n",
    "    # Even though the SQL view has a filter, we can tighten it here if needed\n",
    "    df = df[df['at_bats'] >= min_ab].copy()\n",
    "    \n",
    "    #? Calculate Ranks (Percentiles)\n",
    "    df['ev_rank'] = df['avg_exit_velo'].rank(pct=True)\n",
    "    df['speed_rank'] = df['avg_sprint_speed'].rank(pct=True)\n",
    "    \n",
    "    # Lower time is BETTER for home_to_first, so we rank descending\n",
    "    df['h1_rank'] = df['avg_home_to_first'].rank(pct=True, ascending=False)\n",
    "    df['ba_rank'] = df['batting_avg'].rank(pct=True)\n",
    "    \n",
    "    # 3. Weighting the \"Potential\" (Expected Performance)\n",
    "    # 60% Exit Velo, 20% Sprint, 20% Home-to-1st\n",
    "    df['potential_score'] = (df['ev_rank'] * 0.6) + (df['speed_rank'] * 0.2) + (df['h1_rank'] * 0.2)\n",
    "    \n",
    "    #? Luck Score Calculation\n",
    "    # A positive score means their physical tools exceed their actual results\n",
    "    # If High Positive (Unlucky) e.g. 0.40\n",
    "    # The player‚Äôs tools are elite (e.g., 90th percentile), but their results are \n",
    "    # poor (e.g., 50th percentile). They are likely hitting into the \"loudest outs\" in the league.\n",
    "\n",
    "    # If Near Zero (Fair) e.g. 0.05\n",
    "    # The player is getting exactly what they deserve. \n",
    "    # Their speed and power perfectly explain their batting average.\n",
    "\n",
    "    # If High Negative (Lucky) e.g. -0.4:\n",
    "    # The player has weak tools (e.g., 30th percentile) but a high batting average (e.g., 70th percentile). \n",
    "    # They are likely benefiting from \"bloop\" hits, defensive errors.\n",
    "    df['luck_score'] = df['potential_score'] - df['ba_rank']\n",
    "    \n",
    "    #? Sample Size Adjustment\n",
    "    # This 'confidence' metric tells you how much you should trust the luck score\n",
    "    \n",
    "    # 0.90 to 1.0 -> High Confidence. This player has faced this archetype many times. \n",
    "    # The luck score is likely a true reflection of their performance.\n",
    "    \n",
    "    # 0.5 -> Moderate. There is enough data to see a trend, \n",
    "    # but it could still be swayed by a single lucky or unlucky game.\n",
    "    \n",
    "    # 0.1 or lower -> Low Certainty. The player has very few At-Bats against this archetype. \n",
    "    # The high luck score might just be \"small-sample noise\".\n",
    "    df['luck_confidence'] = df['at_bats'] / df['at_bats'].max()\n",
    "    \n",
    "    # Add the current timestamp to every row\n",
    "    df['calculation_date'] = datetime.now()\n",
    "    \n",
    "    # Sort by the luckiest (most underperforming) players first\n",
    "    return df.sort_values('luck_score', ascending=False)\n",
    "\n",
    "luck_df = calculate_luck_scores(engine, min_ab= 25)\n",
    "luck_df = luck_df[['batter', 'archetype_name', 'luck_score', 'luck_confidence', 'at_bats', 'calculation_date']]\n",
    "# Example usage:\n",
    "# luck_df = calculate_luck_scores(engine, min_ab=30)\n",
    "# print(luck_df[['batter', 'archetype_name', 'at_bats', 'luck_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297d241",
   "metadata": {},
   "source": [
    "# Get probable starters for the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   game_id              home_team           away_team    home_pitcher  \\\n",
      "0   777505         Detroit Tigers     Cincinnati Reds    Tyler Holton   \n",
      "1   777501      Baltimore Orioles  Los Angeles Angels   Scott Blewett   \n",
      "2   777499  Philadelphia Phillies   Toronto Blue Jays    Zack Wheeler   \n",
      "3   777504         Atlanta Braves    Colorado Rockies    Grant Holmes   \n",
      "4   777503   Washington Nationals       Miami Marlins  MacKenzie Gore   \n",
      "\n",
      "    away_pitcher status                        venue  \n",
      "0     Wade Miley  Final                Comerica Park  \n",
      "1  Yusei Kikuchi  Final  Oriole Park at Camden Yards  \n",
      "2   Jos√© Berr√≠os  Final           Citizens Bank Park  \n",
      "3  Austin Gomber  Final                  Truist Park  \n",
      "4     Eury P√©rez  Final               Nationals Park  \n"
     ]
    }
   ],
   "source": [
    "import statsapi\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get today's date automatically\n",
    "today_str = datetime.now().strftime('%m/%d/%Y')\n",
    "\n",
    "def get_daily_starters(date_str):\n",
    "    # 1. Fetch all games for the specified date\n",
    "    # No team ID means it pulls all 30 teams\n",
    "    schedule = statsapi.schedule(date=date_str)\n",
    "    \n",
    "    games_list = []\n",
    "    \n",
    "    for game in schedule:\n",
    "        # Extract the core info for your Matchup Predictor\n",
    "        game_data = {\n",
    "            \"game_id\": game.get(\"game_id\"),\n",
    "            \"home_team\": game.get(\"home_name\"),\n",
    "            \"away_team\": game.get(\"away_name\"),\n",
    "            \"home_pitcher\": game.get(\"home_probable_pitcher\", \"TBD\"),\n",
    "            \"away_pitcher\": game.get(\"away_probable_pitcher\", \"TBD\"),\n",
    "            \"status\": game.get(\"status\"),\n",
    "            \"venue\": game.get(\"venue_name\")\n",
    "        }\n",
    "        games_list.append(game_data)\n",
    "    \n",
    "    # 2. Convert to DataFrame\n",
    "    df = pd.DataFrame(games_list)\n",
    "    return df\n",
    "\n",
    "# Test with June 15, 2025\n",
    "df_starters = get_daily_starters('6/15/2025')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_starters.head())\n",
    "\n",
    "# Integrating with your \"Matchup Predictor\"\n",
    "# Now that you have this DataFrame, you can loop through the home_pitcher and away_pitcher columns.\n",
    "\n",
    "# For each name:\n",
    "\n",
    "# Look up their Archetype in your database.\n",
    "\n",
    "# Highlight the \"Best Matchups\" for that day \n",
    "# (e.g., \"Today, 3 teams are facing 'Soft Tossers'‚Äîcheck your O's hitters' \n",
    "# luck scores against that archetype\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad2c38",
   "metadata": {},
   "source": [
    "### Get today's lineup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5579ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   game_id            team  batting_order        player_name  player_id  \\\n",
      "0   777505  Detroit Tigers              1    Kerry Carpenter     681481   \n",
      "1   777505  Detroit Tigers              2     Gleyber Torres     650402   \n",
      "2   777505  Detroit Tigers              3       Riley Greene     682985   \n",
      "3   777505  Detroit Tigers              4     Dillon Dingler     693307   \n",
      "4   777505  Detroit Tigers              5  Spencer Torkelson     679529   \n",
      "5   777505  Detroit Tigers              6     Zach McKinstry     656716   \n",
      "6   777505  Detroit Tigers              7      Wenceel P√©rez     672761   \n",
      "7   777505  Detroit Tigers              8        Javier B√°ez     595879   \n",
      "8   777505  Detroit Tigers              9         Colt Keith     690993   \n",
      "\n",
      "  position  \n",
      "0       DH  \n",
      "1       2B  \n",
      "2       LF  \n",
      "3        C  \n",
      "4       1B  \n",
      "5       3B  \n",
      "6       RF  \n",
      "7       SS  \n",
      "8       PH  \n"
     ]
    }
   ],
   "source": [
    "import statsapi\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get today's date automatically\n",
    "today_str = datetime.now().strftime('%m/%d/%Y')\n",
    "\n",
    "def get_lineups_to_df(date_str):\n",
    "    # 1. Get all games for the day to find game_ids\n",
    "    schedule = statsapi.schedule(date=date_str)\n",
    "    \n",
    "    lineup_records = []\n",
    "\n",
    "    for game in schedule:\n",
    "        game_id = game['game_id']\n",
    "        team_names = {\n",
    "            'home': game['home_name'],\n",
    "            'away': game['away_name']\n",
    "        }\n",
    "        \n",
    "        # 2. Pull boxscore data which contains the lineup\n",
    "        try:\n",
    "            box = statsapi.boxscore_data(game_id)\n",
    "            \n",
    "            for side in ['home', 'away']:\n",
    "                # The 'battingOrder' list contains player IDs in order (1-9)\n",
    "                order_ids = box[side].get('battingOrder', [])\n",
    "                \n",
    "                for slot, p_id in enumerate(order_ids, start=1):\n",
    "                    player_info = box[side]['players'][f\"ID{p_id}\"]\n",
    "                    \n",
    "                    lineup_records.append({\n",
    "                        \"game_id\": game_id,\n",
    "                        \"team\": team_names[side],\n",
    "                        \"batting_order\": slot,\n",
    "                        \"player_name\": player_info['person']['fullName'],\n",
    "                        \"player_id\": p_id,\n",
    "                        \"position\": player_info['position']['abbreviation']\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Lineup not yet available for {team_names['away']} @ {team_names['home']}\")\n",
    "\n",
    "    return pd.DataFrame(lineup_records)\n",
    "\n",
    "# Test for a game day (using your June 15 example)\n",
    "df_lineups = get_lineups_to_df('06/15/2025')\n",
    "print(df_lineups.head(9)) # Show the lead-off through 9th hitter for the first game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "739bea17",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['player_id', 'avg_exit_velo'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Player's in the lineup that are due for a hit based on their luck score\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming your Luck Score table is loaded as df_luck\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_today_luck \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m      5\u001b[0m     df_lineups, \n\u001b[1;32m----> 6\u001b[0m     \u001b[43mluck_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplayer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mluck_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_exit_velo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, \n\u001b[0;32m      7\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Display the unluckiest players in today's lineup (due for a hit!)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m due_for_hit \u001b[38;5;241m=\u001b[39m df_today_luck\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mluck_score\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4118\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4119\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4121\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adtpi\\python_environments\\PY_312_DEVELOPMENT\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['player_id', 'avg_exit_velo'] not in index\""
     ]
    }
   ],
   "source": [
    "# Player's in the lineup that are due for a hit based on their luck score\n",
    "\n",
    "# Assuming your Luck Score table is loaded as df_luck\n",
    "df_today_luck = pd.merge(\n",
    "    df_lineups, \n",
    "    luck_df[['player_id', 'luck_score', 'avg_exit_velo']], \n",
    "    on='player_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Display the unluckiest players in today's lineup (due for a hit!)\n",
    "due_for_hit = df_today_luck.sort_values('luck_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db14998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your local archetype data\n",
    "# (In a real app, this comes from your SQL database)\n",
    "df_archetypes = pd.read_csv('pitcher_archetypes.csv') \n",
    "\n",
    "# 2. Get the daily starters from the previous step\n",
    "df_starters = get_daily_starters('06/15/2025')\n",
    "\n",
    "# 3. Merge for Away Pitchers\n",
    "# We rename columns so we know which archetype belongs to which team\n",
    "df_final = pd.merge(\n",
    "    df_starters, \n",
    "    df_archetypes[['player_name', 'archetype']], \n",
    "    left_on='away_pitcher', \n",
    "    right_on='player_name', \n",
    "    how='left'\n",
    ").rename(columns={'archetype': 'away_pitcher_type'}).drop(columns=['player_name'])\n",
    "\n",
    "# 4. Merge for Home Pitchers\n",
    "df_final = pd.merge(\n",
    "    df_final, \n",
    "    df_archetypes[['player_name', 'archetype']], \n",
    "    left_on='home_pitcher', \n",
    "    right_on='player_name', \n",
    "    how='left'\n",
    ").rename(columns={'archetype': 'home_pitcher_type'}).drop(columns=['player_name'])\n",
    "\n",
    "# Display the result\n",
    "print(df_final[['away_team', 'away_pitcher', 'away_pitcher_type', 'home_team', 'home_pitcher', 'home_pitcher_type']])\n",
    "\n",
    "# What this enables in your Dashboard\n",
    "# Now that you have the pitcher_type in your DataFrame, you can add \"Smart Alerts\" to your UI:\n",
    "\n",
    "# Filter Logic: if row['home_pitcher_type'] == 'Breaking Ball Specialist':\n",
    "\n",
    "# UI Trigger: Display a message: \"Gunnar Henderson struggles against this pitcher type. Watch out for low-away sliders today.\"\n",
    "\n",
    "# Heatmap Update: Automatically filter your Matchup Heatmap to only show the column for that specific archetype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7220e",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_advanced_archetypes(df):\n",
    "    # 1. Select features for the models\n",
    "    # We use percentiles and rates to normalize the 'scale' of data\n",
    "    model_features = [\n",
    "        'fb_velo', 'velo_pct', 'avg_spin', 'hb_adj', 'ivb', \n",
    "        'total_whiff_rate', 'whiff_pct', 'chase_pct', \n",
    "        'fb_putaway_pct', 'os_putaway_pct', 'zone_rate'\n",
    "    ]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[model_features])\n",
    "\n",
    "    # 2. Model A: Isolation Forest (The \"Elite/Unicorn\" Detector)\n",
    "    # contamination=0.05 targets the top 5% of 'unique' profiles\n",
    "    iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "    df['is_unicorn'] = iso.fit_predict(scaled_data) # -1 is a Unicorn\n",
    "\n",
    "    # 3. Model B: Gaussian Mixture Model (Archetype Probabilities)\n",
    "    n_archetypes = 7\n",
    "    gmm = GaussianMixture(n_components=n_archetypes, random_state=42, n_init=10)\n",
    "    gmm.fit(scaled_data)\n",
    "    probs = gmm.predict_proba(scaled_data)\n",
    "\n",
    "    # 4. Map Clusters to Human Names\n",
    "    # Note: You should check gmm.means_ to align these perfectly.\n",
    "    cluster_names = [\n",
    "        \"Power Flamethrower\",       # High velo_pct, High fb_putaway\n",
    "        \"Command Specialist\",       # High zone_rate, Low hb_adj\n",
    "        \"Vertical Mover (Rise)\",    # High ivb\n",
    "        \"Horizontal Specialist\",    # High hb_adj (Sweeper/Sinker types)\n",
    "        \"Offspeed Artist\",          # High os_putaway_pct\n",
    "        \"Chase Consultant\",         # High chase_pct, Lower zone_rate\n",
    "        \"Kitchen Sink / Junkball\"   # Average across the board\n",
    "    ]\n",
    "\n",
    "    # 5. The Logic: Multi-Tagging\n",
    "    def assign_archetypes(idx, row):\n",
    "        tags = []\n",
    "        \n",
    "        # Add GMM tags if probability > 20%\n",
    "        for i, p in enumerate(probs[idx]):\n",
    "            if p > 0.20:\n",
    "                tags.append(cluster_names[i])\n",
    "        \n",
    "        # Add Performance-Based \"Elite\" Badges\n",
    "        if row['is_unicorn'] == -1: tags.append(\"ü¶Ñ UNICORN\")\n",
    "        if row['whiff_pct'] >= 95: tags.append(\"Elite Whiff\")\n",
    "        if row['is_dead_zone'] == 1: tags.append(\"Dead Zone FB\")\n",
    "        \n",
    "        return \" | \".join(tags)\n",
    "\n",
    "    df['archetype_labels'] = [assign_archetypes(i, row) for i, row in df.iterrows()]\n",
    "    return df\n",
    "\n",
    "# Assuming 'pitcher_df' is the result of your SQL query\n",
    "final_archetypes = run_advanced_archetypes(pitcher_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426c189",
   "metadata": {},
   "source": [
    "# Old pitcher archetype model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71ea6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_scouting_model(df):\n",
    "    \"\"\"\n",
    "    Processes pitcher data through GMM clustering, anomaly detection, \n",
    "    and heuristic tagging to assign archetypes.\n",
    "    \"\"\"\n",
    "    # 1. Advanced Feature Engineering\n",
    "    # Stuff Score: Weights the physical 'lethality' of the pitches\n",
    "    # Precision Score: Weights the control and ability to neutralize hitters\n",
    "    df['stuff_score'] = (\n",
    "        (df['velo_pct'] * 0.35) + \n",
    "        (df['extension_pct'] * 0.25) + \n",
    "        (df['iz_whiff_pct'] * 0.40)\n",
    "    )\n",
    "    \n",
    "    df['precision_score'] = (\n",
    "        (df['command_pct'] * 0.40) + \n",
    "        (df['neutrality_pct'] * 0.30) + \n",
    "        (df['paint_pct'] * 0.30)\n",
    "    )\n",
    "\n",
    "    # 2. Prep for Machine Learning Models\n",
    "    model_features = [\n",
    "        'fb_velo', 'avg_extension', 'vaa_proxy', \n",
    "        'raw_iz_whiff', 'k_bb_rate', 'stuff_score', 'precision_score'\n",
    "    ]\n",
    "    \n",
    "    # Scale data for GMM and Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[model_features])\n",
    "\n",
    "    # 3. Isolation Forest (Detecting Unicorns/Outliers)\n",
    "    iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "    df['is_unicorn'] = iso.fit_predict(scaled_data)\n",
    "\n",
    "    # 4. GMM (Style Clustering)\n",
    "    gmm = GaussianMixture(n_components=7, random_state=42)\n",
    "    df['style_cluster'] = gmm.fit_predict(scaled_data)\n",
    "\n",
    "    # 5. Heuristic Report Generation\n",
    "    def generate_report(row):\n",
    "        tags = []\n",
    "        summary_parts = []\n",
    "        \n",
    "        # Style mapping remains the same\n",
    "        style_map = {0:\"Power\", 1:\"Command\", 2:\"Vertical\", 3:\"Horizontal\", \n",
    "                    4:\"Offspeed\", 5:\"Chase\", 6:\"Finesse\"}\n",
    "        style = style_map[row['style_cluster']]\n",
    "        tags.append(style)\n",
    "\n",
    "        # A. THE ELITE ACE (Tier 1)\n",
    "        if row['stuff_score'] >= 85 and row['precision_score'] >= 70:\n",
    "            tags.append(\"üëë ELITE ACE\")\n",
    "            summary_parts.append(\"A rare front-line starter combining dominant stuff with elite zone mastery.\")\n",
    "        \n",
    "        # B. STUFF MONSTER (Tier 1 physical)\n",
    "        elif row['stuff_score'] >= 92:\n",
    "            tags.append(\"üî• STUFF MONSTER\")\n",
    "            summary_parts.append(\"Possesses an overpowering arsenal that generates elite swing-and-miss.\")\n",
    "\n",
    "        # C. UNICORN STATUS (Geometry/Physics Outlier)\n",
    "        if row['is_unicorn'] == -1:\n",
    "            tags.append(\"ü¶Ñ UNICORN\")\n",
    "            summary_parts.append(\"Mathematically unique physical profile; outlier release or movement traits.\")\n",
    "\n",
    "        # D. FLAT-ANGLE SOUTHPAW (Archetype specific)\n",
    "        if row['p_throws'] == 'L' and row['vaa_proxy'] < -4.0 and row['stuff_score'] > 75:\n",
    "            tags.append(\"üìâ FLAT-ANGLE LHP\")\n",
    "            summary_parts.append(\"Deceptive low-slot lefty with a 'rising' fastball profile.\")\n",
    "\n",
    "        # E. NEUTRALIZER\n",
    "        if row['neutrality_pct'] >= 85:\n",
    "            tags.append(\"‚öñÔ∏è NEUTRALIZER\")\n",
    "            summary_parts.append(\"Negates platoon advantages; equally effective against LHB and RHB.\")\n",
    "\n",
    "        # FIX: Final Summary Construction\n",
    "        if summary_parts:\n",
    "            # Join the high-value descriptions into one cohesive paragraph\n",
    "            summary = \" \".join(summary_parts)\n",
    "        else:\n",
    "            # Default fallback only if NO special traits were found\n",
    "            summary = f\"Reliable {style} pitcher with league-average traits.\"\n",
    "\n",
    "        return \" | \".join(list(set(tags))), summary\n",
    "\n",
    "    # Apply tagging\n",
    "    results = df.apply(generate_report, axis=1)\n",
    "    df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_dim_pitcher_archetypes(engine):\n",
    "    \"\"\"\n",
    "    SQL to extract the necessary metrics for the Python model.\n",
    "    \"\"\"\n",
    "    query = text(\"\"\"\n",
    "    WITH aggregated_stats AS (\n",
    "        SELECT \n",
    "            p.pitcher,\n",
    "            p.p_throws,\n",
    "            -- Velocity & Movement\n",
    "            ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'FS', 'FO', 'SI') THEN p.release_speed END)::numeric, 1) as fb_velo,\n",
    "            ROUND(AVG(p.release_extension)::numeric, 2) as avg_extension,\n",
    "            \n",
    "            -- VAA Calculation (Statcast Physics)\n",
    "            -- Formula: -arctan(vz_f / vy_f) * (180/pi). vy_f is final y-velo, vz_f is final z-velo.\n",
    "            ROUND(AVG(-ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / \n",
    "            (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159))::numeric, 2) as vaa_proxy,\n",
    "\n",
    "            -- Skill Metrics\n",
    "            COALESCE(SUM(CASE WHEN p.zone <= 9 AND p.description IN ('swinging_strike', 'swinging_strike_blocked') THEN 1 ELSE 0 END)::numeric / \n",
    "            NULLIF(SUM(CASE WHEN p.zone <= 9 AND p.description IN ('swinging_strike', 'foul', 'foul_tip', 'hit_into_play', 'swinging_strike_blocked') THEN 1 ELSE 0 END), 0), 0) as raw_iz_whiff,\n",
    "            \n",
    "            COALESCE(SUM(CASE WHEN p.zone IN (11, 12, 13, 14) THEN 1 ELSE 0 END)::numeric / COUNT(*), 0) as raw_paint,\n",
    "            \n",
    "            -- Platoon Split (wOBA Difference)\n",
    "            ABS(\n",
    "            COALESCE(SUM(CASE WHEN p.stand = 'L' THEN p.woba_value END) / NULLIF(SUM(CASE WHEN p.stand = 'L' THEN 1 ELSE 0 END), 0), 0) -\n",
    "            COALESCE(SUM(CASE WHEN p.stand = 'R' THEN p.woba_value END) / NULLIF(SUM(CASE WHEN p.stand = 'R' THEN 1 ELSE 0 END), 0), 0)\n",
    "            ) as platoon_split_abs,\n",
    "\n",
    "            -- Command: K-BB%\n",
    "            (SUM(CASE WHEN p.events = 'strikeout' THEN 1 ELSE 0 END)::numeric - \n",
    "                SUM(CASE WHEN p.events IN ('walk', 'hit_by_pitch') THEN 1 ELSE 0 END)::numeric) / \n",
    "                NULLIF(COUNT(DISTINCT p.at_bat_number), 0) as k_bb_rate,\n",
    "\n",
    "            COALESCE(SUM(CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked') THEN 1 ELSE 0 END)::numeric / \n",
    "                NULLIF(SUM(CASE WHEN p.description IN ('swinging_strike', 'foul', 'foul_tip', 'hit_into_play', 'swinging_strike_blocked') THEN 1 ELSE 0 END), 0), 0) as raw_whiff,\n",
    "            \n",
    "            COALESCE(SUM(CASE WHEN p.launch_speed >= 95 THEN 1 ELSE 0 END)::numeric / \n",
    "                NULLIF(SUM(CASE WHEN p.launch_speed IS NOT NULL THEN 1 ELSE 0 END), 0), 0) as raw_hard_hit\n",
    "\n",
    "        FROM fact_statcast_pitches p\n",
    "        GROUP BY p.pitcher, p.p_throws\n",
    "        HAVING COUNT(*) > 100 AND AVG(p.release_speed) > 86\n",
    "    ),\n",
    "    ranked_stats AS (\n",
    "        SELECT \n",
    "            ast.*,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY fb_velo))::numeric, 2) * 100 as velo_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY raw_whiff))::numeric, 2) * 100 as whiff_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY avg_extension))::numeric, 2) * 100 as extension_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY k_bb_rate))::numeric, 2) * 100 as command_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY platoon_split_abs DESC))::numeric, 2) * 100 as neutrality_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY raw_iz_whiff))::numeric, 2) * 100 as iz_whiff_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY raw_paint))::numeric, 2) * 100 as paint_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY raw_hard_hit DESC))::numeric, 2) * 100 as suppression_pct\n",
    "        FROM aggregated_stats ast\n",
    "    )\n",
    "    SELECT \n",
    "        CONCAT(pn.first_name_chadwick, ' ', pn.last_name_chadwick) as full_name,\n",
    "        rs.*\n",
    "    FROM ranked_stats rs\n",
    "    JOIN dim_player pn ON rs.pitcher = pn.key_mlbam;\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return run_scouting_model(df)\n",
    "\n",
    "# Execute\n",
    "pitcher_archetypes = update_dim_pitcher_archetypes(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Drivers for your Clusters:\n",
      "k_bb_rate        0.436710\n",
      "raw_iz_whiff     0.355542\n",
      "fb_velo          0.305364\n",
      "avg_extension    0.149583\n",
      "vaa_proxy        0.050298\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def explain_clusters_stable(df, model_features):\n",
    "    # 1. Clean Data\n",
    "    X = df[model_features].values\n",
    "    y = df['style_cluster'].values.astype(int)\n",
    "\n",
    "    # 2. Use Random Forest (More stable with various NumPy/SciPy versions)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # 3. Calculate Permutation Importance\n",
    "    # This identifies which features define the clusters\n",
    "    result = permutation_importance(rf, X, y, n_repeats=10, random_state=42)\n",
    "    \n",
    "    # Map importance back to feature names\n",
    "    feature_importance = pd.Series(result.importances_mean, index=model_features)\n",
    "    \n",
    "    return rf, feature_importance\n",
    "\n",
    "# Execute\n",
    "model_features = ['fb_velo', 'avg_extension', 'vaa_proxy', 'raw_iz_whiff', 'k_bb_rate']\n",
    "rf_model, trait_importance = explain_clusters_stable(pitcher_archetypes, model_features)\n",
    "\n",
    "print(\"Top Drivers for your Clusters:\")\n",
    "print(trait_importance.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d7f04",
   "metadata": {},
   "source": [
    "# Pitcher archetype model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8deb066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_scouting_model(df):\n",
    "    \"\"\"\n",
    "    Synthesizes pitching identity (GMM) with performance outcomes (Whiff/Barrel)\n",
    "    to generate advanced scouting archetypes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identity Features (Physical & Tactical only)\n",
    "    # These define HOW he pitches, not how well he's doing.\n",
    "    identity_features = [\n",
    "        'ffour_usage', 'sinker_usage', 'bb_usage', 'offspeed_usage',\n",
    "        'ffour_vaa_pct', 'velo_gap_pct', 'command_pct', 'paint_pct'\n",
    "    ]\n",
    "    \n",
    "    # 2. Effectiveness Scores (Combining Inputs + Outcomes)\n",
    "    # This helps us rank pitchers within their style groups\n",
    "    df['lethality_score'] = (\n",
    "        (df['whiff_pct'] * 0.70) + \n",
    "        (df['suppression_pct'] * 0.20) + \n",
    "        (df['velo_pct'] * 0.10)\n",
    "    ).round(1)\n",
    "\n",
    "    # 3. Clustering (Archetype Definition)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_identity = scaler.fit_transform(df[identity_features].fillna(0))\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=7, random_state=42)\n",
    "    df['style_cluster'] = gmm.fit_predict(scaled_identity)\n",
    "\n",
    "    # 4. Outlier Detection (Unicorns)\n",
    "    iso = IsolationForest(contamination=0.04, random_state=42)\n",
    "    df['is_unicorn'] = iso.fit_predict(scaled_identity)\n",
    "    \n",
    "    # def calculate_pitcher_grade(row):\n",
    "    #     # Pillar 1: Lethality (Results)\n",
    "    #     lethality = (row['whiff_pct'] * 0.7 + row['suppression_pct'] * 0.3)\n",
    "        \n",
    "    #     # Pillar 2: Physicality (Tools)\n",
    "    #     physicality = (row['velo_pct'] * 0.55 + row['ffour_vaa_pct'] * 0.45)\n",
    "        \n",
    "    #     # Pillar 3: Execution (Stability)\n",
    "    #     execution = (row['command_pct'] * 0.8 + row['neutrality_pct'] * 0.2)\n",
    "        \n",
    "    #     # Weighted GPA Score (0-100)\n",
    "    #     gpa_score = (lethality * 0.40) + (physicality * 0.30) + (execution * 0.20)\n",
    "        \n",
    "    #     # Letter Grade Mapping\n",
    "    #     if gpa_score   >= 90: return 'A+'\n",
    "    #     elif gpa_score >= 80: return 'A'\n",
    "    #     elif gpa_score >= 70: return 'B'\n",
    "    #     elif gpa_score >= 55: return 'C'\n",
    "    #     elif gpa_score >= 40: return 'D'\n",
    "    #     else: return 'F'\n",
    "\n",
    "    # # Apply the grade\n",
    "    # df['overall_grade'] = df.apply(calculate_pitcher_grade, axis=1)\n",
    "\n",
    "    # 5. Advanced Tagging Logic\n",
    "    def generate_scouting_report(row):\n",
    "        tags = []\n",
    "        # Initialize Summary with the Grade\n",
    "        #grade = row['overall_grade']\n",
    "        # summary = f\"[{grade} GRADE] \" \n",
    "        summary = \"\"\n",
    "\n",
    "        \n",
    "        # if grade == 'A+':\n",
    "        #     summary += \"A franchise cornerstone with virtually no statistical weaknesses. \"\n",
    "        # elif grade == 'A':\n",
    "        #     summary += \"Top-of-the-rotation talent with elite specialized tools. \"\n",
    "        # elif grade == 'B':\n",
    "        #     summary += \"Above-average contributor with a reliable floor. \"\n",
    "        # elif grade == 'F':\n",
    "        #     summary += \"High-risk profile requiring significant mechanical or tactical overhaul. \"\n",
    "        \n",
    "        # Style Definitions\n",
    "        styles = {\n",
    "            0: \"Power Arsenal\", 1: \"Precision Specialist\", 2: \"High-Rise Vertical\",\n",
    "            3: \"Groundball/Sinker\", 4: \"Deceptive Offspeed\", 5: \"Sweeper/Chase\", \n",
    "            6: \"Hybrid/Workhorse\"\n",
    "        }\n",
    "        main_style = styles[row['style_cluster']]\n",
    "        tags.append(main_style)\n",
    "\n",
    "        # A. THE PERFORMANCE ELITE\n",
    "        if row['lethality_score'] >= 85:\n",
    "            tags.append(\"üëë ELITE DOMINATOR\")\n",
    "            summary += \"Demonstrates league-leading ability to miss bats and suppress contact. \"\n",
    "        elif row['whiff_pct'] >= 85 and row['suppression_pct'] >= 85:\n",
    "            tags.append(\"üíé EFFICIENCY KING\")\n",
    "            summary += \"Rare dual-threat: elite whiff generation paired with barrel avoidance. \"\n",
    "\n",
    "        # B. THE CONTACT SUPPRESSOR (The 'Maddux' Profile)\n",
    "        if row['suppression_pct'] >= 90 and row['whiff_pct'] < 50:\n",
    "            tags.append(\"üõ°Ô∏è CONTACT MANAGER\")\n",
    "            summary += \"Survives by inducing weak contact; masterful at staying off the barrel. \"\n",
    "\n",
    "        # C. THE GLASS CANNON (High Whiff, High Barrels)\n",
    "        if row['whiff_pct'] >= 80 and row['suppression_pct'] < 30:\n",
    "            tags.append(\"‚ö†Ô∏è GLASS CANNON\")\n",
    "            summary += \"High-risk, high-reward. Dominates with whiffs but prone to loud contact when hit. \"\n",
    "\n",
    "        # D. MECHANICAL UNICORN\n",
    "        if row['is_unicorn'] == -1:\n",
    "            tags.append(\"ü¶Ñ UNICORN\")\n",
    "            summary += \"Possesses a highly unique mechanical/usage profile that defies standard categorization. \"\n",
    "\n",
    "        # E. THE NEUTRALIZER\n",
    "        if row['neutrality_pct'] >= 95:\n",
    "            tags.append(\"‚öñÔ∏è NEUTRALIZER\")\n",
    "            summary += \"Shows zero performance drop-off regardless of batter handedness. \"\n",
    "\n",
    "        # F. STRATEGIC BREAKOUT (From SQL)\n",
    "        if row['breakout_potential'] != 'OPTIMIZED':\n",
    "            tags.append(\"üöÄ BREAKOUT CANDIDATE\")\n",
    "            summary += f\"Data suggests significant upside if usage is shifted toward: {row['breakout_potential']}. \"\n",
    "\n",
    "        # G. NEW: ULTRA-POWER OVERRIDE\n",
    "        # This handles the \"97 mph in Deceptive Offspeed\" scenario\n",
    "        if row['fb_velo'] >= 97:\n",
    "            tags.append(\"‚ö° ULTRA-POWER\")\n",
    "            if main_style == \"Power Arsenal\":\n",
    "                tags.append(\"üî® THOR\")\n",
    "                summary += \"A 'Thor' profile: Elite 97+ mph velocity combined with an aggressive power philosophy. \"\n",
    "            else:\n",
    "                summary += f\"Note: Features elite 97+ mph heat, though currently utilizes a {main_style} tactical approach. \"\n",
    "\n",
    "        # Final Polish\n",
    "        if not summary:\n",
    "            summary = f\"A reliable {main_style} with average league effectiveness.\"\n",
    "            \n",
    "        return \" | \".join(list(set(tags))), summary.strip()\n",
    "\n",
    "    # Apply Logic\n",
    "    results = df.apply(generate_scouting_report, axis=1)\n",
    "    df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_dim_pitcher_archetypes(engine):\n",
    "    \"\"\"\n",
    "    SQL to extract the necessary metrics for the Python model.\n",
    "    \"\"\"\n",
    "    query = text(\"\"\"\n",
    "    WITH attack_zone_stats AS (\n",
    "        SELECT \n",
    "            p.*,\n",
    "            -- Define Command/Paint Zones\n",
    "            CASE \n",
    "                WHEN ABS(p.plate_x) <= 0.67 AND p.plate_z BETWEEN (p.sz_bot + 0.33) AND (p.sz_top - 0.33) THEN 'heart'\n",
    "                WHEN ABS(p.plate_x) <= 1.1 AND p.plate_z BETWEEN (p.sz_bot - 0.33) AND (p.sz_top + 0.33) THEN 'shadow'\n",
    "                WHEN ABS(p.plate_x) <= 1.5 AND p.plate_z BETWEEN (p.sz_bot - 0.75) AND (p.sz_top + 0.75) THEN 'chase'\n",
    "                ELSE 'waste'\n",
    "            END as attack_zone,\n",
    "            -- Whiff Logic: 1 if the result was a swinging strike, 0 otherwise\n",
    "            CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked', 'missed_bunt') THEN 1 ELSE 0 END as is_whiff,\n",
    "            -- Swing Logic: 1 if the batter attempted a swing\n",
    "            CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked', 'missed_bunt', 'foul', 'foul_tip', 'hit_into_play') THEN 1 ELSE 0 END as is_swing\n",
    "        FROM fact_statcast_pitches p\n",
    "    ),\n",
    "    aggregated_stats AS (\n",
    "        SELECT \n",
    "            p.pitcher,\n",
    "            p.p_throws,\n",
    "            COUNT(*) as total_pitches,        \n",
    "            -- NEW RAW METRICS: Whiff & Barrel\n",
    "            -- Whiff Rate: Whiffs / Swings\n",
    "            ROUND(100.0 * SUM(p.is_whiff) / NULLIF(SUM(p.is_swing), 0), 2) as whiff_rate_raw,\n",
    "            -- Barrel Rate: Barrels (launch_speed_angle = 6) / Batted Ball Events (type = 'X')\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.launch_speed_angle = 6 THEN 1 ELSE 0 END) / \n",
    "                NULLIF(SUM(CASE WHEN p.type = 'X' THEN 1 ELSE 0 END), 0), 2) as barrel_rate_raw,        \n",
    "            -- Existing Metrics: Command & Paint\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.attack_zone = 'shadow' THEN 1 ELSE 0 END) / COUNT(*), 1) as paint_raw,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.attack_zone IN ('shadow', 'chase') THEN 1 ELSE 0 END) / COUNT(*), 1) as command_raw,        \n",
    "            -- Existing Metrics: Neutrality (Platoon)\n",
    "            AVG(CASE WHEN p.stand = 'L' THEN p.estimated_woba_using_speedangle END) as xwoba_vs_lhb,\n",
    "            AVG(CASE WHEN p.stand = 'R' THEN p.estimated_woba_using_speedangle END) as xwoba_vs_rhb,\n",
    "            -- Velocity Grouping\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed END)::numeric, 1), 0) as fb_velo,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN p.release_speed END)::numeric, 1), 0) as offspeed_velo,                  \n",
    "            -- Usage Percentages\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FC') THEN 1 ELSE 0 END) / COUNT(*), 1) as ffour_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('SI', 'FT') THEN 1 ELSE 0 END) / COUNT(*), 1) as sinker_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('CU', 'SL', 'KC', 'ST', 'SV', 'CS', 'KN') THEN 1 ELSE 0 END) / COUNT(*), 1) as bb_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN 1 ELSE 0 END) / COUNT(*), 1) as offspeed_usage,       \n",
    "            -- Velocity Gap\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed END)::numeric - \n",
    "                        AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN p.release_speed END)::numeric, 1), 0) as velo_gap,       \n",
    "            -- VAA Categories\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FC') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as ffour_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('SI', 'FT') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as sinker_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CU', 'SL', 'KC', 'ST', 'SV', 'CS', 'KN') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as bb_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'EP') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as offspeed_vaa\n",
    "\n",
    "        FROM attack_zone_stats p\n",
    "        GROUP BY p.pitcher, p.p_throws\n",
    "        HAVING COUNT(*) > 100 AND AVG(p.release_speed) > 84\n",
    "    ),\n",
    "    ranked_stats AS (\n",
    "        SELECT \n",
    "            ast.*,\n",
    "            -- Percentile Rankings\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY fb_velo))::numeric, 2) * 100 as velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY offspeed_velo))::numeric, 2) * 100, 0) as offspeed_velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY velo_gap))::numeric, 2) * 100, 0) as velo_gap_pct,                  \n",
    "            -- NEW RANKINGS: Whiff & Suppression\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY whiff_rate_raw))::numeric, 2) * 100 as whiff_pct,\n",
    "            -- We order barrel_rate DESC because a lower rate is better (higher percentile)\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY barrel_rate_raw DESC))::numeric, 2) * 100 as suppression_pct,\n",
    "            -- Existing Rankings\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY command_raw))::numeric, 2) * 100 as command_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY paint_raw))::numeric, 2) * 100 as paint_pct,\n",
    "            ROUND((100 - (ABS(COALESCE(xwoba_vs_lhb, 0.320) - COALESCE(xwoba_vs_rhb, 0.320)) * 100))::numeric, 2) as neutrality_pct,\n",
    "            -- VAA Percentile Rankings\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (ffour_usage > 0) ORDER BY ffour_vaa))::numeric, 2) * 100, 0) as ffour_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (sinker_usage > 0) ORDER BY sinker_vaa DESC))::numeric, 2) * 100, 0) as sinker_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (bb_usage > 0) ORDER BY bb_vaa DESC))::numeric, 2) * 100, 0) as bb_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY offspeed_vaa DESC))::numeric, 2) * 100, 0) as offspeed_vaa_pct\n",
    "        FROM aggregated_stats ast\n",
    "    )\n",
    "    SELECT \n",
    "        CONCAT(pn.first_name_chadwick, ' ', pn.last_name_chadwick) as full_name,\n",
    "        rs.*,\n",
    "        -- Final Pitch Quality Scores\n",
    "        ROUND((velo_pct * 0.25 + ffour_vaa_pct * 0.25 + whiff_pct * 0.5), 0) as ffour_quality_score,\n",
    "        ROUND((offspeed_velo_pct * 0.25 + offspeed_vaa_pct * 0.25 + whiff_pct * 0.5), 0) as offspeed_quality_score,   \n",
    "        -- Breakout Logic\n",
    "        CASE \n",
    "            WHEN (ffour_vaa_pct > 80 AND ffour_usage < 20) THEN 'UNDERUSED ELITE FASTBALL'\n",
    "            WHEN (bb_vaa_pct > 80 AND bb_usage < 15) THEN 'UNDERUSED ELITE BREAKING'\n",
    "            WHEN (offspeed_vaa_pct > 80 AND offspeed_usage < 15) THEN 'UNDERUSED ELITE OFFSPD'\n",
    "            ELSE 'OPTIMIZED'\n",
    "        END as breakout_potential\n",
    "    FROM ranked_stats rs\n",
    "    JOIN dim_player pn ON rs.pitcher = pn.key_mlbam\n",
    "    ORDER BY ffour_quality_score DESC;\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    return run_scouting_model(df)\n",
    "\n",
    "# Execute\n",
    "pitcher_archetypes = update_dim_pitcher_archetypes(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51108be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_pitcher_clusters(df):\n",
    "    \"\"\"\n",
    "    Creates a 2D scatter plot of the clusters and a distribution breakdown.\n",
    "    \"\"\"\n",
    "    # 1. Prepare Data (Same features used in the GMM)\n",
    "    model_features = [\n",
    "        'ffour_usage', 'sinker_usage', 'bb_usage', 'offspeed_usage',\n",
    "        'ffour_vaa_pct', 'velo_gap_pct', 'command_pct', 'neutrality_pct'\n",
    "    ]\n",
    "    \n",
    "    # Scale and Reduce Dimensions\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[model_features].fillna(0))\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_results = pca.fit_transform(scaled_data)\n",
    "    df['pca_1'] = pca_results[:, 0]\n",
    "    df['pca_2'] = pca_results[:, 1]\n",
    "\n",
    "    # 2. Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    scatter = sns.scatterplot(\n",
    "        x='pca_1', y='pca_2', \n",
    "        hue='style_cluster', \n",
    "        style='is_unicorn',\n",
    "        data=df, \n",
    "        palette='viridis', \n",
    "        s=100, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add labels for specific \"Unicorns\" or top pitchers\n",
    "    top_pitchers = df.nlargest(5, 'stuff_score')\n",
    "    for i, row in top_pitchers.iterrows():\n",
    "        plt.text(row['pca_1'] + 0.02, row['pca_2'], row['full_name'], fontsize=9)\n",
    "\n",
    "    plt.title('MLB Pitcher Archetype Clusters (PCA Projection)', fontsize=15)\n",
    "    plt.xlabel('Principal Component 1 (Style Variance)')\n",
    "    plt.ylabel('Principal Component 2 (Mechanical Variance)')\n",
    "    plt.legend(title='Cluster ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Print Cluster Summary\n",
    "    print(\"### Cluster Archetype Breakdown ###\")\n",
    "    summary = df.groupby('style_cluster')[model_features].mean().round(1)\n",
    "    print(summary)\n",
    "\n",
    "# Call the function\n",
    "visualize_pitcher_clusters(pitcher_archetypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95215c7b",
   "metadata": {},
   "source": [
    "### Test new archetype for pitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f61468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_scouting_model(df):\n",
    "    \"\"\"\n",
    "    Synthesizes pitching identity (GMM) with performance outcomes (Whiff/Barrel),\n",
    "    Perceived Power (Extension), and Vertical Separation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identity Features (Physical & Tactical only)\n",
    "    identity_features = [\n",
    "        'ffour_usage', 'sinker_usage', 'bb_usage', 'offspeed_usage',\n",
    "        'ffour_vaa_pct', 'sinker_vaa_pct', 'bb_vaa_pct', 'offspeed_vaa_pct',\n",
    "        'velo_gap_pct', 'command_pct', 'paint_pct'\n",
    "    ]\n",
    "    \n",
    "    # 2. Effectiveness Scores\n",
    "    df['lethality_score'] = (\n",
    "        (df['whiff_pct'] * 0.75) + \n",
    "        (df['suppression_pct'] * 0.20) + \n",
    "        (df['velo_pct'] * 0.05)\n",
    "    ).round(1)\n",
    "\n",
    "    # 3. Clustering (Archetype Definition)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_identity = scaler.fit_transform(df[identity_features].fillna(0))\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=7, random_state=42)\n",
    "    df['style_cluster'] = gmm.fit_predict(scaled_identity)\n",
    "\n",
    "    # 4. Outlier Detection (Unicorns)\n",
    "    iso = IsolationForest(contamination=0.04, random_state=42)\n",
    "    df['is_unicorn'] = iso.fit_predict(scaled_identity)\n",
    "    \n",
    "    # def calculate_pitcher_grade(row):\n",
    "    #     # 1. Lethality (Results)\n",
    "    #     # We give more weight to Suppression for Starters to reward Skubal's profile\n",
    "    #     lethality = (row['whiff_pct'] * 0.5 + row['suppression_pct'] * 0.4 + row['movement_gap_pct'] * 0.1)\n",
    "        \n",
    "    #     # 2. Physicality (Tools)\n",
    "    #     physicality = (row['perceived_velo_pct'] * 0.45 + row['ffour_vaa_pct'] * 0.45 + row['extension_pct'] * 0.1)\n",
    "        \n",
    "    #     # 3. Execution (Stability)\n",
    "    #     #execution = (row['command_pct'] * 0.45 + row['paint_pct'] * 0.45 + row['neutrality_pct'] * 0.1)\n",
    "        \n",
    "    #     # Execution: Now includes Tunneling (0-100 percentile)\n",
    "    #     # Give Tunneling 20% of the Execution weight\n",
    "    #     execution = (row['command_pct'] * 0.35 + \n",
    "    #                  row['paint_pct'] * 0.35 + \n",
    "    #                  row['tunnel_pct'] * 0.20 + \n",
    "    #                  row['neutrality_pct'] * 0.10)\n",
    "        \n",
    "    #     # 4. Weighted GPA Score\n",
    "    #     base_gpa = (lethality * 0.50) + (physicality * 0.30) + (execution * 0.20)\n",
    "        \n",
    "    #     # 5. THE STARTER CURVE (The \"Skubal Adjustment\")\n",
    "    #     # If they are a starter, we boost their GPA by 5 points to account for the difficulty of volume\n",
    "    #     if row['is_starter'] == 1:\n",
    "    #         base_gpa += 5\n",
    "        \n",
    "    #     # 2. THE \"SAMPLE SIZE\" PENALTY (NEW)\n",
    "    #     # If a pitcher has fewer than 5 appearances, they cannot get an A+ \n",
    "    #     # because the data isn't \"stable\" yet.\n",
    "    #     if row['total_appearances'] < 5:\n",
    "    #         base_gpa -= 10\n",
    "        \n",
    "    #     # 6. Final Mapping (More generous thresholds for A+)\n",
    "    #     if base_gpa >= 85: return 'A+'\n",
    "    #     elif base_gpa >= 75: return 'A'\n",
    "    #     elif base_gpa >= 63: return 'B'  # Slightly wider B range\n",
    "    #     elif base_gpa >= 50: return 'C'\n",
    "    #     elif base_gpa >= 35: return 'D'\n",
    "    #     else: return 'F'\n",
    "    \n",
    "    def calculate_pitcher_grade(row):\n",
    "        # 1. STUFF+ (Physicality)\n",
    "        # This is the \"Weapon\" - 60% of the overall grade\n",
    "        stuff_plus = row['stuff_plus_pct']\n",
    "        \n",
    "        # 2. LOCATION+ (Surgicality)\n",
    "        # This is the \"Aim\" - 40% of the overall grade\n",
    "        location_plus = row['location_plus_pct']\n",
    "        \n",
    "        # 3. PITCHING+ (The Master Score)\n",
    "        # Note: We weigh Stuff higher because it's harder to find/teach\n",
    "        base_score = (stuff_plus * 0.60) + (location_plus * 0.40)\n",
    "        \n",
    "        # 4. VOLUME/STARTER ADJUSTMENTS\n",
    "        if row['is_starter'] == 1:\n",
    "            base_score += 5  # The \"Skubal Boost\"\n",
    "        \n",
    "        if row['total_appearances'] < 5:\n",
    "            base_score -= 10 # The \"Sample Size Penalty\"\n",
    "\n",
    "        # 5. FINAL LETTER GRADE\n",
    "        if base_score >= 85: \n",
    "            grade = 'A+'\n",
    "        elif base_score >= 75: \n",
    "            grade = 'A'\n",
    "        elif base_score >= 60: \n",
    "            grade = 'B'\n",
    "        elif base_score >= 45: \n",
    "            grade = 'C'\n",
    "        else: \n",
    "            grade = 'F'\n",
    "            \n",
    "        return grade, stuff_plus, location_plus\n",
    "\n",
    "    #df['overall_grade'] = df.apply(calculate_pitcher_grade, axis=1)\n",
    "    df[['overall_grade', 'stuff_plus_final', 'location_plus_final']] = df.apply(\n",
    "        lambda x: pd.Series(calculate_pitcher_grade(x)), axis=1\n",
    "    )\n",
    "\n",
    "    # def generate_scouting_report(row):\n",
    "    #     tags = []\n",
    "    #     summary = f\"[{row['overall_grade']} GRADE] \"\n",
    "        \n",
    "    #     # 1. Framework Identity\n",
    "    #     s_plus = row['stuff_plus_pct']\n",
    "    #     l_plus = row['location_plus_pct']\n",
    "        \n",
    "    #     # THE \"STUFF\" LABELS\n",
    "    #     if s_plus >= 90:\n",
    "    #         tags.append(\"üí£ PURE FILTH\")\n",
    "    #     elif s_plus <= 20:\n",
    "    #         tags.append(\"üìâ LACKS BITE\")\n",
    "\n",
    "    #     # THE \"LOCATION\" LABELS\n",
    "    #     if l_plus >= 90:\n",
    "    #         tags.append(\"üéØ SURGEON\")\n",
    "    #     elif l_plus <= 20:\n",
    "    #         tags.append(\"üèπ WILD THING\")\n",
    "\n",
    "    #     # 2. COMBINATION SCOUTING (The \"Pitching+\" Profiles)\n",
    "    #     if s_plus >= 85 and l_plus >= 85:\n",
    "    #         tags.append(\"üëë DOMINANT FORCE\")\n",
    "    #         summary += \"A rare combination of elite physical tools and surgical precision. \"\n",
    "        \n",
    "    #     elif s_plus >= 85 and l_plus <= 40:\n",
    "    #         tags.append(\"üíé RAW DIAMOND\")\n",
    "    #         summary += \"Elite stuff that is currently unrefined; a primary candidate for a pitching lab overhaul. \"\n",
    "            \n",
    "    #     elif l_plus >= 85 and s_plus <= 40:\n",
    "    #         tags.append(\"üéì THE PROFESSOR\")\n",
    "    #         summary += \"Succeeds through elite sequencing and location despite below-average raw velocity. \"\n",
    "\n",
    "    #     # 3. KEEPING YOUR FAVORITES\n",
    "    #     if row['tunnel_pct'] >= 90: tags.append(\"üß¨ TUNNELER\")\n",
    "    #     if row['is_unicorn'] == -1: tags.append(\"ü¶Ñ UNICORN\")\n",
    "        \n",
    "    #     return \" | \".join(list(set(tags))), summary.strip()\n",
    "\n",
    "    def generate_scouting_report(row):\n",
    "        tags = []\n",
    "        # Start the summary with the Grade and Handedness\n",
    "        summary_header = f\"[{row['overall_grade']} GRADE] ({row['hand']})\"\n",
    "        \n",
    "        # 1. CORE IDENTITY TAGS (Physicality)\n",
    "        s_plus = row['stuff_plus_pct']\n",
    "        l_plus = row['location_plus_pct']\n",
    "        \n",
    "        if s_plus >= 90: tags.append(\"üí£ PURE FILTH\")\n",
    "        elif s_plus <= 20: tags.append(\"üìâ LACKS BITE\")\n",
    "\n",
    "        if l_plus >= 90: tags.append(\"üéØ SURGEON\")\n",
    "        elif l_plus <= 20: tags.append(\"üèπ WILD THING\")\n",
    "\n",
    "        # 2. MATCHUP TACTICS (New Logic)\n",
    "        # We use the columns we just built in SQL\n",
    "        profile = row['attack_profile']\n",
    "        role = row['matchup_role']\n",
    "        platoon = row['platoon_identity']\n",
    "        \n",
    "        # Build the Narrative Summary\n",
    "        analysis = f\"Identified as a {role}. \"\n",
    "        \n",
    "        if \"NORTH-SOUTH\" in profile:\n",
    "            analysis += \"Wins vertically with high-carry fastballs; elite matchup against low-ball hitters. \"\n",
    "        elif \"EAST-WEST\" in profile:\n",
    "            analysis += \"Heavy horizontal movement profile; ideal for inducing double plays. \"\n",
    "        \n",
    "        if platoon == \"MATCHUP PROOF\":\n",
    "            tags.append(\"üõ°Ô∏è PLATOON NEUTRAL\")\n",
    "            analysis += \"Maintains effectiveness regardless of batter handedness. \"\n",
    "        elif platoon == \"PLATOON SENSITIVE\":\n",
    "            tags.append(\"‚ö†Ô∏è SPLIT RISK\")\n",
    "            analysis += \"Performance drops significantly against opposite-handed hitters. \"\n",
    "\n",
    "        # 3. SPECIAL TRAITS\n",
    "        if row['tunnel_pct'] >= 90: tags.append(\"üß¨ TUNNELER\")\n",
    "        if row['is_unicorn'] == -1: tags.append(\"ü¶Ñ UNICORN\")\n",
    "        if row['breakout_potential'] != 'OPTIMIZED':\n",
    "            tags.append(\"üöÄ BREAKOUT\")\n",
    "            analysis += f\"Tactical Alert: {row['breakout_potential']}. \"\n",
    "\n",
    "        # Create the final string\n",
    "        tag_str = \" | \".join(list(set(tags)))\n",
    "        final_summary = f\"{summary_header} {tag_str} ‚Äî {analysis.strip()}\"\n",
    "        \n",
    "        return tag_str, final_summary\n",
    "\n",
    "    # Apply to your DataFrame\n",
    "    results = df.apply(generate_scouting_report, axis=1)\n",
    "    df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "\n",
    "    # Apply and split into two columns\n",
    "    results = df.apply(generate_scouting_report, axis=1)\n",
    "    df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "    \n",
    "    # # Apply Logic\n",
    "    # results = df.apply(generate_scouting_report, axis=1)\n",
    "    # df['archetype_tags'], df['scouting_summary'] = zip(*results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_dim_pitcher_archetypes(engine):\n",
    "    \"\"\"\n",
    "    SQL to extract the necessary metrics for the Python model.\n",
    "    \"\"\"\n",
    "    query = text(\"\"\"\n",
    "    WITH attack_zone_stats AS (\n",
    "    SELECT \n",
    "        p.*,\n",
    "        -- Define Command/Paint Zones\n",
    "        CASE \n",
    "            WHEN ABS(p.plate_x) <= 0.67 AND p.plate_z BETWEEN (p.sz_bot + 0.33) AND (p.sz_top - 0.33) THEN 'heart'\n",
    "            WHEN ABS(p.plate_x) <= 1.1 AND p.plate_z BETWEEN (p.sz_bot - 0.33) AND (p.sz_top + 0.33) THEN 'shadow'\n",
    "            WHEN ABS(p.plate_x) <= 1.5 AND p.plate_z BETWEEN (p.sz_bot - 0.75) AND (p.sz_top + 0.75) THEN 'chase'\n",
    "            ELSE 'waste'\n",
    "        END as attack_zone,\n",
    "        CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked', 'missed_bunt') THEN 1 ELSE 0 END as is_whiff,\n",
    "        CASE WHEN p.description IN ('swinging_strike', 'swinging_strike_blocked', 'missed_bunt', 'foul', 'foul_tip', 'hit_into_play') THEN 1 ELSE 0 END as is_swing\n",
    "    FROM fact_statcast_pitches p\n",
    "    ),\n",
    "    vaa_base_calc AS (\n",
    "        SELECT \n",
    "            az.*,\n",
    "            CASE WHEN az.pitch_type IN ('FA', 'FF', 'FC') THEN \n",
    "                -ATAN((az.vz0 + (az.az * ((-az.vy0 - SQRT(az.vy0^2 - (2 * az.ay * (50 - (17/12))))) / az.ay))) / \n",
    "                (-SQRT(az.vy0^2 - (2 * az.ay * (50 - (17/12)))))) * (180/3.14159) \n",
    "            END as individual_ff_vaa\n",
    "        FROM attack_zone_stats az\n",
    "    ),\n",
    "    aggregated_stats AS (\n",
    "        SELECT \n",
    "            p.pitcher,\n",
    "            p.p_throws,\n",
    "            COUNT(*) as total_pitches,        \n",
    "            AVG(p.release_extension) as avg_extension,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed + ((p.release_extension - 6.2) * 2) END)::numeric, 1), 0) as perceived_fb_velo,\n",
    "            (AVG(CASE WHEN p.pitch_type IN ('FA', 'FF') THEN p.pfx_z * 12 END) - \n",
    "            AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'SI') THEN p.pfx_z * 12 END)) as v_break_gap_raw,\n",
    "            \n",
    "            ROUND(100.0 * SUM(p.is_whiff) / NULLIF(SUM(p.is_swing), 0), 2) as whiff_rate_raw,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.launch_speed_angle = 6 THEN 1 ELSE 0 END) / \n",
    "                NULLIF(SUM(CASE WHEN p.type = 'X' THEN 1 ELSE 0 END), 0), 2) as barrel_rate_raw,        \n",
    "            \n",
    "            ROUND(100.0 * SUM(CASE WHEN p.attack_zone = 'shadow' THEN 1 ELSE 0 END) / COUNT(*), 1) as paint_raw,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.attack_zone IN ('shadow', 'chase') THEN 1 ELSE 0 END) / COUNT(*), 1) as command_raw,        \n",
    "            \n",
    "            AVG(CASE WHEN p.stand = 'L' THEN p.estimated_woba_using_speedangle END) as xwoba_vs_lhb,\n",
    "            AVG(CASE WHEN p.stand = 'R' THEN p.estimated_woba_using_speedangle END) as xwoba_vs_rhb,           \n",
    "            \n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed END)::numeric, 1), 0) as fb_velo,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN p.release_speed END)::numeric, 1), 0) as offspeed_velo,                                                                                                                                                                    \n",
    "            \n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FC') THEN 1 ELSE 0 END) / COUNT(*), 1) as ffour_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('SI', 'FT') THEN 1 ELSE 0 END) / COUNT(*), 1) as sinker_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('CU', 'SL', 'KC', 'ST', 'SV', 'CS', 'KN') THEN 1 ELSE 0 END) / COUNT(*), 1) as bb_usage,\n",
    "            ROUND(100.0 * SUM(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN 1 ELSE 0 END) / COUNT(*), 1) as offspeed_usage,        \n",
    "            \n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('FA', 'FF', 'FT', 'FC', 'SI') THEN p.release_speed END)::numeric - \n",
    "                        AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'ST', 'SL', 'KC', 'GY', 'SV', 'CS', 'KN', 'EP') THEN p.release_speed END)::numeric, 1), 0) as velo_gap,        \n",
    "            COALESCE(ROUND(AVG(p.individual_ff_vaa)::numeric, 2), 0) as ffour_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('SI', 'FT') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as sinker_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CU', 'SL', 'KC', 'ST', 'SV', 'CS', 'KN') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as bb_vaa,\n",
    "            COALESCE(ROUND(AVG(CASE WHEN p.pitch_type IN ('CH', 'FS', 'FO', 'SC', 'EP') THEN -ATAN((p.vz0 + (p.az * ((-p.vy0 - SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12))))) / p.ay))) / (-SQRT(p.vy0^2 - (2 * p.ay * (50 - (17/12)))))) * (180/3.14159) END)::numeric, 2), 0) as offspeed_vaa,\n",
    "\n",
    "            COUNT(DISTINCT game_pk) as total_appearances,\n",
    "            (COUNT(*) / COUNT(DISTINCT game_pk)) as avg_pitches_per_app,\n",
    "            CASE WHEN (COUNT(*) / COUNT(DISTINCT game_pk)) >= 40 AND COUNT(DISTINCT game_pk) >= 3 THEN 1 ELSE 0 END as is_starter,\n",
    "            \n",
    "            STDDEV(p.release_pos_x) as release_x_std,\n",
    "            STDDEV(p.release_pos_z) as release_z_std,\n",
    "            (STDDEV(p.release_pos_x) + STDDEV(p.release_pos_z)) as tunnel_raw,\n",
    "            AVG(p.individual_ff_vaa - ((-0.68 * p.plate_z) - 3.8)) as vaa_above_expected_raw,       \n",
    "            -- RAW STUFF+ (Process)\n",
    "            ( (AVG(p.release_speed) * 0.4) + (AVG(p.release_extension) * 0.2) + (AVG(ABS(p.pfx_x)) * 12 * 0.2) + (AVG(p.pfx_z) * 12 * 0.2) ) as stuff_raw,\n",
    "            -- RAW LOCATION+ (Process)\n",
    "            ( (SUM(CASE WHEN p.attack_zone = 'shadow' THEN 1 ELSE 0 END)::float / COUNT(*)) * 0.6 + (SUM(CASE WHEN p.attack_zone = 'heart' THEN 0 ELSE 1 END)::float / COUNT(*)) * 0.4 ) as location_raw\n",
    "\n",
    "        FROM vaa_base_calc p\n",
    "        GROUP BY p.pitcher, p.p_throws\n",
    "        HAVING COUNT(*) > 100 AND AVG(p.release_speed) > 84\n",
    "    ),\n",
    "    ranked_stats AS (\n",
    "        SELECT \n",
    "            ast.*,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY fb_velo))::numeric, 2) * 100 as velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY offspeed_velo))::numeric, 2) * 100, 0) as offspeed_velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY velo_gap))::numeric, 2) * 100, 0) as velo_gap_pct,                   \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY whiff_rate_raw))::numeric, 2) * 100 as whiff_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY barrel_rate_raw DESC))::numeric, 2) * 100 as suppression_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY command_raw))::numeric, 2) * 100 as command_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY paint_raw))::numeric, 2) * 100 as paint_pct,            \n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY perceived_fb_velo))::numeric, 2) * 100 as perceived_velo_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0 OR sinker_usage > 0) ORDER BY v_break_gap_raw))::numeric, 2) * 100, 0) as movement_gap_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY avg_extension))::numeric, 2) * 100 as extension_pct,           \n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (ffour_usage > 0) ORDER BY ffour_vaa))::numeric, 2) * 100, 0) as ffour_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (sinker_usage > 0) ORDER BY sinker_vaa DESC))::numeric, 2) * 100, 0) as sinker_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (bb_usage > 0) ORDER BY bb_vaa DESC))::numeric, 2) * 100, 0) as bb_vaa_pct,\n",
    "            COALESCE(ROUND((PERCENT_RANK() OVER (PARTITION BY (offspeed_usage > 0) ORDER BY offspeed_vaa DESC))::numeric, 2) * 100, 0) as offspeed_vaa_pct,\n",
    "            ROUND((100 - (ABS(COALESCE(xwoba_vs_lhb, 0.320) - COALESCE(xwoba_vs_rhb, 0.320)) * 100))::numeric, 2) as neutrality_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY tunnel_raw DESC))::numeric, 2) * 100 as tunnel_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY vaa_above_expected_raw))::numeric, 2) * 100 as vaa_plus_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY stuff_raw))::numeric, 2) * 100 as stuff_plus_pct,\n",
    "            ROUND((PERCENT_RANK() OVER (ORDER BY location_raw))::numeric, 2) * 100 as location_plus_pct\n",
    "        FROM aggregated_stats ast\n",
    "    )\n",
    "    SELECT \n",
    "        CONCAT(pn.first_name_chadwick, ' ', pn.last_name_chadwick) as full_name,\n",
    "        rs.p_throws as hand,\n",
    "        rs.*,\n",
    "        -- MATCHUP COLUMN 1: ATTACK PROFILE (Rise vs Run)\n",
    "        CASE \n",
    "            WHEN vaa_plus_pct > 75 THEN 'NORTH-SOUTH (High Rise)'\n",
    "            WHEN sinker_usage > 25 THEN 'EAST-WEST (Sinker/Run)'\n",
    "            WHEN movement_gap_pct > 75 THEN 'DECEPTIVE (High Break)'\n",
    "            ELSE 'BALANCED'\n",
    "        END as attack_profile,\n",
    "        -- MATCHUP COLUMN 2: ROLE IDENTITY\n",
    "        CASE \n",
    "            WHEN rs.whiff_pct > 75 AND rs.location_plus_pct > 75 THEN 'DOMINANT ACE'\n",
    "            WHEN rs.whiff_pct > 75 AND rs.location_plus_pct < 40 THEN 'POWER ARMS (High Risk)'\n",
    "            WHEN rs.location_plus_pct > 75 AND rs.whiff_pct < 45 THEN 'PITCH TO CONTACT SURGEON'\n",
    "            ELSE 'ROTATION STABILIZER'\n",
    "        END as matchup_role,\n",
    "        -- MATCHUP COLUMN 3: PLATOON RESISTANCE\n",
    "        CASE \n",
    "            WHEN rs.neutrality_pct > 75 THEN 'MATCHUP PROOF'\n",
    "            WHEN rs.neutrality_pct < 35 THEN 'PLATOON SENSITIVE'\n",
    "            ELSE 'STANDARD SPLITS'\n",
    "        END as platoon_identity,\n",
    "        ROUND((perceived_velo_pct * 0.25 + ffour_vaa_pct * 0.25 + whiff_pct * 0.5), 0) as ffour_quality_score,\n",
    "        ROUND((movement_gap_pct * 0.25 + offspeed_vaa_pct * 0.25 + whiff_pct * 0.5), 0) as offspeed_quality_score,   \n",
    "        CASE \n",
    "            WHEN (ffour_vaa_pct > 80 AND ffour_usage < 20) THEN 'UNDERUSED ELITE FASTBALL'\n",
    "            WHEN (bb_vaa_pct > 80 AND bb_usage < 15) THEN 'UNDERUSED ELITE BREAKING'\n",
    "            WHEN (offspeed_vaa_pct > 80 AND offspeed_usage < 15) THEN 'UNDERUSED ELITE OFFSPD'\n",
    "            ELSE 'OPTIMIZED'\n",
    "        END as breakout_potential\n",
    "    FROM ranked_stats rs\n",
    "    JOIN dim_player pn ON rs.pitcher = pn.key_mlbam\n",
    "    ORDER BY stuff_plus_pct DESC;\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    return run_scouting_model(df)\n",
    "\n",
    "# Execute\n",
    "pitcher_archetypes = update_dim_pitcher_archetypes(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
