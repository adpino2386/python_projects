{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from rapidfuzz import process\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rotowire.com: Get matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing datatable1 from rotowire.com ...\n",
      "datatable1 table loaded successfully.\n",
      "Processing datatable2 from rotowire.com ...\n",
      "datatable2 table loaded successfully.\n",
      "Processing datatable3 from rotowire.com ...\n",
      "datatable3 table loaded successfully.\n",
      "Processing datatable4 from rotowire.com ...\n",
      "datatable4 table loaded successfully.\n",
      "Skipping datatable4: Table is empty.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_matchups(datatable_id):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)\n",
    "    driver.get(\"https://www.rotowire.com/baseball/stats-bvp.php\")\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@view_id='${datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "    \n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    if len(rows) <= 10:\n",
    "        print(f\"Skipping {datatable_id}: Table is empty.\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame()  # Return an empty DataFrame when skipping\n",
    "    \n",
    "    if len(rows) > 10:\n",
    "        # Convert rows into columns (headers for hot/cold batters)\n",
    "        n = 16 if datatable_id in [\"datatable3\", \"datatable4\"] else 18\n",
    "        reshaped_data = [df[0][i:i+n].values for i in range(0, len(df), n)]\n",
    "        reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "        # Use the first row as headers\n",
    "        reshaped_df.columns = reshaped_df.iloc[0]\n",
    "        reshaped_df = reshaped_df[1:].reset_index(drop=True)\n",
    "\n",
    "        # Remove rows based on datatable_id\n",
    "        rows_to_remove = 16 if datatable_id in [\"datatable3\", \"datatable4\"] else 18\n",
    "        df = df.iloc[rows_to_remove:].reset_index(drop=True)\n",
    "\n",
    "        # Check the character count\n",
    "        df['char_count'] = df[0].apply(len)\n",
    "        cutoff_value = 3 if datatable_id in [\"datatable3\", \"datatable4\"] else 2\n",
    "        cutoff_index = df[df[\"char_count\"] <= cutoff_value].index.min()\n",
    "\n",
    "        # Keep only rows above the cutoff index\n",
    "        df_filtered = df.loc[:cutoff_index - 1]\n",
    "        num_rows = len(df_filtered)\n",
    "\n",
    "        # Group and pivot\n",
    "        df[\"group\"] = (df.index // num_rows)\n",
    "        df = df.rename(columns={0: \"data_column\"})\n",
    "        df[\"mod_index\"] = df.index % num_rows\n",
    "        df_new = df.pivot(index=\"mod_index\", columns=\"group\", values=\"data_column\")\n",
    "        df_new.columns = reshaped_df.columns\n",
    "        \n",
    "        # Add suffix for hot and cold\n",
    "        suffix = '_hot' if datatable_id in [\"datatable1\", \"datatable3\"] else '_cold'\n",
    "        \n",
    "        # Add the suffix to the table\n",
    "        df_new =  df_new.rename(columns= lambda col: col + suffix)\n",
    "        \n",
    "        # Close the driver\n",
    "        driver.quit()\n",
    "    else:\n",
    "        df_new = pd.DataFrame()\n",
    "        driver.quit()\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Create a dictionary to store the dataframes\n",
    "dataframes_matchups = {}\n",
    "\n",
    "# ?Apply the function\n",
    "# Create a list with the 4 datatables\n",
    "datatable_ids = ['datatable1', 'datatable2', 'datatable3', 'datatable4']\n",
    "\n",
    "for datatable_id in datatable_ids:\n",
    "    print(f\"Processing {datatable_id} from rotowire.com ...\")\n",
    "    result_df = get_matchups(datatable_id)\n",
    "    if result_df is not None:\n",
    "        # Store the dataframe in dictionary\n",
    "        dataframes_matchups[datatable_id] = result_df\n",
    "\n",
    "# Extract each one into their dataframes\n",
    "hot_hitter_matchups   = dataframes_matchups['datatable1']\n",
    "cold_hitter_matchups  = dataframes_matchups['datatable2']\n",
    "hot_pitcher_matchups  = dataframes_matchups['datatable3']\n",
    "cold_pitcher_matchups = dataframes_matchups['datatable4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fantasypros.com: Get hitters' stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data table loaded successfully.\n",
      "Hitter's last 1 games loaded successfully.\n",
      "data table loaded successfully.\n",
      "Hitter's last 7 games loaded successfully.\n",
      "data table loaded successfully.\n",
      "Hitter's last 15 games loaded successfully.\n",
      "data table loaded successfully.\n",
      "Hitter's last 30 games loaded successfully.\n",
      "data table loaded successfully.\n",
      "Hitter's full season loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def hitting_stats(nb_last_days):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)\n",
    "    if nb_last_days == 1:\n",
    "        driver.get(\"https://www.fantasypros.com/mlb/stats/hitters.php?range=1&page=ALL\")\n",
    "    elif nb_last_days == 7:\n",
    "        driver.get(\"https://www.fantasypros.com/mlb/stats/hitters.php?range=7&page=ALL\")\n",
    "    elif nb_last_days == 15:\n",
    "        driver.get(\"https://www.fantasypros.com/mlb/stats/hitters.php?range=15&page=ALL\")\n",
    "    elif nb_last_days == 30:\n",
    "        driver.get(\"https://www.fantasypros.com/mlb/stats/hitters.php?range=30&page=ALL\")\n",
    "    else:\n",
    "        # Get the full season\n",
    "        driver.get(\"https://www.fantasypros.com/mlb/stats/hitters.php?range=2025&page=ALL\")\n",
    "\n",
    "    datatable_id = 'data'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Remove the first row\n",
    "    df = df.iloc[1:].reset_index(drop= True)\n",
    "\n",
    "    # Convert rows into columns\n",
    "    reshaped_data = [df[0][i:i+17].values for i in range(0, len(df), 17)]\n",
    "\n",
    "    # Create the reshaped dataframe\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Use the first row as headers\n",
    "    reshaped_df.columns = reshaped_df.iloc[0]\n",
    "    reshaped_df = reshaped_df[1:].reset_index(drop= True)\n",
    "\n",
    "    # Remove all data from the table\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.index)\n",
    "\n",
    "    # Drop the first column\n",
    "    reshaped_df.drop(reshaped_df.columns[0], axis= 1, inplace= True)\n",
    "\n",
    "    # Remove the unnecessary rows \n",
    "    df = df.iloc[17:].reset_index(drop= True)\n",
    "    \n",
    "    # Keep rows where the text starts with a number\n",
    "    df = df[df[0].str.match(r'^\\d')]\n",
    "\n",
    "    # Splitting the column from right to left 15 times\n",
    "    df_split = df[0].str.rsplit(' ', n= 15, expand= True)\n",
    "\n",
    "    # # Remove strings starting from the parenthesis\n",
    "    # df_split[0] = df_split[0].str.replace(r\"\\s*\\(.*\\)\", \"\", regex= True)\n",
    "\n",
    "    # # Remove numbers at the beginning of the string\n",
    "    # df_split[0] = df_split[0].str.replace(r\"^\\d+\\s*\", \"\", regex= True)\n",
    "    \n",
    "    # Update the headers\n",
    "    df_split.columns = reshaped_df.columns\n",
    "\n",
    "    if nb_last_days == 1:\n",
    "        suffix = '_yesterday'\n",
    "    elif nb_last_days == 7:\n",
    "        suffix = '_7'\n",
    "    elif nb_last_days == 15:\n",
    "        suffix = '_15'\n",
    "    elif nb_last_days == 30:\n",
    "        suffix = '_30'\n",
    "    else:\n",
    "        suffix = '_full'\n",
    "\n",
    "    # Add the suffix to the table\n",
    "    df_split =  df_split.rename(columns= lambda col: col + suffix)\n",
    "    \n",
    "    # Show a message\n",
    "    if nb_last_days != 99:\n",
    "        print(f\"Hitter's last {nb_last_days} games loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Hitter's full season loaded successfully.\")\n",
    "        \n",
    "    return df_split\n",
    "\n",
    "\n",
    "# Call the function\n",
    "yesterday_games      = hitting_stats(1)\n",
    "last_seven_games     = hitting_stats(7)\n",
    "last_fifteen_games   = hitting_stats(15)\n",
    "last_thirty_games    = hitting_stats(30)\n",
    "season_to_date_games = hitting_stats(99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine rotowire and fantasypros data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def match_names(name, choices):\n",
    "    \"\"\"Find a match between the players\n",
    "\n",
    "    Args:\n",
    "        name (_type_): Player name to match\n",
    "        choices (_type_): List of possible matches\n",
    "\n",
    "    Returns:\n",
    "        _type_: Best match or None if no match found\n",
    "    \"\"\"\n",
    "    match = process.extractOne(name, choices, score_cutoff=80)\n",
    "    if match:  # Check if a match is found\n",
    "        best_match, score, _ = match\n",
    "        return best_match\n",
    "    # Return None if no match was found. \n",
    "    # It means the player didn't play yesterday.\n",
    "    return None  \n",
    "\n",
    "\n",
    "def create_matched_tables(hot_df, cold_df, hot_field_name, cold_field_name ):\n",
    "    # Add the matched names in the table.\n",
    "    # Join the fields (hot hitters)\n",
    "    hot_df['matched_player_yesterday']      = hot_df[hot_field_name].apply(lambda x: match_names(x, yesterday_games['PLAYER_yesterday']))\n",
    "    hot_df['matched_player_last_seven']     = hot_df[hot_field_name].apply(lambda x: match_names(x, last_seven_games['PLAYER_7']))\n",
    "    hot_df['matched_player_last_fifteen']   = hot_df[hot_field_name].apply(lambda x: match_names(x, last_fifteen_games['PLAYER_15']))\n",
    "    hot_df['matched_player_last_thirty']    = hot_df[hot_field_name].apply(lambda x: match_names(x, last_thirty_games['PLAYER_30']))\n",
    "    hot_df['matched_player_season_to_date'] = hot_df[hot_field_name].apply(lambda x: match_names(x, season_to_date_games['PLAYER_full']))\n",
    "\n",
    "    # Join the fields (cold hitters)\n",
    "    cold_df['matched_player_yesterday']      = cold_df[cold_field_name].apply(lambda x: match_names(x, yesterday_games['PLAYER_yesterday']))\n",
    "    cold_df['matched_player_last_seven']     = cold_df[cold_field_name].apply(lambda x: match_names(x, last_seven_games['PLAYER_7']))\n",
    "    cold_df['matched_player_last_fifteen']   = cold_df[cold_field_name].apply(lambda x: match_names(x, last_fifteen_games['PLAYER_15']))\n",
    "    cold_df['matched_player_last_thirty']    = cold_df[cold_field_name].apply(lambda x: match_names(x, last_thirty_games['PLAYER_30']))\n",
    "    cold_df['matched_player_season_to_date'] = cold_df[cold_field_name].apply(lambda x: match_names(x, season_to_date_games['PLAYER_full']))\n",
    "    \n",
    "    # Select the last 4 columns and replace NaN values with \"Did not play\"\n",
    "    hot_df.iloc[:, -4:]  = hot_df.iloc[:, -4:].fillna(\"Did not play\")\n",
    "    cold_df.iloc[:, -4:] = cold_df.iloc[:, -4:].fillna(\"Did not play\")\n",
    "    \n",
    "    # Join the rest of the data (hot/cold)\n",
    "    new_hot_df = pd.merge(hot_df, yesterday_games,  left_on= 'matched_player_yesterday', right_on= 'PLAYER_yesterday', how= 'left')\n",
    "    new_hot_df = new_hot_df.merge(last_seven_games, left_on= 'matched_player_last_seven', right_on= 'PLAYER_7', how= 'left')\n",
    "    new_hot_df = new_hot_df.merge(last_fifteen_games, left_on= 'matched_player_last_fifteen', right_on= 'PLAYER_15', how= 'left')\n",
    "    new_hot_df = new_hot_df.merge(last_thirty_games, left_on= 'matched_player_last_thirty', right_on= 'PLAYER_30', how= 'left')\n",
    "    new_hot_df = new_hot_df.merge(season_to_date_games, left_on= 'matched_player_season_to_date', right_on= 'PLAYER_full', how= 'left')\n",
    "    \n",
    "    new_cold_df = pd.merge(cold_df, yesterday_games,  left_on= 'matched_player_yesterday', right_on= 'PLAYER_yesterday', how= 'left')\n",
    "    new_cold_df = new_cold_df.merge(last_seven_games, left_on= 'matched_player_last_seven', right_on= 'PLAYER_7', how= 'left')\n",
    "    new_cold_df = new_cold_df.merge(last_fifteen_games, left_on= 'matched_player_last_fifteen', right_on= 'PLAYER_15', how= 'left')\n",
    "    new_cold_df = new_cold_df.merge(last_thirty_games, left_on= 'matched_player_last_thirty', right_on= 'PLAYER_30', how= 'left')\n",
    "    new_cold_df = new_cold_df.merge(season_to_date_games, left_on= 'matched_player_season_to_date', right_on= 'PLAYER_full', how= 'left')\n",
    "    \n",
    "    # Remove columns that start with \"matched\"\n",
    "    new_hot_df  = new_hot_df.drop(columns=[col for col in new_hot_df.columns if col.startswith(\"matched\")])\n",
    "    new_cold_df = new_cold_df.drop(columns=[col for col in new_cold_df.columns if col.startswith(\"matched\")])\n",
    "    \n",
    "    # Fill empty with '-'\n",
    "    new_hot_df  = new_hot_df.fillna('-')\n",
    "    new_cold_df = new_cold_df.fillna('-')\n",
    "    \n",
    "    return new_hot_df, new_cold_df\n",
    "\n",
    "\n",
    "# Call the function\n",
    "hot_hitter_matchups, cold_hitter_matchups = create_matched_tables(hot_hitter_matchups, cold_hitter_matchups, 'Name_hot', 'Name_cold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean tables\n",
    "#### Combine yesterday results, last 7 games, last 30 games and season-to-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframes. Replace the suffixes for all the columns.\n",
    "# and add the PA column to the dataframes.\n",
    "def clean_suffix(df, suffix):\n",
    "    # Remove the suffix from the column names\n",
    "    df.columns = df.columns.str.replace(suffix, '', regex=False)\n",
    "    \n",
    "    # Create the stats_from column based on the suffix\n",
    "    if suffix == '_7':\n",
    "        df['stats_from']= \"Last 7 games\"\n",
    "    elif suffix == '_15':\n",
    "        df['stats_from']= \"Last 15 games\"\n",
    "    elif suffix == '_30':\n",
    "        df['stats_from']= \"Last 30 games\"\n",
    "    elif suffix == '_full':\n",
    "        df['stats_from']= \"Full season\"\n",
    "    elif suffix == '_yesterday':\n",
    "        df['stats_from']= \"Yesterday's game(s)\"\n",
    "    \n",
    "    # First convert the columns to numeric\n",
    "    df['AB'] = pd.to_numeric(df['AB'], errors='coerce')\n",
    "    df['BB'] = pd.to_numeric(df['BB'], errors='coerce')\n",
    "    \n",
    "    # Create the PA column based on AB and BB\n",
    "    df['PA'] = df['AB'] + df['BB']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Call the functions to clean the dataframes\n",
    "yesterday_games      = clean_suffix(yesterday_games,      '_yesterday')\n",
    "last_seven_games     = clean_suffix(last_seven_games,     '_7')\n",
    "last_fifteen_games   = clean_suffix(last_fifteen_games,   '_15')\n",
    "last_thirty_games    = clean_suffix(last_thirty_games,    '_30')\n",
    "season_to_date_games = clean_suffix(season_to_date_games, '_full')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep the columns in the dataframes for yesterday_hitters\n",
    "columns_to_keep = ['PLAYER' # It is Name in the other dataframes \n",
    "                , 'PA'\n",
    "                , 'AB'\n",
    "                , 'H'\n",
    "                , 'HR'\n",
    "                , 'RBI'\n",
    "                , 'SB'\n",
    "                , 'BB'\n",
    "                , 'K' # It is SO in the other dataframes\n",
    "                , 'AVG'\n",
    "                , 'OBP'\n",
    "                , 'SLG'\n",
    "                , 'OPS'\n",
    "                , 'stats_from']\n",
    "\n",
    "yesterday_games      = yesterday_games.loc[:,      columns_to_keep]\n",
    "last_seven_games     = last_seven_games.loc[:,     columns_to_keep]\n",
    "last_fifteen_games   = last_fifteen_games.loc[:,   columns_to_keep]\n",
    "last_thirty_games    = last_thirty_games.loc[:,    columns_to_keep]\n",
    "season_to_date_games = season_to_date_games.loc[:, columns_to_keep]\n",
    "\n",
    "# Rename PLAYER for Name and K for SO\n",
    "yesterday_games      = yesterday_games.rename(columns={'PLAYER': 'Name', 'K': 'SO'})\n",
    "last_seven_games     = last_seven_games.rename(columns={'PLAYER': 'Name', 'K': 'SO'})\n",
    "last_fifteen_games   = last_fifteen_games.rename(columns={'PLAYER': 'Name', 'K': 'SO'})\n",
    "last_thirty_games    = last_thirty_games.rename(columns={'PLAYER': 'Name', 'K': 'SO'})\n",
    "season_to_date_games = season_to_date_games.rename(columns={'PLAYER': 'Name', 'K': 'SO'})\n",
    "\n",
    "# Concatenate the dataframes\n",
    "main_df = pd.concat([yesterday_games\n",
    "                    ,last_seven_games\n",
    "                    ,last_fifteen_games\n",
    "                    ,last_thirty_games\n",
    "                    ,season_to_date_games\n",
    "                    ], ignore_index= True)\n",
    "\n",
    "\n",
    "def convert_columns_to_integer(df, col):\n",
    "    \"\"\"Convert the columns to integer\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        col (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Convert the 'AB' column to numeric, coercing errors to NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors= 'coerce')\n",
    "\n",
    "    # Fill NaN values with 0 (or handle them as needed)\n",
    "    df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_columns_to_float(df, col):\n",
    "    \"\"\"Convert the columns to float\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        col (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Convert the 'AB' column to numeric, coercing errors to NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors= 'coerce')\n",
    "\n",
    "    # Fill NaN values with 0 (or handle them as needed)\n",
    "    df[col] = df[col].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Call the function\n",
    "main_df = convert_columns_to_integer(main_df, 'H')\n",
    "main_df = convert_columns_to_integer(main_df, 'HR')\n",
    "main_df = convert_columns_to_integer(main_df, 'RBI')\n",
    "main_df = convert_columns_to_integer(main_df, 'SB')\n",
    "main_df = convert_columns_to_integer(main_df, 'SO')\n",
    "\n",
    "main_df = convert_columns_to_float(main_df, 'AVG')\n",
    "main_df = convert_columns_to_float(main_df, 'OBP')\n",
    "main_df = convert_columns_to_float(main_df, 'SLG')\n",
    "main_df = convert_columns_to_float(main_df, 'OPS')\n",
    "\n",
    "# Remove ( and - from the Name column\n",
    "main_df['Name'] = main_df['Name'].str.replace('-', '', regex= True).str.strip()\n",
    "main_df['Name'] = main_df['Name'].str.replace(r\"\\(\", '', regex=True).str.strip()\n",
    "\n",
    "# Continue the cleaning process\n",
    "# First create a table that contains the abbreviations of the teams\n",
    "teams_abbr = {\n",
    "    'ATL': 'Atlanta Braves',\n",
    "    'NYM': 'New York Mets', \n",
    "    'PHI': 'Philadelphia Phillies',\n",
    "    'WSH': 'Washington Nationals',\n",
    "    'MIA': 'Miami Marlins',\n",
    "    \n",
    "    'CHC': 'Chicago Cubs',\n",
    "    'CIN': 'Cincinnati Reds',\n",
    "    'MIL': 'Milwaukee Brewers',\n",
    "    'PIT': 'Pittsburgh Pirates',\n",
    "    'STL': 'St. Louis Cardinals',\n",
    "    \n",
    "    'SD': 'San Diego Padres',\n",
    "    'SF': 'San Francisco Giants',\n",
    "    'LAD': 'Los Angeles Dodgers',\n",
    "    'ARI': 'Arizona Diamondbacks',\n",
    "    'COL': 'Colorado Rockies',\n",
    "    \n",
    "    'BOS': 'Boston Red Sox',\n",
    "    'NYY': 'New York Yankees',\n",
    "    'TOR': 'Toronto Blue Jays',\n",
    "    'BAL': 'Baltimore Orioles',\n",
    "    'TB': 'Tampa Bay Rays',\n",
    "    \n",
    "    'CWS': 'Chicago White Sox',\n",
    "    'CLE': 'Cleveland Guardians',\n",
    "    'DET': 'Detroit Tigers',\n",
    "    'KC': 'Kansas City Royals',\n",
    "    'MIN': 'Minnesota Twins',\n",
    "    \n",
    "    'HOU': 'Houston Astros',\n",
    "    'TEX': 'Texas Rangers',\n",
    "    'LAA': 'Los Angeles Angels',\n",
    "    'SEA': 'Seattle Mariners',\n",
    "    'ATH': 'Athletics'\n",
    "    \n",
    "}\n",
    "\n",
    "# Build a regex pattern that matches any of the abbreviations as whole words\n",
    "# Using \\b (word boundary) ensures only whole word matches\n",
    "pattern = r'\\b(?:' + '|'.join(teams_abbr.keys()) + r')\\b'\n",
    "\n",
    "# Extract the abbreviation from column 'Name' into a new column 'new_team'\n",
    "main_df['Team'] = main_df['Name'].str.extract('(' + pattern + ')', expand=False)\n",
    "\n",
    "# Remove the abbreviation from column 'A'\n",
    "# The regex flag 'regex=True' allows pattern matching to remove the abbreviation wherever found\n",
    "#main_df['Name'] = main_df['Name'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Optionally, clean up extra whitespace that may have been left behind\n",
    "# and remove brackets on the right side of the string.\n",
    "main_df['Name'] = main_df['Name'].str.strip()\n",
    "main_df['Name'] = main_df['Name'].str.rstrip(')')\n",
    "\n",
    "# Remove the leading numbers from the Name column\n",
    "main_df['Name'] = main_df['Name'].str.replace(r'^\\d+\\s*', '', regex= True)\n",
    "\n",
    "# Remove just the brackets\n",
    "main_df['Name'] = main_df['Name'].str.replace(r'[\\(\\)]', '', regex=True)\n",
    "\n",
    "# Map the abbreviation to its full team name\n",
    "main_df['Team Full Name'] = main_df['Team'].map(teams_abbr)\n",
    "\n",
    "\n",
    "def split_name_column(row):\n",
    "    \"\"\"Define a function to split the 'Name' column based on the team abbreviation\n",
    "\n",
    "    Args:\n",
    "        row (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for abbr in teams_abbr.keys():\n",
    "        if abbr in row['Name']:\n",
    "            # Split the name into two parts: before and after the team abbreviation\n",
    "            parts = row['Name'].split(abbr, 1)\n",
    "            return parts[0].strip(), abbr, parts[1].strip() if len(parts) > 1 else ''\n",
    "    return row['Name'], None, None\n",
    "\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "main_df[['Player Name', 'Team Abbreviation', 'Position']] = main_df.apply(\n",
    "    lambda row: pd.Series(split_name_column(row)), axis=1\n",
    ")\n",
    "\n",
    "# Drop the original 'Name' column if no longer needed\n",
    "main_df.drop(columns=['Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create unique combination of Name and Team\n",
    "# unique_combination = main_df.drop_duplicates(subset=['Name', 'Team'])\n",
    "\n",
    "# # Keep only the columns Name and Team\n",
    "# unique_combination = unique_combination[['Name', 'Team']]\n",
    "\n",
    "# # Drop the rows with NaN values\n",
    "# unique_combination = unique_combination.dropna()\n",
    "\n",
    "# # Reset the index\n",
    "# unique_combination = unique_combination.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find names with multiple occurrences\n",
    "# name_counts = unique_combination['Name'].value_counts()\n",
    "\n",
    "# # Filter names that appear more than once\n",
    "# repeated_names = name_counts[name_counts > 1].index.tolist()\n",
    "\n",
    "# print(\"Names appearing multiple times:\", repeated_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get hot and cold hitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_hitters(df, period):\n",
    "    if period == 'yesterday':\n",
    "        # Filter the dataframe for yesterday's game(s)\n",
    "        df = df.loc[df['stats_from'] == \"Yesterday's game(s)\"]\n",
    "        \n",
    "        # Keep players with at least an AVG of .250 or higher,\n",
    "        # an OPS of .750, an OBP of .300 or higher and 2 or more hits.\n",
    "        df = df.loc[df['AVG'] >= .27]\n",
    "        df = df.loc[df['OBP'] >= .3]\n",
    "        df = df.loc[df['OPS'] >= .75] \n",
    "        df = df.loc[df['H']   >= 2] \n",
    "        \n",
    "    elif period == 'last_seven':\n",
    "        # Filter the dataframe for last seven games\n",
    "        df = df.loc[df['stats_from'] == \"Last 7 games\"]\n",
    "        \n",
    "        # Keep players with at least 4 games played, an AVG of .250 or higher and\n",
    "        # an OPS of .900 or higher in the last 7 games.\n",
    "        #df = df.loc[df['G'] >= 4]\n",
    "        df = df.loc[df['AVG'] >= .27]\n",
    "        df = df.loc[df['OPS'] >= .85]\n",
    "    \n",
    "    elif period == 'last_fifteen':\n",
    "        # Filter the dataframe for last fifteen games\n",
    "        df = df.loc[df['stats_from'] == \"Last 15 games\"]\n",
    "        \n",
    "        # Keep players with at least 10 games played, an AVG of .250 or higher and\n",
    "        # an OPS of .900 or higher in the last 7 games.\n",
    "        #df = df.loc[df['G'] >= 10]\n",
    "        df = df.loc[df['AVG'] < .27]\n",
    "        df = df.loc[df['OPS'] < .85]    \n",
    "        \n",
    "    elif period == 'last_thirty':\n",
    "        # Filter the dataframe for last thirty games\n",
    "        df = df.loc[df['stats_from'] == \"Last 30 games\"]\n",
    "        \n",
    "        # Keep players with at least 4 games played, an AVG of .250 or higher and\n",
    "        # an OPS of .900 or higher in the last 18 games.\n",
    "        #df = df.loc[df['G'] >= 20]\n",
    "        df = df.loc[df['AVG'] >= .27]\n",
    "        df = df.loc[df['OPS'] >= .85]\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "# Call the function\n",
    "hot_hitters_yesterday    = hot_hitters(main_df, 'yesterday')\n",
    "hot_hitters_last_seven   = hot_hitters(main_df, 'last_seven')\n",
    "hot_hitters_last_fifteen = hot_hitters(main_df, 'last_fifteen')\n",
    "hot_hitters_last_thirty  = hot_hitters(main_df, 'last_thirty')\n",
    "\n",
    "\n",
    "def cold_hitters(df, period):\n",
    "    if period == 'yesterday':\n",
    "        # Filter the dataframe for yesterday's game(s)\n",
    "        df = df.loc[df['stats_from'] == \"Yesterday's game(s)\"]\n",
    "        \n",
    "        # Keep players with at least an AVG of .250 or higher,\n",
    "        # an OPS of .750, an OBP of .300 or higher and 2 or more hits.\n",
    "        df = df.loc[df['AVG'] < .27]\n",
    "        df = df.loc[df['OBP'] < .3]\n",
    "        df = df.loc[df['OPS'] < .75] \n",
    "        df = df.loc[df['H']   < 2] \n",
    "        \n",
    "    elif period == 'last_seven':\n",
    "        # Filter the dataframe for last seven games\n",
    "        df = df.loc[df['stats_from'] == \"Last 7 games\"]\n",
    "        \n",
    "        # Keep players with at least 4 games played, an AVG of .250 or higher and\n",
    "        # an OPS of .900 or higher in the last 7 games.\n",
    "        #df = df.loc[df['G'] >= 4]\n",
    "        df = df.loc[df['AVG'] < .27]\n",
    "        df = df.loc[df['OPS'] < .85]\n",
    "        \n",
    "    elif period == 'last_fifteen':\n",
    "        # Filter the dataframe for last fifteen games\n",
    "        df = df.loc[df['stats_from'] == \"Last 15 games\"]\n",
    "        \n",
    "        # Keep players with at least 10 games played, an AVG of .250 or higher and\n",
    "        # an OPS of .900 or higher in the last 7 games.\n",
    "        #df = df.loc[df['G'] >= 10]\n",
    "        df = df.loc[df['AVG'] < .27]\n",
    "        df = df.loc[df['OPS'] < .85]\n",
    "        \n",
    "    elif period == 'last_thirty':\n",
    "        # Filter the dataframe for last thirty games\n",
    "        df = df.loc[df['stats_from'] == \"Last 30 games\"]\n",
    "        \n",
    "        # Keep players with at least 10 games played, an AVG of .250 or higher and\n",
    "        # an OPS of .900 or higher in the last 18 games.\n",
    "        #df = df.loc[df['G'] >= 20]\n",
    "        df = df.loc[df['AVG'] < .27]\n",
    "        df = df.loc[df['OPS'] < .85]\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# Call the function\n",
    "cold_hitters_yesterday    = cold_hitters(main_df, 'yesterday')\n",
    "cold_hitters_last_seven   = cold_hitters(main_df, 'last_seven')\n",
    "cold_hitters_last_fifteen = cold_hitters(main_df, 'last_fifteen')\n",
    "cold_hitters_last_thirty  = cold_hitters(main_df, 'last_thirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of DataFrames\n",
    "dfs = {\n",
    "    \n",
    "    'main_df': main_df\n",
    "    \n",
    "    ,'hot_hitter_matchup':     hot_hitter_matchups\n",
    "    , 'cold_hitter_matchup':   cold_hitter_matchups\n",
    "    , 'hot_pitcher_matchups':  hot_pitcher_matchups\n",
    "    , 'cold_pitcher_matchups': cold_pitcher_matchups\n",
    "\n",
    "    \n",
    "    , 'hot_hitters_yesterday':    hot_hitters_yesterday\n",
    "    , 'hot_hitters_last_seven':   hot_hitters_last_seven\n",
    "    , 'hot_hitters_last_fifteen': hot_hitters_last_fifteen\n",
    "    , 'hot_hitters_last_seven':   hot_hitters_last_thirty\n",
    "    \n",
    "    , 'cold_hitters_yesterday':    cold_hitters_yesterday\n",
    "    , 'cold_hitters_last_seven':   cold_hitters_last_seven\n",
    "    , 'cold_hitters_last_fifteen': cold_hitters_last_fifteen\n",
    "    , 'cold_hitters_last_seven':   cold_hitters_last_thirty\n",
    "    \n",
    "    }\n",
    "\n",
    "#! This works only for .py files.\n",
    "# # Get the current working directory and create the path for the 'output' folder\n",
    "# output_folder = os.path.join(os.path.dirname(__file__), 'output')\n",
    "\n",
    "# Get the current working directory and create the path for the 'output' folder\n",
    "# output_folder = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "output_folder = ('D:\\\\MLB Analyzer\\\\output\\\\')\n",
    "\n",
    "# Save each DataFrame in the 'output' folder\n",
    "for name, dataframe in dfs.items():\n",
    "    dataframe.to_csv(os.path.join(output_folder, f'{name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to here, all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to know the pitcher against the hitter for today\n",
    "# Extract the W, L and ERA and add it to hot_hitters, cold_hitters\n",
    "# Need to add in full season numbers to see how it goes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
