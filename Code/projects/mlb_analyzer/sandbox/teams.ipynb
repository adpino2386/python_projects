{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e133bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from rapidfuzz import process\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36493d1f",
   "metadata": {},
   "source": [
    "### Rotowire.com scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2251f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for date: 2025-04-30\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-04-29\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-04-28\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-04-27\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-04-26\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-04-25\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-04-24\n",
      "grid-noGutter mb-15 table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def teams_matchups(game_date):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.rotowire.com/baseball/scoreboard.php?date={game_date}\")\n",
    "\n",
    "    datatable_id = 'grid-noGutter mb-15'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Find indices for where \"Final\" and \"View Box Score\" appear\n",
    "    start_indices = df[df[0].str.contains(\"Final\", case=False)].index\n",
    "    end_indices = df[df[0].str.contains(\"View Box Score\", case=False)].index\n",
    "\n",
    "    # Extract and split data\n",
    "    game_dataframes = []\n",
    "    for start in start_indices:\n",
    "        # Find the corresponding end index that's greater than the start index\n",
    "        end = end_indices[end_indices > start].min()\n",
    "        if pd.notna(end):  # Ensure there's a valid end index\n",
    "            game_data = df.iloc[start:end+1]  # Capture all rows in between\n",
    "            # Split every 13 rows and create DataFrame\n",
    "            reshaped_data = [game_data.iloc[i:i+13] for i in range(0, len(game_data), 13)]\n",
    "            game_dataframes.extend(reshaped_data)\n",
    "\n",
    "    # Final dataframe with all games\n",
    "    final_df = pd.concat(game_dataframes, ignore_index= True)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array for reshaping\n",
    "    data = final_df.values  \n",
    "\n",
    "    # Reshape: each group of 13 rows becomes one row with 13 columns\n",
    "    reshaped_data = [data[i:i+13].flatten() for i in range(0, len(data), 13)]\n",
    "\n",
    "    # Convert reshaped data back to a DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Drop the first 4 columns and last column\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.columns[[0, 1, 2, 3, -1]], axis= 1)\n",
    "\n",
    "    # Add the headers\n",
    "    reshaped_df.columns = ['Away', 'Home', 'R_Away', 'H_Away', 'E_Away', 'R_Home', 'H_Home', 'E_Home']\n",
    "\n",
    "    # Add the date\n",
    "    reshaped_df['date'] = game_date\n",
    "\n",
    "    # Calculate the winner, loser and the difference in runs, hits and errors\n",
    "    reshaped_df['winner']    = reshaped_df.apply(lambda x: x['Away'] if int(x['R_Away']) > int(x['R_Home']) else x['Home'], axis= 1)\n",
    "    reshaped_df['loser']     = reshaped_df.apply(lambda x: x['Away'] if int(x['R_Away']) < int(x['R_Home']) else x['Home'], axis= 1)\n",
    "    reshaped_df['diff_runs_away_vs_home_team']   = reshaped_df.apply(lambda x: int(x['R_Away']) - int(x['R_Home']), axis= 1)\n",
    "    reshaped_df['diff_hits_away_vs_home_team']   = reshaped_df.apply(lambda x: int(x['H_Away']) - int(x['H_Home']), axis= 1)\n",
    "    reshaped_df['diff_errors_away_vs_home_team'] = reshaped_df.apply(lambda x: int(x['E_Away']) - int(x['E_Home']), axis= 1)\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Define dates\n",
    "dates = [\n",
    "    (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=4)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    ]\n",
    "\n",
    "# Initialize a dictionary to store dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each date and store results in a unique dataframe\n",
    "for game_date in dates:\n",
    "    print(f\"Processing data for date: {game_date}\")\n",
    "    \n",
    "    # Run the teams_matchups function and save the resulting dataframe\n",
    "    result_df = teams_matchups(game_date)\n",
    "    \n",
    "    # Store the dataframe in the dictionary with the date as the key\n",
    "    dataframes[game_date] = result_df\n",
    "\n",
    "# Create the final dataframe\n",
    "games = pd.concat(dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Convert to datetime format\n",
    "games[\"date\"] = pd.to_datetime(games[\"date\"])\n",
    "\n",
    "# Create a game_id column using the index\n",
    "games[\"game_id\"] = games.index + 1\n",
    "games[\"game_id\"] = games[\"game_id\"].astype(str)\n",
    "\n",
    "# Create a key column for the game\n",
    "games[\"key\"] = games[\"date\"].dt.strftime(\"%Y%m%d\") + \"_\" + games[\"game_id\"]\n",
    "\n",
    "# Know if the winner was the away or home team\n",
    "games[\"visitor_won\"]           = games.apply(lambda x: 1 if int(x[\"R_Away\"]) > int(x[\"R_Home\"]) else 0, axis=1)\n",
    "games[\"home_won\"]              = games.apply(lambda x: 1 if int(x[\"R_Away\"]) < int(x[\"R_Home\"]) else 0, axis=1) \n",
    "games[\"visit_or_home_victory\"] = games.apply(lambda x: 'H' if int(x[\"R_Away\"]) < int(x[\"R_Home\"]) else 'V', axis=1) \n",
    "\n",
    "# Create a column that indicates if the game was a shutout\n",
    "games[\"shutout\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) == 0 or int(x[\"R_Home\"]) == 0 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a one-run game\n",
    "games[\"one_run_game\"] = games.apply(lambda x: 1 if abs(int(x[\"R_Away\"]) - int(x[\"R_Home\"])) == 1 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a high-scoring game\n",
    "games[\"high_scoring_game\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) + int(x[\"R_Home\"]) >= 10 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a low-scoring game\n",
    "games[\"low_scoring_game\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) + int(x[\"R_Home\"]) <= 3 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a blowout\n",
    "games[\"blowout\"] = games.apply(lambda x: 1 if abs(int(x[\"R_Away\"]) - int(x[\"R_Home\"])) >= 5 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates how many runs were scored in the game\n",
    "games[\"total_runs\"] = games.apply(lambda x: int(x[\"R_Away\"]) + int(x[\"R_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that indicates how many hits were scored in the game\n",
    "games[\"total_hits\"] = games.apply(lambda x: int(x[\"H_Away\"]) + int(x[\"H_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that indicates how many errors were scored in the game\n",
    "games[\"total_errors\"] = games.apply(lambda x: int(x[\"E_Away\"]) + int(x[\"E_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that join the home and away teams\n",
    "games[\"teams\"] = games.apply(lambda x: x[\"Away\"] + \" vs \" + x[\"Home\"], axis=1)\n",
    "\n",
    "# Count occurrences of each team matchup in the 'teams' column\n",
    "team_counts = games['teams'].value_counts()\n",
    "\n",
    "# Map the counts back to the original dataFrame\n",
    "games['team_matchup_count'] = games['teams'].map(team_counts)\n",
    "\n",
    "# Create a group id for each team matchup\n",
    "games['series_id'] = games.groupby('teams').ngroup() + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebf90b",
   "metadata": {},
   "source": [
    "### up to here - GOOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0f6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid-noGutter mb-15 table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def todays_matchup():\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.rotowire.com/baseball/scoreboard.php?date={today}\")\n",
    "\n",
    "    datatable_id = 'grid-noGutter mb-15'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Find indices for where \"Final\" and \"View Box Score\" appear\n",
    "    start_indices = df[df[0].str.contains(\" ET\", case= False)].index\n",
    "    end_indices = df[df[0].str.contains(\"View Box Score\", case=False)].index\n",
    "\n",
    "    # Extract and split data\n",
    "    game_dataframes = []\n",
    "    for start in start_indices:\n",
    "        # Find the corresponding end index that's greater than the start index\n",
    "        end = end_indices[end_indices > start].min()\n",
    "        if pd.notna(end):  # Ensure there's a valid end index\n",
    "            game_data = df.iloc[start:end+1]  # Capture all rows in between\n",
    "            # Split every 8 rows and create DataFrame\n",
    "            reshaped_data = [game_data.iloc[i:i+8] for i in range(0, len(game_data), 8)]\n",
    "            game_dataframes.extend(reshaped_data)\n",
    "\n",
    "    # Final dataframe with all games\n",
    "    final_df = pd.concat(game_dataframes, ignore_index= True)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array for reshaping\n",
    "    data = final_df.values  \n",
    "\n",
    "    # Reshape: each group of 8 rows becomes one row with 8 columns\n",
    "    reshaped_data = [data[i:i+8].flatten() for i in range(0, len(data), 8)]\n",
    "\n",
    "    # Convert reshaped data back to a DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Drop the last column\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.columns[[-1]], axis= 1)\n",
    "\n",
    "    # Add the headers\n",
    "    reshaped_df.columns = ['game_time', 'away_team', 'home_team', 'away_pitcher_name', 'away_pitcher_record', 'home_pitcher_name', 'home_pitcher_record']\n",
    "\n",
    "    # Add the date\n",
    "    reshaped_df['date'] = today\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Get today's matchups\n",
    "todays_matchups = todays_matchup()\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "todays_matchups[\"date\"] = pd.to_datetime(todays_matchups[\"date\"])\n",
    "\n",
    "# Create a game_id column using the index\n",
    "todays_matchups[\"game_id\"] = todays_matchups.index + 1\n",
    "todays_matchups[\"game_id\"] = todays_matchups[\"game_id\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9014c0",
   "metadata": {},
   "source": [
    "## Extract probable pitcher data from https://baseballsavant.mlb.com/probable-pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24fc4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_matchups():\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://baseballsavant.mlb.com/probable-pitchers\")\n",
    "\n",
    "    datatable_id = 'template__content template--two-column__content--one'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Identify rows that contain \" @ \"\n",
    "    split_indices = df[df[0].str.contains(\" @ \")].index.tolist()\n",
    "\n",
    "    # Add start and end indices\n",
    "    split_indices.append(len(df))\n",
    "    split_data = [df.iloc[split_indices[i]:split_indices[i+1]].values.flatten().tolist()\n",
    "                    for i in range(len(split_indices)-1)]\n",
    "\n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame(split_data)\n",
    "\n",
    "    # Splitting on the \" | \" symbol. Splitting only col 2 while keeping other columns\n",
    "    df_expanded = new_df.copy()  # Preserve other columns\n",
    "    df_expanded[['Column1', 'Column2']] = df_expanded[2].str.split(\"ET\", expand=True)\n",
    "\n",
    "    # Drop original column\n",
    "    df_expanded = df_expanded.drop(columns=[2, 3])\n",
    "\n",
    "    #! Removing rows where col 4 contains 'to be announced'\n",
    "    df_filtered = df_expanded[df_expanded[4] != \"To be announced.\"]\n",
    "\n",
    "    # Filtering rows where col6 contains \"Never Faced Any Players on this Team.\"\n",
    "    df_never_faced_the_team = df_filtered[df_filtered[6] == \"Never Faced Any Players on this Team.\"].copy()\n",
    "\n",
    "    # Removing rows from the filtered DataFrame\n",
    "    df_filtered = df_filtered[df_filtered[6] != \"Never Faced Any Players on this Team.\"]\n",
    "\n",
    "    # Convert empty strings to NaN for better handling\n",
    "    df_filtered[17] = df_filtered[17].replace(\"\", pd.NA)\n",
    "\n",
    "    # Count occurrences of each unique non-empty value\n",
    "    value_counts = df_filtered[17].dropna().value_counts()\n",
    "\n",
    "    if not value_counts.empty:\n",
    "        # Identify the most frequent value\n",
    "        most_frequent_value = value_counts.idxmax()\n",
    "\n",
    "        # Fill NaN values with the most frequent value\n",
    "        df_filtered[17] = df_filtered[17].fillna(most_frequent_value)\n",
    "\n",
    "    # Splitting on the \" ET \". Splitting only col 2 while keeping other columns\n",
    "    df_expanded = new_df.copy()  # Preserve other columns\n",
    "    df_expanded[['Column1', 'Column2']] = df_expanded[2].str.split(\"ET\", expand=True)\n",
    "\n",
    "    # Drop original column\n",
    "    df_expanded = df_expanded.drop(columns=[2, 3])\n",
    "\n",
    "    # Splitting columns\n",
    "    df_split = df_filtered.copy()  # Preserve other columns\n",
    "    df_split[['PA_away_pitcher', 'K%_away_pitcher', 'BB%_away_pitcher', 'AVG_away_pitcher', 'wOBA_away_pitcher']] = df_split[8].str.split(\" \", expand=True)\n",
    "    df_split[['Exit_Velo_away_pitcher', 'unit_away', 'Lunch_Angle_away_pitcher', 'xBA_away_pitcher', 'xSLG_away_pitcher', 'xwOBA_away_pitcher']] = df_split[10].str.split(\" \", expand=True)\n",
    "\n",
    "    df_split[['PA_home_pitcher', 'K%_home_pitcher', 'BB%_home_pitcher', 'AVG_home_pitcher', 'wOBA_home_pitcher']] = df_split[16].str.split(\" \", expand=True)\n",
    "    df_split[['Exit_Velo_home_pitcher', 'unit_home', 'Lunch_Angle_home_pitcher', 'xBA_home_pitcher', 'xSLG_home_pitcher', 'xwOBA_home_pitcher']] = df_split[18].str.split(\" \", expand=True)\n",
    "\n",
    "    # Drop original columns if needed\n",
    "    df_split = df_split.drop(columns=[6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19])\n",
    "\n",
    "    # Function to remove city names\n",
    "    def remove_city_names(text):\n",
    "        return re.sub(r'\\b(?:San Diego|Pittsburgh|Arizona|Philadelphia|Kansas City|Baltimore|Tampa Bay|New York|Cleveland|Toronto|Minnesota|Boston|Los Angeles|Atlanta|Houston|Chicago|Seattle|Texas|Milwaukee|St. Louis|Detroit|Colorado|San Francisco)\\b ', '', text)\n",
    "\n",
    "    # Apply the function to the first column of the DataFrame\n",
    "    df_split[0] = df_split[0].apply(remove_city_names)\n",
    "\n",
    "    # Replace @ in col 0 with \"vs\"\n",
    "    df_split[0] = df_split[0].str.replace(\" @ \", \" vs \", regex= True)\n",
    "\n",
    "    # Splitting on the \" vs \". Splitting only col 0 while keeping other columns.\n",
    "    df_split = df_split.copy()  # Preserve other columns\n",
    "    df_split[['away_team', 'home_team']] = df_split[0].str.split(\" vs \", expand=True)\n",
    "\n",
    "    # Update the column with the date\n",
    "    df_split[1] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # # Add the headers\n",
    "    # new_df.columns = ['teams', 'date', 'time_and_park', 'away_pitcher_name', 'away_pitcher_throws', 'away_pitcher_info', \n",
    "    #                     'away_pitcher_fields_1', 'away_pitcher_data_1', 'away_pitcher_fields_2', 'away_pitcher_data_2',]\n",
    "    \n",
    "    return df_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "52da57ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template__content template--two-column__content--one table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "advanced_matchups_df = advanced_matchups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
