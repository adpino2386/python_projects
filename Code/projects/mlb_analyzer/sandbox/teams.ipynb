{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e133bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from rapidfuzz import process\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36493d1f",
   "metadata": {},
   "source": [
    "### Rotowire.com scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2251f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for date: 2025-05-08\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-05-07\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-05-06\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-05-05\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-05-04\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-05-03\n",
      "grid-noGutter mb-15 table loaded successfully.\n",
      "Processing data for date: 2025-05-02\n",
      "grid-noGutter mb-15 table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def teams_matchups(game_date):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.rotowire.com/baseball/scoreboard.php?date={game_date}\")\n",
    "\n",
    "    datatable_id = 'grid-noGutter mb-15'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Find indices for where \"Final\" and \"View Box Score\" appear\n",
    "    start_indices = df[df[0].str.contains(\"Final\", case=False)].index\n",
    "    end_indices = df[df[0].str.contains(\"View Box Score\", case=False)].index\n",
    "\n",
    "    # Extract and split data\n",
    "    game_dataframes = []\n",
    "    for start in start_indices:\n",
    "        # Find the corresponding end index that's greater than the start index\n",
    "        end = end_indices[end_indices > start].min()\n",
    "        if pd.notna(end):  # Ensure there's a valid end index\n",
    "            game_data = df.iloc[start:end+1]  # Capture all rows in between\n",
    "            # Split every 13 rows and create DataFrame\n",
    "            reshaped_data = [game_data.iloc[i:i+13] for i in range(0, len(game_data), 13)]\n",
    "            game_dataframes.extend(reshaped_data)\n",
    "\n",
    "    # Final dataframe with all games\n",
    "    final_df = pd.concat(game_dataframes, ignore_index= True)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array for reshaping\n",
    "    data = final_df.values  \n",
    "\n",
    "    # Reshape: each group of 13 rows becomes one row with 13 columns\n",
    "    reshaped_data = [data[i:i+13].flatten() for i in range(0, len(data), 13)]\n",
    "\n",
    "    # Convert reshaped data back to a DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Drop the first 4 columns and last column\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.columns[[0, 1, 2, 3, -1]], axis= 1)\n",
    "\n",
    "    # Add the headers\n",
    "    reshaped_df.columns = ['Away', 'Home', 'R_Away', 'H_Away', 'E_Away', 'R_Home', 'H_Home', 'E_Home']\n",
    "\n",
    "    # Add the date\n",
    "    reshaped_df['date'] = game_date\n",
    "\n",
    "    # Calculate the winner, loser and the difference in runs, hits and errors\n",
    "    reshaped_df['winner']    = reshaped_df.apply(lambda x: x['Away'] if int(x['R_Away']) > int(x['R_Home']) else x['Home'], axis= 1)\n",
    "    reshaped_df['loser']     = reshaped_df.apply(lambda x: x['Away'] if int(x['R_Away']) < int(x['R_Home']) else x['Home'], axis= 1)\n",
    "    reshaped_df['diff_runs_away_vs_home_team']   = reshaped_df.apply(lambda x: int(x['R_Away']) - int(x['R_Home']), axis= 1)\n",
    "    reshaped_df['diff_hits_away_vs_home_team']   = reshaped_df.apply(lambda x: int(x['H_Away']) - int(x['H_Home']), axis= 1)\n",
    "    reshaped_df['diff_errors_away_vs_home_team'] = reshaped_df.apply(lambda x: int(x['E_Away']) - int(x['E_Home']), axis= 1)\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Define dates\n",
    "dates = [\n",
    "    (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=4)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    ]\n",
    "\n",
    "# Initialize a dictionary to store dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each date and store results in a unique dataframe\n",
    "for game_date in dates:\n",
    "    print(f\"Processing data for date: {game_date}\")\n",
    "    \n",
    "    # Run the teams_matchups function and save the resulting dataframe\n",
    "    result_df = teams_matchups(game_date)\n",
    "    \n",
    "    # Store the dataframe in the dictionary with the date as the key\n",
    "    dataframes[game_date] = result_df\n",
    "\n",
    "# Create the final dataframe\n",
    "games = pd.concat(dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Convert to datetime format\n",
    "games[\"date\"] = pd.to_datetime(games[\"date\"])\n",
    "\n",
    "# Create a game_id column using the index\n",
    "games[\"game_id\"] = games.index + 1\n",
    "games[\"game_id\"] = games[\"game_id\"].astype(str)\n",
    "\n",
    "# Create a key column for the game\n",
    "games[\"key\"] = games[\"date\"].dt.strftime(\"%Y%m%d\") + \"_\" + games[\"game_id\"]\n",
    "\n",
    "# Know if the winner was the away or home team\n",
    "games[\"visitor_won\"]           = games.apply(lambda x: 1 if int(x[\"R_Away\"]) > int(x[\"R_Home\"]) else 0, axis=1)\n",
    "games[\"home_won\"]              = games.apply(lambda x: 1 if int(x[\"R_Away\"]) < int(x[\"R_Home\"]) else 0, axis=1) \n",
    "games[\"visit_or_home_victory\"] = games.apply(lambda x: 'H' if int(x[\"R_Away\"]) < int(x[\"R_Home\"]) else 'V', axis=1) \n",
    "\n",
    "# Create a column that indicates if the game was a shutout\n",
    "games[\"shutout\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) == 0 or int(x[\"R_Home\"]) == 0 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a one-run game\n",
    "games[\"one_run_game\"] = games.apply(lambda x: 1 if abs(int(x[\"R_Away\"]) - int(x[\"R_Home\"])) == 1 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a high-scoring game\n",
    "games[\"high_scoring_game\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) + int(x[\"R_Home\"]) >= 10 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a low-scoring game\n",
    "games[\"low_scoring_game\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) + int(x[\"R_Home\"]) <= 3 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a blowout\n",
    "games[\"blowout\"] = games.apply(lambda x: 1 if abs(int(x[\"R_Away\"]) - int(x[\"R_Home\"])) >= 5 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates how many runs were scored in the game\n",
    "games[\"total_runs\"] = games.apply(lambda x: int(x[\"R_Away\"]) + int(x[\"R_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that indicates how many hits were scored in the game\n",
    "games[\"total_hits\"] = games.apply(lambda x: int(x[\"H_Away\"]) + int(x[\"H_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that indicates how many errors were scored in the game\n",
    "games[\"total_errors\"] = games.apply(lambda x: int(x[\"E_Away\"]) + int(x[\"E_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that join the home and away teams\n",
    "games[\"teams\"] = games.apply(lambda x: x[\"Away\"] + \" vs \" + x[\"Home\"], axis=1)\n",
    "\n",
    "# Count occurrences of each team matchup in the 'teams' column\n",
    "team_counts = games['teams'].value_counts()\n",
    "\n",
    "# Map the counts back to the original dataFrame\n",
    "games['team_matchup_count'] = games['teams'].map(team_counts)\n",
    "\n",
    "# Create a group id for each team matchup\n",
    "games['series_id'] = games.groupby('teams').ngroup() + 1\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "games.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\teams_matchup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384c986",
   "metadata": {},
   "source": [
    "## Basic pitcher information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0f6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid-noGutter mb-15 table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def games_today():\n",
    "    \"\"\"Get today's matchups from the RotoWire scoreboard.\n",
    "    This function uses Selenium to scrape the RotoWire website for today's MLB matchups.\n",
    "    It extracts the game time, away team, home team, away pitcher name, away pitcher record,\n",
    "    home pitcher name, and home pitcher record from the scoreboard.\n",
    "    The data is then reshaped into a DataFrame format for further analysis.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.rotowire.com/baseball/scoreboard.php?date={today}\")\n",
    "\n",
    "    datatable_id = 'grid-noGutter mb-15'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Find indices for where \"Final\" and \"View Box Score\" appear\n",
    "    start_indices = df[df[0].str.contains(\" ET\", case= False)].index\n",
    "    end_indices = df[df[0].str.contains(\"View Box Score\", case=False)].index\n",
    "\n",
    "    # Extract and split data\n",
    "    game_dataframes = []\n",
    "    for start in start_indices:\n",
    "        # Find the corresponding end index that's greater than the start index\n",
    "        end = end_indices[end_indices > start].min()\n",
    "        if pd.notna(end):  # Ensure there's a valid end index\n",
    "            game_data = df.iloc[start:end+1]  # Capture all rows in between\n",
    "            # Split every 8 rows and create DataFrame\n",
    "            reshaped_data = [game_data.iloc[i:i+8] for i in range(0, len(game_data), 8)]\n",
    "            game_dataframes.extend(reshaped_data)\n",
    "\n",
    "    # Final dataframe with all games\n",
    "    final_df = pd.concat(game_dataframes, ignore_index= True)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array for reshaping\n",
    "    data = final_df.values  \n",
    "\n",
    "    # Reshape: each group of 8 rows becomes one row with 8 columns\n",
    "    reshaped_data = [data[i:i+8].flatten() for i in range(0, len(data), 8)]\n",
    "\n",
    "    # Convert reshaped data back to a DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Drop the last column\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.columns[[-1]], axis= 1)\n",
    "\n",
    "    # Add the headers\n",
    "    reshaped_df.columns = ['game_time', 'away_team', 'home_team', 'away_pitcher_name', 'away_pitcher_record', 'home_pitcher_name', 'home_pitcher_record']\n",
    "\n",
    "    # Add the date\n",
    "    reshaped_df['date'] = today\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Get today's matchups\n",
    "games_today_df = games_today()\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "games_today_df[\"date\"] = pd.to_datetime(games_today_df[\"date\"])\n",
    "\n",
    "# Create a game_id column using the index\n",
    "games_today_df[\"game_id\"] = games_today_df.index + 1\n",
    "games_today_df[\"game_id\"] = games_today_df[\"game_id\"].astype(str)\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "games_today_df.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\games_today.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec890c",
   "metadata": {},
   "source": [
    "## Advanced Matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fc4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template__content template--two-column__content--one table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def advanced_matchups():\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://baseballsavant.mlb.com/probable-pitchers\")\n",
    "\n",
    "    datatable_id = 'template__content template--two-column__content--one'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Identify rows that contain \" @ \"\n",
    "    split_indices = df[df[0].str.contains(\" @ \")].index.tolist()\n",
    "\n",
    "    # Add start and end indices\n",
    "    split_indices.append(len(df))\n",
    "    split_data = [df.iloc[split_indices[i]:split_indices[i+1]].values.flatten().tolist()\n",
    "                    for i in range(len(split_indices)-1)]\n",
    "\n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame(split_data)\n",
    "\n",
    "    # Splitting on the \" | \" symbol. Splitting only col 2 while keeping other columns\n",
    "    df_expanded = new_df.copy()  # Preserve other columns\n",
    "    df_expanded[['Column1', 'Column2']] = df_expanded[2].str.split(\"ET\", expand=True)\n",
    "\n",
    "    # Drop original column\n",
    "    df_expanded = df_expanded.drop(columns=[2, 3])\n",
    "\n",
    "    #! Removing rows where col 4 contains 'to be announced'\n",
    "    df_filtered = df_expanded[df_expanded[4] != \"To be announced.\"]\n",
    "\n",
    "    # Filtering rows where col6 contains \"Never Faced Any Players on this Team.\"\n",
    "    df_never_faced_the_team = df_filtered[df_filtered[6] == \"Never Faced Any Players on this Team.\"].copy()\n",
    "\n",
    "    # Removing rows from the filtered DataFrame\n",
    "    df_filtered = df_filtered[df_filtered[6] != \"Never Faced Any Players on this Team.\"]\n",
    "\n",
    "    # Filtering rows where col9 does not contains \"Exit Velo Launch Angle xBA xSLG xwOBA\"\n",
    "    df_not_complete = df_filtered[df_filtered[9] != \"Exit Velo Launch Angle xBA xSLG xwOBA\"].copy()\n",
    "\n",
    "    # Removing rows from the filtered DataFrame\n",
    "    df_filtered = df_filtered[df_filtered[9] == \"Exit Velo Launch Angle xBA xSLG xwOBA\"]\n",
    "\n",
    "    # Convert empty strings to NaN for better handling\n",
    "    df_filtered[17] = df_filtered[17].replace(\"\", pd.NA)\n",
    "\n",
    "    # Count occurrences of each unique non-empty value\n",
    "    value_counts = df_filtered[17].dropna().value_counts()\n",
    "\n",
    "    if not value_counts.empty:\n",
    "        # Identify the most frequent value\n",
    "        most_frequent_value = value_counts.idxmax()\n",
    "\n",
    "        # Fill NaN values with the most frequent value\n",
    "        df_filtered[17] = df_filtered[17].fillna(most_frequent_value)\n",
    "\n",
    "    # Splitting on the \" ET \". Splitting only col 2 while keeping other columns\n",
    "    df_expanded = new_df.copy()  # Preserve other columns\n",
    "    df_expanded[['Column1', 'Column2']] = df_expanded[2].str.split(\"ET\", expand=True)\n",
    "\n",
    "    # Drop original column\n",
    "    df_expanded = df_expanded.drop(columns=[2, 3])\n",
    "\n",
    "    # Splitting columns\n",
    "    df_split = df_filtered.copy()  # Preserve other columns\n",
    "    df_split[['PA_away_pitcher', 'K%_away_pitcher', 'BB%_away_pitcher', 'AVG_away_pitcher', 'wOBA_away_pitcher']] = df_split[8].str.split(\" \", expand=True)\n",
    "    df_split[['Exit_Velo_away_pitcher', 'unit_away', 'Lunch_Angle_away_pitcher', 'xBA_away_pitcher', 'xSLG_away_pitcher', 'xwOBA_away_pitcher']] = df_split[10].str.split(\" \", expand=True)\n",
    "\n",
    "    df_split[['PA_home_pitcher', 'K%_home_pitcher', 'BB%_home_pitcher', 'AVG_home_pitcher', 'wOBA_home_pitcher']] = df_split[16].str.split(\" \", expand=True)\n",
    "    df_split[['Exit_Velo_home_pitcher', 'unit_home', 'Lunch_Angle_home_pitcher', 'xBA_home_pitcher', 'xSLG_home_pitcher', 'xwOBA_home_pitcher']] = df_split[18].str.split(\" \", expand=True)\n",
    "\n",
    "    # Drop original columns if needed\n",
    "    df_split = df_split.drop(columns=[6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19])\n",
    "\n",
    "    # Function to remove city names\n",
    "    def remove_city_names(text):\n",
    "        return re.sub(r'\\b(?:San Diego|Pittsburgh|Arizona|Philadelphia|Kansas City|Baltimore|Tampa Bay|New York|Cleveland|Toronto|Minnesota|Boston|Los Angeles|Atlanta|Houston|Chicago|Seattle|Texas|Milwaukee|St. Louis|Detroit|Colorado|San Francisco)\\b ', '', text)\n",
    "\n",
    "    # Apply the function to the first column of the DataFrame\n",
    "    df_split[0] = df_split[0].apply(remove_city_names)\n",
    "\n",
    "    # Replace @ in col 0 with \"vs\"\n",
    "    df_split[0] = df_split[0].str.replace(\" @ \", \" vs \", regex= True)\n",
    "\n",
    "    # Splitting on the \" vs \". Splitting only col 0 while keeping other columns.\n",
    "    df_split = df_split.copy()  # Preserve other columns\n",
    "    df_split[['away_team', 'home_team']] = df_split[0].str.split(\" vs \", expand=True)\n",
    "\n",
    "    # Update the column with the date\n",
    "    df_split[1] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # # Add the headers\n",
    "    # new_df.columns = ['teams', 'date', 'time_and_park', 'away_pitcher_name', 'away_pitcher_throws', 'away_pitcher_info', \n",
    "    #                     'away_pitcher_fields_1', 'away_pitcher_data_1', 'away_pitcher_fields_2', 'away_pitcher_data_2',]\n",
    "    \n",
    "    return df_split, df_never_faced_the_team, df_not_complete\n",
    "\n",
    "\n",
    "# Call the function to get today's matchups with advanced stats\n",
    "advanced_matchups_df, never_faced_the_team_df, df_not_complete = advanced_matchups()\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "advanced_matchups_df.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\advanced_matchups\\\\advanced_matchups.csv', index=False)\n",
    "never_faced_the_team_df.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\advanced_matchups\\\\never_faced_the_team.csv', index=False)\n",
    "df_not_complete.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\advanced_matchups\\\\not_complete.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b4df6",
   "metadata": {},
   "source": [
    "# Import advanced data and gamelogs for each team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d2900",
   "metadata": {},
   "source": [
    "#### This works for hitting and pitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84020fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def team_advanced_stats(analysis_type):\n",
    "#     \"\"\"Get the advanced stats for the team.\n",
    "#     The function scrapes the advanced stats from the MLB website using Selenium and returns three dataframes:\n",
    "#     statcast, plate_discipline and batted_ball_profile.\n",
    "\n",
    "#     Returns:\n",
    "#         _type_: _description_\n",
    "#     \"\"\"\n",
    "#     # Load the options\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "#     options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "#     year = datetime.now().year\n",
    "\n",
    "#     # Set up the WebDriver\n",
    "#     driver = webdriver.Chrome(options= options)    \n",
    "#     driver.get(f\"https://baseballsavant.mlb.com/team/114?view=statcast&nav={analysis_type}&season={year}\")\n",
    "\n",
    "#     datatable_id = 'div_statcast'\n",
    "#     datatable_xpath = f\"//div[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "\n",
    "#     # Initialize an empty dataframe\n",
    "#     team_advanced_stats_df = pd.DataFrame()\n",
    "\n",
    "#     try:\n",
    "#         # Explicitly wait for the table element to load\n",
    "#         WebDriverWait(driver, 20).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "#         )\n",
    "#         print(f\"{datatable_id} table loaded successfully.\")\n",
    "\n",
    "#         # Locate the table\n",
    "#         table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "#         text_content = table_element.text\n",
    "\n",
    "#         # Process the table content\n",
    "#         rows = text_content.split(\"\\n\")\n",
    "#         table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "#         # Convert to dataframe\n",
    "#         team_advanced_stats_df = pd.DataFrame(table_data)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: Table {datatable_id} did not load. Returning an empty dataframe. Details: {e}\")\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "    \n",
    "#     # Check if the dataframe is empty\n",
    "#     # If the dataframe is empty, return empty dataframes\n",
    "#     if team_advanced_stats_df.empty:\n",
    "#         print(\"No data found. Skipping table creation.\")        \n",
    "#         return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()  # Return empty dataframes\n",
    "#     else:\n",
    "        \n",
    "#         #! STATCAST TABLE\n",
    "#         #! Cleaning the data for the statcast table\n",
    "\n",
    "\n",
    "#         def combine_rows(df, row_number_1, row_number_2):\n",
    "#             \"\"\"Combine two rows in a DataFrame into one row.\n",
    "#             The first row will contain the combined data, and the second row will be dropped.\n",
    "\n",
    "#             Args:\n",
    "#                 df (_type_): _description_\n",
    "#                 row_number_1 (_type_): _description_\n",
    "#                 row_number_2 (_type_): _description_\n",
    "\n",
    "#             Returns:\n",
    "#                 _type_: _description_\n",
    "#             \"\"\"\n",
    "#             # Combine the two rows\n",
    "#             df.loc[row_number_1, 0] = df.loc[row_number_1, 0] + ' ' + df.loc[row_number_2, 0]\n",
    "\n",
    "#             # Drop the second row\n",
    "#             df = df.drop(row_number_2).reset_index(drop=True)\n",
    "            \n",
    "#             return df\n",
    "\n",
    "\n",
    "#         # Join rows\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 19, 20)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 23, 24)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 24, 25)\n",
    "\n",
    "#         #! Create the headers for the first table (statcast)\n",
    "#         # Find the first occurrence of 'Player' and 'XWOBACON'\n",
    "#         # Find indices\n",
    "#         start_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Player'].index[0]\n",
    "#         end_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'XWOBACON'].index[0]\n",
    "\n",
    "#         # Slice and transpose\n",
    "#         headers_statcast = team_advanced_stats_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "#         # Reset column names\n",
    "#         headers_statcast.columns = headers_statcast.iloc[0]\n",
    "#         headers_statcast         = headers_statcast[1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove those rows from original dataframe\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(team_advanced_stats_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "#         # Remove the first 3 rows\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[3:].reset_index(drop=True)  # Using iloc\n",
    "\n",
    "#         # Use regex to split the column while preserving negative numbers\n",
    "#         team_advanced_stats_df[['Name', 'Numbers']] = team_advanced_stats_df[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "\n",
    "#         # Split the 'Numbers' column into separate columns (25 columns)\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.join(team_advanced_stats_df['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "\n",
    "#         # Drop the original 'Numbers' column\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(columns=['Numbers'])\n",
    "\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Extract rows from the start until the first empty row\n",
    "#         statcast = team_advanced_stats_df.iloc[:first_empty_idx]\n",
    "\n",
    "#         # Drop the first column\n",
    "#         statcast = statcast.drop(columns=[0])\n",
    "\n",
    "#         # Swap last name and first name\n",
    "#         statcast['Name'] = statcast['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "#         # Add the headers to the dataframe\n",
    "#         statcast.columns = headers_statcast.columns\n",
    "\n",
    "#         #! PLATE DISCIPLINE TABLE\n",
    "#         #! Cleaning the data for the plate discipline table\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Remove rows from the first row until the first empty row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove the first row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "#         # Join rows\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 4, 5)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 5, 6)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 7, 8)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 9, 10)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 13, 14)\n",
    "\n",
    "#         #! Create the headers for the second table (plate discipline)\n",
    "#         # Find the first occurrence of 'Player' and 'Meatball Swing %'\n",
    "#         # Find indices\n",
    "#         start_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Player'].index[0]\n",
    "#         end_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Meatball Swing %'].index[0]\n",
    "\n",
    "#         # Slice and transpose\n",
    "#         headers_plate_discipline = team_advanced_stats_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "#         # Reset column names\n",
    "#         headers_plate_discipline.columns = headers_plate_discipline.iloc[0]\n",
    "#         headers_plate_discipline         = headers_plate_discipline[1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove those rows from original dataframe\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(team_advanced_stats_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Extract rows from the start until the first empty row\n",
    "#         plate_discipline = team_advanced_stats_df.iloc[:first_empty_idx]\n",
    "\n",
    "#         # Drop the first column\n",
    "#         plate_discipline = plate_discipline.drop(columns=[0])\n",
    "\n",
    "#         # Swap last name and first name\n",
    "#         plate_discipline['Name'] = plate_discipline['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "#         # Drop columns where all values are NaN (empty)\n",
    "#         plate_discipline = plate_discipline.dropna(axis= 1, how= 'all')\n",
    "\n",
    "#         # Add the headers to the dataframe\n",
    "#         plate_discipline.columns = headers_plate_discipline.columns\n",
    "\n",
    "#         #! BATTED BALL PROFILE TABLE\n",
    "#         #! Cleaning the data for the batted ball profile table\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Remove rows from the first row until the first empty row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove the first row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "#         # Join rows\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 15, 16)\n",
    "\n",
    "#         #! Create the headers for the third table (batted ball profile)\n",
    "#         # Find the first occurrence of 'Player' and 'Barrel %'\n",
    "#         # Find indices\n",
    "#         start_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Player'].index[0]\n",
    "#         end_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Barrel %'].index[0]\n",
    "\n",
    "#         # Slice and transpose\n",
    "#         headers_batted_ball_profile = team_advanced_stats_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "#         # Reset column names\n",
    "#         headers_batted_ball_profile.columns = headers_batted_ball_profile.iloc[0]\n",
    "#         headers_batted_ball_profile         = headers_batted_ball_profile[1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove those rows from original dataframe\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(team_advanced_stats_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         if pd.isna(first_empty_idx):\n",
    "#             batted_ball_profile = team_advanced_stats_df.copy()  # If no empty row, use the entire DataFrame\n",
    "#         else:\n",
    "#             # Extract rows from the start until the first empty row\n",
    "#             batted_ball_profile = team_advanced_stats_df.iloc[:first_empty_idx]\n",
    "\n",
    "#         # Drop the first column\n",
    "#         batted_ball_profile = batted_ball_profile.drop(columns=[0])\n",
    "\n",
    "#         # Swap last name and first name\n",
    "#         batted_ball_profile['Name'] = batted_ball_profile['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "#         # Drop columns where all values are NaN (empty)\n",
    "#         batted_ball_profile = batted_ball_profile.dropna(axis= 1, how= 'all')\n",
    "\n",
    "#         # Add the headers to the dataframe\n",
    "#         batted_ball_profile.columns = headers_batted_ball_profile.columns\n",
    "        \n",
    "#         return statcast, plate_discipline, batted_ball_profile\n",
    "\n",
    "\n",
    "# # Call the function to get today's matchups with advanced stats\n",
    "# #hitting_statcast_df,  hitting_plate_discipline_df,  hitting_batted_ball_profile_df  = team_advanced_stats(analysis_type= 'hitting')\n",
    "# pitching_statcast_df, pitching_plate_discipline_df, pitching_batted_ball_profile_df = team_advanced_stats(analysis_type= 'pitching')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #! STATCAST TABLE\n",
    "# #! Cleaning the data for the statcast table\n",
    "\n",
    "# def combine_rows(df, row_number_1, row_number_2):\n",
    "#     \"\"\"Combine two rows in a DataFrame into one row.\n",
    "#     The first row will contain the combined data, and the second row will be dropped.\n",
    "\n",
    "#     Args:\n",
    "#         df (_type_): _description_\n",
    "#         row_number_1 (_type_): _description_\n",
    "#         row_number_2 (_type_): _description_\n",
    "\n",
    "#     Returns:\n",
    "#         _type_: _description_\n",
    "#     \"\"\"\n",
    "#     # Combine the two rows\n",
    "#     df.loc[row_number_1, 0] = df.loc[row_number_1, 0] + ' ' + df.loc[row_number_2, 0]\n",
    "\n",
    "#     # Drop the second row\n",
    "#     df = df.drop(row_number_2).reset_index(drop=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# # Join rows\n",
    "# test_df = combine_rows(test_df, 19, 20)\n",
    "# test_df = combine_rows(test_df, 23, 24)\n",
    "# test_df = combine_rows(test_df, 24, 25)\n",
    "\n",
    "# #! Create the headers for the first table (statcast)\n",
    "# # Find the first occurrence of 'Player' and 'XWOBACON'\n",
    "# # Find indices\n",
    "# start_idx = test_df[test_df[0] == 'Player'].index[0]\n",
    "# end_idx = test_df[test_df[0] == 'XWOBACON'].index[0]\n",
    "\n",
    "# # Slice and transpose\n",
    "# headers_statcast = test_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "# # Reset column names\n",
    "# headers_statcast.columns = headers_statcast.iloc[0]\n",
    "# headers_statcast         = headers_statcast[1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove those rows from original dataframe\n",
    "# test_df = test_df.drop(test_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "# # Remove the first 3 rows\n",
    "# test_df = test_df.iloc[3:].reset_index(drop=True)  # Using iloc\n",
    "\n",
    "# # Use regex to split the column while preserving negative numbers\n",
    "# test_df[['Name', 'Numbers']] = test_df[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "\n",
    "# # Split the 'Numbers' column into separate columns (25 columns)\n",
    "# test_df = test_df.join(test_df['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "\n",
    "# # Drop the original 'Numbers' column\n",
    "# test_df = test_df.drop(columns=['Numbers'])\n",
    "\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Extract rows from the start until the first empty row\n",
    "# statcast = test_df.iloc[:first_empty_idx]\n",
    "\n",
    "# # Drop the first column\n",
    "# statcast = statcast.drop(columns=[0])\n",
    "\n",
    "# # Swap last name and first name\n",
    "# statcast['Name'] = statcast['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "# # Add the headers to the dataframe\n",
    "# statcast.columns = headers_statcast.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89195ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #! PLATE DISCIPLINE TABLE\n",
    "# #! Cleaning the data for the plate discipline table\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Remove rows from the first row until the first empty row\n",
    "# test_df = test_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove the first row\n",
    "# test_df = test_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "# # Join rows\n",
    "# test_df = combine_rows(test_df, 4, 5)\n",
    "# test_df = combine_rows(test_df, 5, 6)\n",
    "# test_df = combine_rows(test_df, 7, 8)\n",
    "# test_df = combine_rows(test_df, 9, 10)\n",
    "# test_df = combine_rows(test_df, 13, 14)\n",
    "\n",
    "# #! Create the headers for the second table (plate discipline)\n",
    "# # Find the first occurrence of 'Player' and 'Meatball Swing %'\n",
    "# # Find indices\n",
    "# start_idx = test_df[test_df[0] == 'Player'].index[0]\n",
    "# end_idx = test_df[test_df[0] == 'Meatball Swing %'].index[0]\n",
    "\n",
    "# # Slice and transpose\n",
    "# headers_plate_discipline = test_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "# # Reset column names\n",
    "# headers_plate_discipline.columns = headers_plate_discipline.iloc[0]\n",
    "# headers_plate_discipline         = headers_plate_discipline[1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove those rows from original dataframe\n",
    "# test_df = test_df.drop(test_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Extract rows from the start until the first empty row\n",
    "# plate_discipline = test_df.iloc[:first_empty_idx]\n",
    "\n",
    "# # Drop the first column\n",
    "# plate_discipline = plate_discipline.drop(columns=[0])\n",
    "\n",
    "# # Swap last name and first name\n",
    "# plate_discipline['Name'] = plate_discipline['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "# # Drop columns where all values are NaN (empty)\n",
    "# plate_discipline = plate_discipline.dropna(axis= 1, how= 'all')\n",
    "\n",
    "# # Add the headers to the dataframe\n",
    "# plate_discipline.columns = headers_plate_discipline.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536985b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #! BATTED BALL PROFILE TABLE\n",
    "# #! Cleaning the data for the batted ball profile table\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Remove rows from the first row until the first empty row\n",
    "# test_df = test_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove the first row\n",
    "# test_df = test_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "# # Join rows\n",
    "# test_df = combine_rows(test_df, 15, 16)\n",
    "\n",
    "# #! Create the headers for the third table (batted ball profile)\n",
    "# # Find the first occurrence of 'Player' and 'Barrel %'\n",
    "# # Find indices\n",
    "# start_idx = test_df[test_df[0] == 'Player'].index[0]\n",
    "# end_idx = test_df[test_df[0] == 'Barrel %'].index[0]\n",
    "\n",
    "# # Slice and transpose\n",
    "# headers_batted_ball_profile = test_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "# # Reset column names\n",
    "# headers_batted_ball_profile.columns = headers_batted_ball_profile.iloc[0]\n",
    "# headers_batted_ball_profile         = headers_batted_ball_profile[1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove those rows from original dataframe\n",
    "# test_df = test_df.drop(test_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# if pd.isna(first_empty_idx):\n",
    "#     batted_ball_profile = test_df.copy()  # If no empty row, use the entire DataFrame\n",
    "# else:\n",
    "#     # Extract rows from the start until the first empty row\n",
    "#     batted_ball_profile = test_df.iloc[:first_empty_idx]\n",
    "\n",
    "# # Drop the first column\n",
    "# batted_ball_profile = batted_ball_profile.drop(columns=[0])\n",
    "\n",
    "# # Swap last name and first name\n",
    "# batted_ball_profile['Name'] = batted_ball_profile['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "# # Drop columns where all values are NaN (empty)\n",
    "# batted_ball_profile = batted_ball_profile.dropna(axis= 1, how= 'all')\n",
    "\n",
    "# # Add the headers to the dataframe\n",
    "# batted_ball_profile.columns = headers_batted_ball_profile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76b1deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitching data successfully retrieved for team 108\n",
      "pitching data successfully retrieved for team 117\n",
      "pitching data successfully retrieved for team 133\n",
      "pitching data successfully retrieved for team 141\n",
      "pitching data successfully retrieved for team 144\n",
      "pitching data successfully retrieved for team 158\n",
      "pitching data successfully retrieved for team 138\n",
      "pitching data successfully retrieved for team 112\n",
      "Skipping team 109 due to error: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7A9EFCF25+75717]\n",
      "\tGetHandleVerifier [0x00007FF7A9EFCF80+75808]\n",
      "\t(No symbol) [0x00007FF7A9CC8F9A]\n",
      "\t(No symbol) [0x00007FF7A9D1F4C6]\n",
      "\t(No symbol) [0x00007FF7A9D1F77C]\n",
      "\t(No symbol) [0x00007FF7A9D72577]\n",
      "\t(No symbol) [0x00007FF7A9D473BF]\n",
      "\t(No symbol) [0x00007FF7A9D6F39C]\n",
      "\t(No symbol) [0x00007FF7A9D47153]\n",
      "\t(No symbol) [0x00007FF7A9D10421]\n",
      "\t(No symbol) [0x00007FF7A9D111B3]\n",
      "\tGetHandleVerifier [0x00007FF7AA1FD6FD+3223453]\n",
      "\tGetHandleVerifier [0x00007FF7AA1F7CA2+3200322]\n",
      "\tGetHandleVerifier [0x00007FF7AA215AD3+3322739]\n",
      "\tGetHandleVerifier [0x00007FF7A9F169FA+180890]\n",
      "\tGetHandleVerifier [0x00007FF7A9F1E0FF+211359]\n",
      "\tGetHandleVerifier [0x00007FF7A9F05274+109332]\n",
      "\tGetHandleVerifier [0x00007FF7A9F05422+109762]\n",
      "\tGetHandleVerifier [0x00007FF7A9EEBA39+4825]\n",
      "\tBaseThreadInitThunk [0x00007FFFA6BB7374+20]\n",
      "\tRtlUserThreadStart [0x00007FFFA889CC91+33]\n",
      "\n",
      "pitching data successfully retrieved for team 119\n",
      "pitching data successfully retrieved for team 137\n",
      "pitching data successfully retrieved for team 114\n",
      "pitching data successfully retrieved for team 137\n",
      "pitching data successfully retrieved for team 114\n",
      "pitching data successfully retrieved for team 136\n",
      "pitching data successfully retrieved for team 146\n",
      "pitching data successfully retrieved for team 121\n",
      "pitching data successfully retrieved for team 120\n",
      "pitching data successfully retrieved for team 110\n",
      "pitching data successfully retrieved for team 135\n",
      "pitching data successfully retrieved for team 143\n",
      "pitching data successfully retrieved for team 134\n",
      "pitching data successfully retrieved for team 140\n",
      "pitching data successfully retrieved for team 139\n",
      "pitching data successfully retrieved for team 113\n",
      "pitching data successfully retrieved for team 111\n",
      "pitching data successfully retrieved for team 115\n",
      "pitching data successfully retrieved for team 118\n",
      "pitching data successfully retrieved for team 116\n",
      "pitching data successfully retrieved for team 142\n",
      "pitching data successfully retrieved for team 145\n",
      "pitching data successfully retrieved for team 147\n",
      "hitting data successfully retrieved for team 108\n",
      "hitting data successfully retrieved for team 117\n",
      "hitting data successfully retrieved for team 133\n",
      "hitting data successfully retrieved for team 141\n",
      "hitting data successfully retrieved for team 144\n",
      "hitting data successfully retrieved for team 158\n",
      "hitting data successfully retrieved for team 138\n",
      "hitting data successfully retrieved for team 112\n",
      "hitting data successfully retrieved for team 109\n",
      "hitting data successfully retrieved for team 119\n",
      "hitting data successfully retrieved for team 137\n",
      "hitting data successfully retrieved for team 114\n",
      "hitting data successfully retrieved for team 137\n",
      "hitting data successfully retrieved for team 114\n",
      "hitting data successfully retrieved for team 136\n",
      "hitting data successfully retrieved for team 146\n",
      "hitting data successfully retrieved for team 121\n",
      "hitting data successfully retrieved for team 120\n",
      "hitting data successfully retrieved for team 110\n",
      "hitting data successfully retrieved for team 135\n",
      "hitting data successfully retrieved for team 143\n",
      "hitting data successfully retrieved for team 134\n",
      "hitting data successfully retrieved for team 140\n",
      "hitting data successfully retrieved for team 139\n",
      "hitting data successfully retrieved for team 113\n",
      "hitting data successfully retrieved for team 111\n",
      "hitting data successfully retrieved for team 115\n",
      "hitting data successfully retrieved for team 118\n",
      "hitting data successfully retrieved for team 116\n",
      "hitting data successfully retrieved for team 142\n",
      "hitting data successfully retrieved for team 145\n",
      "hitting data successfully retrieved for team 147\n",
      "pitching gamelogs successfully retrieved for team 108\n",
      "pitching gamelogs successfully retrieved for team 117\n",
      "pitching gamelogs successfully retrieved for team 133\n",
      "pitching gamelogs successfully retrieved for team 141\n",
      "pitching gamelogs successfully retrieved for team 144\n",
      "pitching gamelogs successfully retrieved for team 158\n",
      "pitching gamelogs successfully retrieved for team 138\n",
      "pitching gamelogs successfully retrieved for team 112\n",
      "pitching gamelogs successfully retrieved for team 109\n",
      "pitching gamelogs successfully retrieved for team 119\n",
      "pitching gamelogs successfully retrieved for team 137\n",
      "pitching gamelogs successfully retrieved for team 114\n",
      "pitching gamelogs successfully retrieved for team 137\n",
      "pitching gamelogs successfully retrieved for team 114\n",
      "pitching gamelogs successfully retrieved for team 136\n",
      "pitching gamelogs successfully retrieved for team 146\n",
      "pitching gamelogs successfully retrieved for team 121\n",
      "pitching gamelogs successfully retrieved for team 120\n",
      "pitching gamelogs successfully retrieved for team 110\n",
      "pitching gamelogs successfully retrieved for team 135\n",
      "pitching gamelogs successfully retrieved for team 143\n",
      "pitching gamelogs successfully retrieved for team 134\n",
      "pitching gamelogs successfully retrieved for team 140\n",
      "pitching gamelogs successfully retrieved for team 139\n",
      "pitching gamelogs successfully retrieved for team 113\n",
      "pitching gamelogs successfully retrieved for team 111\n",
      "pitching gamelogs successfully retrieved for team 115\n",
      "pitching gamelogs successfully retrieved for team 118\n",
      "pitching gamelogs successfully retrieved for team 116\n",
      "pitching gamelogs successfully retrieved for team 142\n",
      "pitching gamelogs successfully retrieved for team 145\n",
      "pitching gamelogs successfully retrieved for team 147\n",
      "hitting gamelogs successfully retrieved for team 108\n",
      "hitting gamelogs successfully retrieved for team 117\n",
      "hitting gamelogs successfully retrieved for team 133\n",
      "hitting gamelogs successfully retrieved for team 141\n",
      "hitting gamelogs successfully retrieved for team 144\n",
      "hitting gamelogs successfully retrieved for team 158\n",
      "hitting gamelogs successfully retrieved for team 138\n",
      "hitting gamelogs successfully retrieved for team 112\n",
      "hitting gamelogs successfully retrieved for team 109\n",
      "hitting gamelogs successfully retrieved for team 119\n",
      "hitting gamelogs successfully retrieved for team 137\n",
      "hitting gamelogs successfully retrieved for team 114\n",
      "hitting gamelogs successfully retrieved for team 137\n",
      "hitting gamelogs successfully retrieved for team 114\n",
      "hitting gamelogs successfully retrieved for team 136\n",
      "hitting gamelogs successfully retrieved for team 146\n",
      "hitting gamelogs successfully retrieved for team 121\n",
      "hitting gamelogs successfully retrieved for team 120\n",
      "hitting gamelogs successfully retrieved for team 110\n",
      "hitting gamelogs successfully retrieved for team 135\n",
      "hitting gamelogs successfully retrieved for team 143\n",
      "hitting gamelogs successfully retrieved for team 134\n",
      "hitting gamelogs successfully retrieved for team 140\n",
      "hitting gamelogs successfully retrieved for team 139\n",
      "hitting gamelogs successfully retrieved for team 113\n",
      "hitting gamelogs successfully retrieved for team 111\n",
      "hitting gamelogs successfully retrieved for team 115\n",
      "hitting gamelogs successfully retrieved for team 118\n",
      "hitting gamelogs successfully retrieved for team 116\n",
      "hitting gamelogs successfully retrieved for team 142\n",
      "hitting gamelogs successfully retrieved for team 145\n",
      "hitting gamelogs successfully retrieved for team 147\n"
     ]
    }
   ],
   "source": [
    "def import_advanced_stats(analysis_type, team_ids, category):\n",
    "    \"\"\"Get the advanced stats for multiple teams.\n",
    "    The function scrapes the advanced stats from the MLB website using Selenium and returns a dictionary of dataframes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping team IDs to their respective advanced stats dataframe.\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    year = datetime.now().year\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    team_data = {}\n",
    "\n",
    "    for team_id in team_ids:\n",
    "        try:\n",
    "            if category == \"statcast\":\n",
    "                url = f\"https://baseballsavant.mlb.com/team/{team_id}?view={category}&nav={analysis_type}&season={year}\"\n",
    "                datatable_id = 'div_statcast'\n",
    "            elif category == \"gamelogs\":\n",
    "                url = f\"https://baseballsavant.mlb.com/team/{team_id}?view={category}&nav={analysis_type}&season={year}\"\n",
    "                datatable_id = 'div_gamelogs'\n",
    "\n",
    "            driver.get(url)\n",
    "            \n",
    "            datatable_xpath = f\"//div[@id='{datatable_id}']\"\n",
    "\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "            )\n",
    "\n",
    "            table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "            text_content = table_element.text\n",
    "\n",
    "            rows = text_content.split(\"\\n\")\n",
    "            table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "            team_data[team_id] = pd.DataFrame(table_data)\n",
    "\n",
    "            # Print the status of the data retrieval\n",
    "            if category == \"statcast\":\n",
    "                print(f\"{analysis_type} data successfully retrieved for team {team_id}\")\n",
    "            elif category == \"gamelogs\":\n",
    "                print(f\"{analysis_type} gamelogs successfully retrieved for team {team_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping team {team_id} due to error: {e}\")\n",
    "            continue  # Skip to the next iteration\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    return team_data\n",
    "\n",
    "# Import the tables for each team\n",
    "team_ids = [108, 117, 133, 141, 144, 158, 138, 112, 109, 119, 137, 114,\n",
    "            137, 114, 136, 146, 121, 120, 110, 135, 143, 134, 140, 139,\n",
    "            113, 111, 115, 118, 116, 142, 145, 147]\n",
    "\n",
    "# Import the advanced stats for each team\n",
    "data_pitching = import_advanced_stats(\"pitching\", team_ids, \"statcast\")\n",
    "data_hitting = import_advanced_stats(\"hitting\", team_ids, \"statcast\")\n",
    "\n",
    "# Import the gamelogs for each team\n",
    "gamelog_data_pitching = import_advanced_stats(\"pitching\", team_ids, \"gamelogs\")\n",
    "gamelog_data_hitting = import_advanced_stats(\"hitting\", team_ids, \"gamelogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c37ce2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_advanced_stats(dataframe_name):\n",
    "    \"\"\"Get the advanced stats for the team.\n",
    "    The function scrapes the advanced stats from the MLB website using Selenium and returns three dataframes:\n",
    "    statcast, plate_discipline and batted_ball_profile.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"   \n",
    "        \n",
    "    #! STATCAST TABLE\n",
    "    #! Cleaning the data for the statcast table\n",
    "    def combine_rows(df, row_number_1, row_number_2):\n",
    "        \"\"\"Combine two rows in a DataFrame into one row.\n",
    "        The first row will contain the combined data, and the second row will be dropped.\n",
    "\n",
    "        Args:\n",
    "            df (_type_): _description_\n",
    "            row_number_1 (_type_): _description_\n",
    "            row_number_2 (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Combine the two rows\n",
    "        df.loc[row_number_1, 0] = df.loc[row_number_1, 0] + ' ' + df.loc[row_number_2, 0]\n",
    "\n",
    "        # Drop the second row\n",
    "        df = df.drop(row_number_2).reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    # Join rows\n",
    "    dataframe_name = combine_rows(dataframe_name, 19, 20)\n",
    "    dataframe_name = combine_rows(dataframe_name, 23, 24)\n",
    "    dataframe_name = combine_rows(dataframe_name, 24, 25)\n",
    "\n",
    "    #! Create the headers for the first table (statcast)\n",
    "    # Find the first occurrence of 'Player' and 'XWOBACON'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Player'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'XWOBACON'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_statcast = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_statcast.columns = headers_statcast.iloc[0]\n",
    "    headers_statcast         = headers_statcast[1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "    # Remove the first 3 rows\n",
    "    dataframe_name = dataframe_name.iloc[3:].reset_index(drop=True)  # Using iloc\n",
    "\n",
    "    # Use regex to split the column while preserving negative numbers\n",
    "    dataframe_name[['Name', 'Numbers']] = dataframe_name[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "    \n",
    "    # Remove initial dot from values that start with \".\"\n",
    "    dataframe_name['Numbers'] = dataframe_name['Numbers'].apply(lambda x: x[1:] if x.startswith('. ') else x)\n",
    "\n",
    "    # Split the 'Numbers' column into separate columns (25 columns)\n",
    "    dataframe_name = dataframe_name.join(dataframe_name['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "\n",
    "    # Drop the original 'Numbers' column\n",
    "    dataframe_name = dataframe_name.drop(columns=['Numbers'])\n",
    "\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Extract rows from the start until the first empty row\n",
    "    statcast = dataframe_name.iloc[:first_empty_idx]\n",
    "\n",
    "    # Drop the first column\n",
    "    statcast = statcast.drop(columns=[0])\n",
    "\n",
    "    # Swap last name and first name\n",
    "    statcast['Name'] = statcast['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "    # Add the headers to the dataframe\n",
    "    statcast.columns = headers_statcast.columns\n",
    "\n",
    "    #! PLATE DISCIPLINE TABLE\n",
    "    #! Cleaning the data for the plate discipline table\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Remove rows from the first row until the first empty row\n",
    "    dataframe_name = dataframe_name.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove the first row\n",
    "    dataframe_name = dataframe_name.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "    # Join rows\n",
    "    dataframe_name = combine_rows(dataframe_name, 4, 5)\n",
    "    dataframe_name = combine_rows(dataframe_name, 5, 6)\n",
    "    dataframe_name = combine_rows(dataframe_name, 7, 8)\n",
    "    dataframe_name = combine_rows(dataframe_name, 9, 10)\n",
    "    dataframe_name = combine_rows(dataframe_name, 13, 14)\n",
    "\n",
    "    #! Create the headers for the second table (plate discipline)\n",
    "    # Find the first occurrence of 'Player' and 'Meatball Swing %'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Player'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'Meatball Swing %'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_plate_discipline = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_plate_discipline.columns = headers_plate_discipline.iloc[0]\n",
    "    headers_plate_discipline         = headers_plate_discipline[1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Extract rows from the start until the first empty row\n",
    "    plate_discipline = dataframe_name.iloc[:first_empty_idx]\n",
    "\n",
    "    # Drop the first column\n",
    "    plate_discipline = plate_discipline.drop(columns=[0])\n",
    "\n",
    "    # Swap last name and first name\n",
    "    plate_discipline['Name'] = plate_discipline['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "    # Drop columns where all values are NaN (empty)\n",
    "    plate_discipline = plate_discipline.dropna(axis= 1, how= 'all')\n",
    "\n",
    "    # Add the headers to the dataframe\n",
    "    plate_discipline.columns = headers_plate_discipline.columns\n",
    "\n",
    "    #! BATTED BALL PROFILE TABLE\n",
    "    #! Cleaning the data for the batted ball profile table\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Remove rows from the first row until the first empty row\n",
    "    dataframe_name = dataframe_name.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove the first row\n",
    "    dataframe_name = dataframe_name.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "    # Join rows\n",
    "    dataframe_name = combine_rows(dataframe_name, 15, 16)\n",
    "\n",
    "    #! Create the headers for the third table (batted ball profile)\n",
    "    # Find the first occurrence of 'Player' and 'Barrel %'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Player'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'Barrel %'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_batted_ball_profile = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_batted_ball_profile.columns = headers_batted_ball_profile.iloc[0]\n",
    "    headers_batted_ball_profile         = headers_batted_ball_profile[1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    if pd.isna(first_empty_idx):\n",
    "        batted_ball_profile = dataframe_name.copy()  # If no empty row, use the entire DataFrame\n",
    "    else:\n",
    "        # Extract rows from the start until the first empty row\n",
    "        batted_ball_profile = dataframe_name.iloc[:first_empty_idx]\n",
    "\n",
    "    # Drop the first column\n",
    "    batted_ball_profile = batted_ball_profile.drop(columns=[0])\n",
    "\n",
    "    # Swap last name and first name\n",
    "    batted_ball_profile['Name'] = batted_ball_profile['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "    # Drop columns where all values are NaN (empty)\n",
    "    batted_ball_profile = batted_ball_profile.dropna(axis= 1, how= 'all')\n",
    "\n",
    "    # Add the headers to the dataframe\n",
    "    batted_ball_profile.columns = headers_batted_ball_profile.columns\n",
    "    \n",
    "    \n",
    "    def remove_duplicates(df):\n",
    "        \"\"\"\n",
    "        Removes duplicate rows from a given DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to process.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A new DataFrame without duplicate rows.\n",
    "        \"\"\"\n",
    "        return df.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    # Remove duplicated rows from the dataframes\n",
    "    statcast_cleaned            = remove_duplicates(statcast)    \n",
    "    plate_discipline_cleaned    = remove_duplicates(plate_discipline)    \n",
    "    batted_ball_profile_cleaned = remove_duplicates(batted_ball_profile)    \n",
    "    \n",
    "    return statcast_cleaned, plate_discipline_cleaned, batted_ball_profile_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c50a2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gamelogs(dataframe_name):        \n",
    "    #! Create the headers for the gamelogs\n",
    "    # Find the first occurrence of 'Game Date' and 'Hard Hit %'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Game Date'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'Hard Hit %'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_gamelogs = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_gamelogs.columns = headers_gamelogs.iloc[0]\n",
    "    headers_gamelogs         = headers_gamelogs[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "    \n",
    "    # Use regex to split the column while preserving negative numbers\n",
    "    dataframe_name[['Name', 'Numbers']] = dataframe_name[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "        \n",
    "    # Split the 'Numbers' column into separate columns (25 columns)\n",
    "    dataframe_name = dataframe_name.join(dataframe_name['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "    \n",
    "    # Drop the first column\n",
    "    dataframe_name = dataframe_name.drop(columns=[0])\n",
    "    \n",
    "    # Drop the original 'Numbers' column\n",
    "    dataframe_name = dataframe_name.drop(columns=['Numbers'])\n",
    "    \n",
    "    # Remove the first 2 rows\n",
    "    dataframe_name = dataframe_name.iloc[2:].reset_index(drop= True)  # Using iloc\n",
    "    \n",
    "    # # Split the 'Name' column into 'Game Date' and 'Opponent'\n",
    "    # dataframe_name[['Game Date', 'Opponent']] = dataframe_name['Name'].str.split(\" \", n=2, expand=True)\n",
    "    \n",
    "    # Create a copy of the dataframe to store the gamelogs\n",
    "    gamelogs = dataframe_name.copy()\n",
    "    \n",
    "    # Split the first column into two\n",
    "    gamelogs[['Game Date', 'Opponent']] = gamelogs['Name'].str.split(\" \", n=1, expand=True)\n",
    "    \n",
    "    # Drop the first column\n",
    "    gamelogs = gamelogs.drop(columns= ['Name'])\n",
    "    \n",
    "    # Get the last two column names\n",
    "    last_two_cols  = gamelogs.columns[-2:].tolist()\n",
    "    remaining_cols = gamelogs.columns[:-2].tolist()\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    gamelogs = gamelogs[last_two_cols + remaining_cols]\n",
    "    \n",
    "    # Add the headers to the dataframe\n",
    "    gamelogs.columns = headers_gamelogs.columns\n",
    "\n",
    "    return gamelogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbed8c9",
   "metadata": {},
   "source": [
    "### Apply a function to the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a001f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! STATCAST TABLES\n",
    "# Initialize dictionaries to store the results for each team\n",
    "statcast_data_pitching            = {}\n",
    "plate_discipline_data_pitching    = {}\n",
    "batted_ball_profile_data_pitching = {}\n",
    "\n",
    "statcast_data_hitting             = {}\n",
    "plate_discipline_data_hitting     = {}\n",
    "batted_ball_profile_data_hitting  = {}\n",
    "\n",
    "#? Pitching\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in data_pitching.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    statcast, plate_discipline, batted_ball_profile = team_advanced_stats(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    statcast_data_pitching[team_id]            = statcast\n",
    "    plate_discipline_data_pitching[team_id]    = plate_discipline\n",
    "    batted_ball_profile_data_pitching[team_id] = batted_ball_profile\n",
    "\n",
    "#? Hitting\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in data_hitting.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    statcast, plate_discipline, batted_ball_profile = team_advanced_stats(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    statcast_data_hitting[team_id]            = statcast\n",
    "    plate_discipline_data_hitting[team_id]    = plate_discipline\n",
    "    batted_ball_profile_data_hitting[team_id] = batted_ball_profile\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ce2bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! GAMELOGS\n",
    "# Initialize dictionaries to store the results for each team\n",
    "gamelog_pitching = {}\n",
    "gamelog_hitting  = {}\n",
    "\n",
    "#? Pitching\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in gamelog_data_pitching.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    gamelogs = process_gamelogs(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    gamelog_pitching[team_id] = gamelogs\n",
    "\n",
    "#? Hitting\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in gamelog_data_hitting.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    gamelogs = process_gamelogs(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    gamelog_hitting[team_id] = gamelogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22d58658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_files(data_dict, suffix):\n",
    "    \"\"\"Create Parquet files from a dictionary of DataFrames, ensuring uniform column names.\"\"\"\n",
    "    \n",
    "    df_store = {}\n",
    "\n",
    "    for key, df in data_dict.items():\n",
    "        new_name = f\"{key}{suffix}\"  # Assigning a suffix\n",
    "        df_copy = df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "        df_copy[\"ID\"] = key  # Add the original dictionary key as a column\n",
    "\n",
    "        # Convert all column names to strings\n",
    "        df_copy.columns = df_copy.columns.astype(str)\n",
    "\n",
    "        df_store[new_name] = df_copy  # Store the modified DataFrame\n",
    "\n",
    "    # for name, df in df_store.items():\n",
    "    #     # Save the DataFrame as a Parquet file\n",
    "    #     df.to_parquet(f\"D:\\\\mlb_analyzer\\\\output\\\\teams\\\\{suffix}\\\\{name}.parquet\")\n",
    "\n",
    "def merge_dataframes(data_dict):\n",
    "    \"\"\"Append all DataFrames in a dictionary into a single DataFrame, preserving IDs.\"\"\"\n",
    "    \n",
    "    # Add an 'ID' column to track each DataFrame's origin\n",
    "    for name, df in data_dict.items():\n",
    "        df[\"ID\"] = name  # Append the key as a column\n",
    "    \n",
    "    # Concatenate all DataFrames into one\n",
    "    merged_df = pd.concat(data_dict.values(), ignore_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Call the function and export the DataFrames with an ID column\n",
    "create_parquet_files(statcast_data_hitting, \"_hitting_statcast\")\n",
    "create_parquet_files(plate_discipline_data_hitting, \"_hitting_plate_discipline\")\n",
    "create_parquet_files(batted_ball_profile_data_hitting, \"_hitting_batted_ball_profile\")\n",
    "\n",
    "create_parquet_files(statcast_data_pitching, \"_pitching_statcast\")\n",
    "create_parquet_files(plate_discipline_data_pitching, \"_pitching_plate_discipline\")\n",
    "create_parquet_files(batted_ball_profile_data_pitching, \"_pitching_batted_ball_profile\")\n",
    "\n",
    "create_parquet_files(gamelog_hitting, \"_gamelogs_hitting\")\n",
    "create_parquet_files(gamelog_pitching, \"_gamelogs_pitching\")\n",
    "\n",
    "# Append all DataFrames into a single DataFrame\n",
    "statcast_hitting_df = merge_dataframes(statcast_data_hitting)\n",
    "plate_discipline_hitting_df = merge_dataframes(plate_discipline_data_hitting)   \n",
    "batted_ball_profile_hitting_df = merge_dataframes(batted_ball_profile_data_hitting)\n",
    "\n",
    "statcast_pitching_df = merge_dataframes(statcast_data_pitching)\n",
    "plate_discipline_pitching_df = merge_dataframes(plate_discipline_data_pitching)\n",
    "batted_ball_profile_pitching_df = merge_dataframes(batted_ball_profile_data_pitching)\n",
    "\n",
    "gamelog_hitting_df = merge_dataframes(gamelog_hitting)\n",
    "gamelog_pitching_df = merge_dataframes(gamelog_pitching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a523f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a column with type of analysis\n",
    "statcast_hitting_df['type']  = 'hitting'\n",
    "statcast_pitching_df['type'] = 'pitching'\n",
    "\n",
    "plate_discipline_hitting_df['type']  = 'hitting'\n",
    "plate_discipline_pitching_df['type'] = 'pitching'\n",
    "\n",
    "batted_ball_profile_hitting_df['type']  = 'hitting'\n",
    "batted_ball_profile_pitching_df['type'] = 'pitching'\n",
    "\n",
    "gamelog_hitting_df['type']  = 'hitting'\n",
    "gamelog_pitching_df['type'] = 'pitching'\n",
    "\n",
    "# Append statcast DataFrames\n",
    "statcast_df = pd.concat([statcast_hitting_df, statcast_pitching_df], ignore_index=True)\n",
    "statcast_df['analysis_type'] = 'statcast'\n",
    "\n",
    "# Append plate discipline DataFrames\n",
    "plate_discipline_df = pd.concat([plate_discipline_hitting_df, plate_discipline_pitching_df], ignore_index=True)\n",
    "plate_discipline_df['analysis_type'] = 'plate_discipline'\n",
    "\n",
    "# Append batted ball profile DataFrames\n",
    "batted_ball_profile_df = pd.concat([batted_ball_profile_hitting_df, batted_ball_profile_pitching_df], ignore_index=True)\n",
    "batted_ball_profile_df['analysis_type'] = 'batted_ball_profile'\n",
    "\n",
    "# Append gamelog DataFrames\n",
    "gamelog_df = pd.concat([gamelog_hitting_df, gamelog_pitching_df], ignore_index=True)\n",
    "gamelog_df['analysis_type'] = 'gamelog'\n",
    "\n",
    "# Export the merged DataFrames to Parquet files\n",
    "output_path = \"D:\\\\mlb_analyzer\\\\output\\\\teams\\\\\"\n",
    "\n",
    "statcast_df.to_parquet(f\"{output_path}statcast.parquet\")\n",
    "plate_discipline_df.to_parquet(f\"{output_path}plate_discipline.parquet\")\n",
    "batted_ball_profile_df.to_parquet(f\"{output_path}batted_ball_profile.parquet\")\n",
    "gamelog_df.to_parquet(f\"{output_path}gamelog.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3921f",
   "metadata": {},
   "source": [
    "### Add Splits by team https://www.baseball-reference.com/leagues/split.cgi?t=b&lg=MLB&year=2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba80d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams_split(split_type, clean_mode):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Define year\n",
    "    year = datetime.now().year\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)  \n",
    "    \n",
    "    if split_type == 'LHP' or split_type == 'RHP': # for LHP and RHP pitchers\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == '7' or split_type == '14' or split_type == '28': # for the last 7, 14 and 28 days\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=total%7CLast%20{split_type}%20days%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'RH' or split_type == 'LH': # for RH and LH Starters\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=plato%7Cvs%20{split_type}%20Starter%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'Home' or split_type == 'Away': # for home and away games\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=hmvis%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'first_batter_game':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=leado%7C1st%20Batter%20G%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_power_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Power%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_weak_pitcher':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=power%7Cvs.%20Finesse%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    # For each team:\n",
    "    elif split_type == 'ANA' or split_type == 'ARI' or split_type == 'ATL' or split_type == 'BAL' or split_type == 'BOS' \\\n",
    "        or split_type == 'CHC' or split_type == 'CHW' or split_type == 'CIN' or split_type == 'CLE' or split_type == 'COL' \\\n",
    "        or split_type == 'DET' or split_type == 'HOU' or split_type == 'KCR' or split_type == 'LAD' or split_type == 'FLA' \\\n",
    "        or split_type == 'MIL' or split_type == 'MIN' or split_type == 'NYM' or split_type == 'NYY' or split_type == 'OAK' \\\n",
    "        or split_type == 'PHI' or split_type == 'PIT' or split_type == 'SDP' or split_type == 'SEA' or split_type == 'SFG' \\\n",
    "        or split_type == 'STL' or split_type == 'TBD' or split_type == 'TEX' or split_type == 'TOR' or split_type == 'WSN':\n",
    "            driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7C{split_type}%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_less_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3C%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    elif split_type == 'vs_greater_or_equal_than_500_WP':\n",
    "        driver.get(f\"https://www.baseball-reference.com/tools/split_stats_lg.cgi?full=1&params=oppon%7CWP%20%3E%3D%20.500%7CML%7C{year}%7Cbat%7CAB%7C\")\n",
    "    \n",
    "    \n",
    "    # Name of the table\n",
    "    datatable_id = 'split1'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} ({split_type}) table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()    \n",
    "    \n",
    "    if clean_mode == 1:\n",
    "        # Remove 'Roe' exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "\n",
    "        # Rename last 3 columns\n",
    "        new_column_names = [\"BAbip\", \"tOPS+\", \"sOPS+\"]  # New names for last 3 columns\n",
    "        df.columns.values[-3:] = new_column_names  # Assign new names\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "    else:\n",
    "        # Remove 'Roe' and GS exactly (case-sensitive)\n",
    "        df[0] = df[0].str.replace('Roe', '', regex=False)\n",
    "        df[0] = df[0].str.replace('GS', '', regex=False)\n",
    "\n",
    "        # Remove last row\n",
    "        df = df.iloc[:-1]\n",
    "\n",
    "        # Remove rows where column 'A' contains 'Rk', but keep the first row\n",
    "        df = df[~((df.index > 0) & (df[0].str.contains('Rk', na=False)))]\n",
    "\n",
    "        # Split column from right using spaces\n",
    "        df = df[0].str.split(\" \", n= 30, expand=True)\n",
    "\n",
    "        # Set first row as header\n",
    "        df.columns = df.iloc[0]  # Assign first row as column names\n",
    "        df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "        # Remove the first column\n",
    "        df = df.iloc[:, 1:]\n",
    "\n",
    "        # Remove the last 2 columns\n",
    "        df = df.iloc[:, :-2]\n",
    "\n",
    "        # New column names\n",
    "        new_column_names = ['Team', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'RBI', 'SB',\n",
    "                            'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS', 'TB', 'GDP', 'HBP', 'SH',\n",
    "                            'SF', 'IBB', 'ROE', 'BAbip', 'tOPS+', 'sOPS+']\n",
    "\n",
    "        # Rename all columns\n",
    "        df.columns = new_column_names\n",
    "\n",
    "    return df\n",
    "\n",
    "# Call the function to get the teams split data\n",
    "team_vs_lhp             = teams_split(split_type= 'LHP',  clean_mode= 0) # GS empty\n",
    "team_vs_rhp             = teams_split(split_type= 'RHP',  clean_mode= 0) # GS empty\n",
    "team_vs_lh_starters     = teams_split(split_type= 'LH',   clean_mode= 1)\n",
    "team_vs_rh_starters     = teams_split(split_type= 'RH',   clean_mode= 1)\n",
    "team_last_seven_days    = teams_split(split_type= '7',    clean_mode= 1)\n",
    "team_last_fourteen_days = teams_split(split_type= '14',   clean_mode= 1)\n",
    "team_last_28_days       = teams_split(split_type= '28',   clean_mode= 1)\n",
    "team_home_games         = teams_split(split_type= 'Home', clean_mode= 1)\n",
    "team_away_games         = teams_split(split_type= 'Away', clean_mode= 1)\n",
    "team_first_batter_game  = teams_split(split_type= 'first_batter_game', clean_mode= 0) # GS empty\n",
    "team_vs_power_pitcher   = teams_split(split_type= 'vs_power_pitcher',  clean_mode= 0) # GS empty\n",
    "team_vs_weak_pitcher    = teams_split(split_type= 'vs_weak_pitcher',   clean_mode= 0) # GS empty\n",
    "team_vs_power_team      = teams_split(split_type= 'vs_greater_or_equal_than_500_WP', clean_mode= 1)\n",
    "team_vs_weak_team       = teams_split(split_type= 'vs_less_than_500_WP',             clean_mode= 1)\n",
    "\n",
    "# # Direct matchups\n",
    "team_laa = teams_split(split_type= 'ANA', clean_mode= 1)\n",
    "team_ari = teams_split(split_type= 'ARI', clean_mode= 1)\n",
    "team_atl = teams_split(split_type= 'ATL', clean_mode= 1)\n",
    "team_bal = teams_split(split_type= 'BAL', clean_mode= 1)\n",
    "team_bos = teams_split(split_type= 'BOS', clean_mode= 1)\n",
    "team_chc = teams_split(split_type= 'CHC', clean_mode= 1)\n",
    "team_chw = teams_split(split_type= 'CHW', clean_mode= 1)\n",
    "team_cin = teams_split(split_type= 'CIN', clean_mode= 1)\n",
    "team_cle = teams_split(split_type= 'CLE', clean_mode= 1)\n",
    "team_col = teams_split(split_type= 'COL', clean_mode= 1)\n",
    "team_det = teams_split(split_type= 'DET', clean_mode= 1)\n",
    "team_hou = teams_split(split_type= 'HOU', clean_mode= 1)\n",
    "team_kcr = teams_split(split_type= 'KCR', clean_mode= 1)\n",
    "team_lad = teams_split(split_type= 'LAD', clean_mode= 1)\n",
    "team_mia = teams_split(split_type= 'FLA', clean_mode= 1) \n",
    "team_mil = teams_split(split_type= 'MIL', clean_mode= 1)\n",
    "team_min = teams_split(split_type= 'MIN', clean_mode= 1)\n",
    "team_nym = teams_split(split_type= 'NYM', clean_mode= 1)\n",
    "team_nyy = teams_split(split_type= 'NYY', clean_mode= 1)\n",
    "team_oak = teams_split(split_type= 'OAK', clean_mode= 1)\n",
    "team_phi = teams_split(split_type= 'PHI', clean_mode= 1)\n",
    "team_pit = teams_split(split_type= 'PIT', clean_mode= 1)\n",
    "team_sdp = teams_split(split_type= 'SDP', clean_mode= 1)\n",
    "team_sea = teams_split(split_type= 'SEA', clean_mode= 1)\n",
    "team_sfg = teams_split(split_type= 'SFG', clean_mode= 1)\n",
    "team_stl = teams_split(split_type= 'STL', clean_mode= 1)\n",
    "team_tbr = teams_split(split_type= 'TBD', clean_mode= 1)\n",
    "team_tex = teams_split(split_type= 'TEX', clean_mode= 1)\n",
    "team_tor = teams_split(split_type= 'TOR', clean_mode= 1)\n",
    "team_wsn = teams_split(split_type= 'WSN', clean_mode= 1)\n",
    "\n",
    "# Dictionary of dataframes for the teams\n",
    "dic_team = {\n",
    "    'LAA': team_laa,\n",
    "    'AZ': team_ari,\n",
    "    'ATL': team_atl,\n",
    "    'BAL': team_bal,\n",
    "    'BOS': team_bos,\n",
    "    'CHC': team_chc,\n",
    "    'CHW': team_chw,\n",
    "    'CIN': team_cin,\n",
    "    'CLE': team_cle,\n",
    "    'COL': team_col,\n",
    "    'DET': team_det,\n",
    "    'HOU': team_hou,\n",
    "    'KC': team_kcr,\n",
    "    'LAD': team_lad,\n",
    "    'MIA': team_mia,\n",
    "    'MIL': team_mil,\n",
    "    'MIN': team_min,\n",
    "    'NYM': team_nym,\n",
    "    'NYY': team_nyy,\n",
    "    'ATH': team_oak,\n",
    "    'PHI': team_phi,\n",
    "    'PIT': team_pit,\n",
    "    'SD': team_sdp,\n",
    "    'SEA': team_sea,\n",
    "    'SF': team_sfg,\n",
    "    'STL': team_stl,\n",
    "    'TB': team_tbr,\n",
    "    'TEX': team_tex,\n",
    "    'TOR': team_tor,\n",
    "    'WSH': team_wsn   \n",
    "    }\n",
    "\n",
    "# Add an ID column with the dictionary key as the identifier\n",
    "for key, df in dic_team.items():\n",
    "    df['ID'] = key  # Assign the dictionary key as the ID\n",
    "\n",
    "# Concatenate all dataFrames in the dictionary\n",
    "direct_matches = pd.concat(dic_team.values(), ignore_index=True)  # Resets index\n",
    "\n",
    "dic_splits = {\n",
    "    'team_vs_lhp'        :team_vs_lhp,        \n",
    "    'team_vs_rhp'        :team_vs_rhp,\n",
    "    'team_vs_lh_starters':team_vs_lh_starters,\n",
    "    'team_vs_rh_starters':team_vs_rh_starters,\n",
    "    'team_last_seven_days':team_last_seven_days,\n",
    "    'team_last_fourteen_days':team_last_fourteen_days,\n",
    "    'team_last_28_days':team_last_28_days,\n",
    "    'team_home_games':team_home_games,\n",
    "    'team_away_games':team_away_games,\n",
    "    'team_first_batter_game':team_first_batter_game,\n",
    "    'team_vs_power_pitcher':team_vs_power_pitcher,\n",
    "    'team_vs_weak_pitcher':team_vs_weak_pitcher,\n",
    "    'team_vs_power_team':team_vs_power_team,\n",
    "    'team_vs_weak_team':team_vs_weak_team      \n",
    "}\n",
    "#! This works only for .py files.\n",
    "# # Get the current working directory and create the path for the 'output' folder\n",
    "# output_folder = os.path.join(os.path.dirname(__file__), 'output')\n",
    "\n",
    "# Get the current working directory and create the path for the 'output' folder\n",
    "# output_folder = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "output_folder_teams           = ('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\direct_matches\\\\')\n",
    "output_folder_splits          = ('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\splits\\\\')\n",
    "output_folder_individual_team = ('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\team\\\\')\n",
    "\n",
    "# Save each DataFrame in the 'output' folder\n",
    "# For direct matches\n",
    "direct_matches.to_csv(os.path.join(output_folder_teams, 'direct_matches.csv'), index=False)\n",
    "\n",
    "# For Splits\n",
    "for name, dataframe in dic_splits.items():\n",
    "    dataframe.to_csv(os.path.join(output_folder_splits, f'{name}.csv'), index=False)\n",
    "    \n",
    "# For Teams\n",
    "for name, dataframe in dic_team.items():\n",
    "    dataframe.to_csv(os.path.join(output_folder_individual_team, f'{name}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536476f",
   "metadata": {},
   "source": [
    "### Import Standings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1798cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_standings():\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.baseball-reference.com/leagues/MLB-standings.shtml\")\n",
    "\n",
    "    datatable_id = 'expanded_standings_overall'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//table[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    #? Clean the extraction\n",
    "    # Remove last row\n",
    "    df = df.iloc[:-1]\n",
    "\n",
    "    # Split column from right using spaces\n",
    "    df = df[0].str.rsplit(\" \", n= 27, expand=True)\n",
    "\n",
    "    # Use regex to separate the leading number from the team name\n",
    "    df[['Number', 'Team_Name']] = df[0].str.extract(r'(\\d+)\\s(.+)')\n",
    "\n",
    "    # Remove the first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Remove rows where column '0 contains 'Rk', but keep the first row\n",
    "    df = df[~((df.index > 0) & (df[1].str.contains('Tm', na=False)))]\n",
    "\n",
    "    # Set first row as header\n",
    "    df.columns = df.iloc[0]  # Assign first row as column names\n",
    "    df = df[1:].reset_index(drop=True)  # Remove first row and reset index\n",
    "\n",
    "    # New column names\n",
    "    new_column_names = ['W', 'L', 'W-L%', 'StrkWL', 'StrkNb', 'R', 'RA', 'Rdiff', 'SOS', 'SRS', 'pythWL', 'Luck',\n",
    "                        'vEast', 'vCent', 'vWest', 'Inter', 'Home', 'Road', 'ExInn', '1Run', 'vRHP', 'vLHP', '≥.500',\n",
    "                        '<.500', 'last10', 'last20', 'last30', 'Rk', 'Team']\n",
    "\n",
    "    # Rename all columns\n",
    "    df.columns = new_column_names\n",
    "\n",
    "    # Drop the column named 'Rk'\n",
    "    df = df.drop(columns=['Rk'])\n",
    "\n",
    "    # Step 2: Move the last column to the first position\n",
    "    cols = df.columns.tolist()\n",
    "    df = df[[cols[-1]] + cols[:-1]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Call the function\n",
    "standings_df = import_standings()\n",
    "\n",
    "# Export the table\n",
    "output_folder_teams  = ('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\')\n",
    "standings_df.to_csv(os.path.join(output_folder_teams, 'standings.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001f869",
   "metadata": {},
   "source": [
    "### Import Odds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
