{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e133bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from rapidfuzz import process\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36493d1f",
   "metadata": {},
   "source": [
    "### Rotowire.com scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams_matchups(game_date):\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.rotowire.com/baseball/scoreboard.php?date={game_date}\")\n",
    "\n",
    "    datatable_id = 'grid-noGutter mb-15'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Find indices for where \"Final\" and \"View Box Score\" appear\n",
    "    start_indices = df[df[0].str.contains(\"Final\", case=False)].index\n",
    "    end_indices = df[df[0].str.contains(\"View Box Score\", case=False)].index\n",
    "\n",
    "    # Extract and split data\n",
    "    game_dataframes = []\n",
    "    for start in start_indices:\n",
    "        # Find the corresponding end index that's greater than the start index\n",
    "        end = end_indices[end_indices > start].min()\n",
    "        if pd.notna(end):  # Ensure there's a valid end index\n",
    "            game_data = df.iloc[start:end+1]  # Capture all rows in between\n",
    "            # Split every 13 rows and create DataFrame\n",
    "            reshaped_data = [game_data.iloc[i:i+13] for i in range(0, len(game_data), 13)]\n",
    "            game_dataframes.extend(reshaped_data)\n",
    "\n",
    "    # Final dataframe with all games\n",
    "    final_df = pd.concat(game_dataframes, ignore_index= True)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array for reshaping\n",
    "    data = final_df.values  \n",
    "\n",
    "    # Reshape: each group of 13 rows becomes one row with 13 columns\n",
    "    reshaped_data = [data[i:i+13].flatten() for i in range(0, len(data), 13)]\n",
    "\n",
    "    # Convert reshaped data back to a DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Drop the first 4 columns and last column\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.columns[[0, 1, 2, 3, -1]], axis= 1)\n",
    "\n",
    "    # Add the headers\n",
    "    reshaped_df.columns = ['Away', 'Home', 'R_Away', 'H_Away', 'E_Away', 'R_Home', 'H_Home', 'E_Home']\n",
    "\n",
    "    # Add the date\n",
    "    reshaped_df['date'] = game_date\n",
    "\n",
    "    # Calculate the winner, loser and the difference in runs, hits and errors\n",
    "    reshaped_df['winner']    = reshaped_df.apply(lambda x: x['Away'] if int(x['R_Away']) > int(x['R_Home']) else x['Home'], axis= 1)\n",
    "    reshaped_df['loser']     = reshaped_df.apply(lambda x: x['Away'] if int(x['R_Away']) < int(x['R_Home']) else x['Home'], axis= 1)\n",
    "    reshaped_df['diff_runs_away_vs_home_team']   = reshaped_df.apply(lambda x: int(x['R_Away']) - int(x['R_Home']), axis= 1)\n",
    "    reshaped_df['diff_hits_away_vs_home_team']   = reshaped_df.apply(lambda x: int(x['H_Away']) - int(x['H_Home']), axis= 1)\n",
    "    reshaped_df['diff_errors_away_vs_home_team'] = reshaped_df.apply(lambda x: int(x['E_Away']) - int(x['E_Home']), axis= 1)\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Define dates\n",
    "dates = [\n",
    "    (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=4)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d'),\n",
    "    (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    ]\n",
    "\n",
    "# Initialize a dictionary to store dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each date and store results in a unique dataframe\n",
    "for game_date in dates:\n",
    "    print(f\"Processing data for date: {game_date}\")\n",
    "    \n",
    "    # Run the teams_matchups function and save the resulting dataframe\n",
    "    result_df = teams_matchups(game_date)\n",
    "    \n",
    "    # Store the dataframe in the dictionary with the date as the key\n",
    "    dataframes[game_date] = result_df\n",
    "\n",
    "# Create the final dataframe\n",
    "games = pd.concat(dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Convert to datetime format\n",
    "games[\"date\"] = pd.to_datetime(games[\"date\"])\n",
    "\n",
    "# Create a game_id column using the index\n",
    "games[\"game_id\"] = games.index + 1\n",
    "games[\"game_id\"] = games[\"game_id\"].astype(str)\n",
    "\n",
    "# Create a key column for the game\n",
    "games[\"key\"] = games[\"date\"].dt.strftime(\"%Y%m%d\") + \"_\" + games[\"game_id\"]\n",
    "\n",
    "# Know if the winner was the away or home team\n",
    "games[\"visitor_won\"]           = games.apply(lambda x: 1 if int(x[\"R_Away\"]) > int(x[\"R_Home\"]) else 0, axis=1)\n",
    "games[\"home_won\"]              = games.apply(lambda x: 1 if int(x[\"R_Away\"]) < int(x[\"R_Home\"]) else 0, axis=1) \n",
    "games[\"visit_or_home_victory\"] = games.apply(lambda x: 'H' if int(x[\"R_Away\"]) < int(x[\"R_Home\"]) else 'V', axis=1) \n",
    "\n",
    "# Create a column that indicates if the game was a shutout\n",
    "games[\"shutout\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) == 0 or int(x[\"R_Home\"]) == 0 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a one-run game\n",
    "games[\"one_run_game\"] = games.apply(lambda x: 1 if abs(int(x[\"R_Away\"]) - int(x[\"R_Home\"])) == 1 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a high-scoring game\n",
    "games[\"high_scoring_game\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) + int(x[\"R_Home\"]) >= 10 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a low-scoring game\n",
    "games[\"low_scoring_game\"] = games.apply(lambda x: 1 if int(x[\"R_Away\"]) + int(x[\"R_Home\"]) <= 3 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates if the game was a blowout\n",
    "games[\"blowout\"] = games.apply(lambda x: 1 if abs(int(x[\"R_Away\"]) - int(x[\"R_Home\"])) >= 5 else 0, axis=1)\n",
    "\n",
    "# Create a column that indicates how many runs were scored in the game\n",
    "games[\"total_runs\"] = games.apply(lambda x: int(x[\"R_Away\"]) + int(x[\"R_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that indicates how many hits were scored in the game\n",
    "games[\"total_hits\"] = games.apply(lambda x: int(x[\"H_Away\"]) + int(x[\"H_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that indicates how many errors were scored in the game\n",
    "games[\"total_errors\"] = games.apply(lambda x: int(x[\"E_Away\"]) + int(x[\"E_Home\"]), axis=1)\n",
    "\n",
    "# Create a column that join the home and away teams\n",
    "games[\"teams\"] = games.apply(lambda x: x[\"Away\"] + \" vs \" + x[\"Home\"], axis=1)\n",
    "\n",
    "# Count occurrences of each team matchup in the 'teams' column\n",
    "team_counts = games['teams'].value_counts()\n",
    "\n",
    "# Map the counts back to the original dataFrame\n",
    "games['team_matchup_count'] = games['teams'].map(team_counts)\n",
    "\n",
    "# Create a group id for each team matchup\n",
    "games['series_id'] = games.groupby('teams').ngroup() + 1\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "games.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\teams_matchup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384c986",
   "metadata": {},
   "source": [
    "## Basic pitcher information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def games_today():\n",
    "    \"\"\"Get today's matchups from the RotoWire scoreboard.\n",
    "    This function uses Selenium to scrape the RotoWire website for today's MLB matchups.\n",
    "    It extracts the game time, away team, home team, away pitcher name, away pitcher record,\n",
    "    home pitcher name, and home pitcher record from the scoreboard.\n",
    "    The data is then reshaped into a DataFrame format for further analysis.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://www.rotowire.com/baseball/scoreboard.php?date={today}\")\n",
    "\n",
    "    datatable_id = 'grid-noGutter mb-15'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Find indices for where \"Final\" and \"View Box Score\" appear\n",
    "    start_indices = df[df[0].str.contains(\" ET\", case= False)].index\n",
    "    end_indices = df[df[0].str.contains(\"View Box Score\", case=False)].index\n",
    "\n",
    "    # Extract and split data\n",
    "    game_dataframes = []\n",
    "    for start in start_indices:\n",
    "        # Find the corresponding end index that's greater than the start index\n",
    "        end = end_indices[end_indices > start].min()\n",
    "        if pd.notna(end):  # Ensure there's a valid end index\n",
    "            game_data = df.iloc[start:end+1]  # Capture all rows in between\n",
    "            # Split every 8 rows and create DataFrame\n",
    "            reshaped_data = [game_data.iloc[i:i+8] for i in range(0, len(game_data), 8)]\n",
    "            game_dataframes.extend(reshaped_data)\n",
    "\n",
    "    # Final dataframe with all games\n",
    "    final_df = pd.concat(game_dataframes, ignore_index= True)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array for reshaping\n",
    "    data = final_df.values  \n",
    "\n",
    "    # Reshape: each group of 8 rows becomes one row with 8 columns\n",
    "    reshaped_data = [data[i:i+8].flatten() for i in range(0, len(data), 8)]\n",
    "\n",
    "    # Convert reshaped data back to a DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    # Drop the last column\n",
    "    reshaped_df = reshaped_df.drop(reshaped_df.columns[[-1]], axis= 1)\n",
    "\n",
    "    # Add the headers\n",
    "    reshaped_df.columns = ['game_time', 'away_team', 'home_team', 'away_pitcher_name', 'away_pitcher_record', 'home_pitcher_name', 'home_pitcher_record']\n",
    "\n",
    "    # Add the date\n",
    "    reshaped_df['date'] = today\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Get today's matchups\n",
    "games_today_df = games_today()\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "games_today_df[\"date\"] = pd.to_datetime(games_today_df[\"date\"])\n",
    "\n",
    "# Create a game_id column using the index\n",
    "games_today_df[\"game_id\"] = games_today_df.index + 1\n",
    "games_today_df[\"game_id\"] = games_today_df[\"game_id\"].astype(str)\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "games_today_df.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\games_today.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec890c",
   "metadata": {},
   "source": [
    "## Advanced Matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_matchups():\n",
    "    # Load the options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome(options= options)    \n",
    "    driver.get(f\"https://baseballsavant.mlb.com/probable-pitchers\")\n",
    "\n",
    "    datatable_id = 'template__content template--two-column__content--one'\n",
    "\n",
    "    # Explicitly wait for the table element to load\n",
    "    datatable_xpath = f\"//div[@class='{datatable_id}']\"  # Update XPATH as needed\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "        )\n",
    "        print(f\"{datatable_id} table loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Table {datatable_id} did not load. Details: {e}\")\n",
    "        driver.quit()\n",
    "\n",
    "    # Wait for the load of the page\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Locate the table\n",
    "    table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "    text_content = table_element.text\n",
    "\n",
    "    # Process the table content\n",
    "    rows = text_content.split(\"\\n\")\n",
    "    table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Identify rows that contain \" @ \"\n",
    "    split_indices = df[df[0].str.contains(\" @ \")].index.tolist()\n",
    "\n",
    "    # Add start and end indices\n",
    "    split_indices.append(len(df))\n",
    "    split_data = [df.iloc[split_indices[i]:split_indices[i+1]].values.flatten().tolist()\n",
    "                    for i in range(len(split_indices)-1)]\n",
    "\n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame(split_data)\n",
    "\n",
    "    # Splitting on the \" | \" symbol. Splitting only col 2 while keeping other columns\n",
    "    df_expanded = new_df.copy()  # Preserve other columns\n",
    "    df_expanded[['Column1', 'Column2']] = df_expanded[2].str.split(\"ET\", expand=True)\n",
    "\n",
    "    # Drop original column\n",
    "    df_expanded = df_expanded.drop(columns=[2, 3])\n",
    "\n",
    "    #! Removing rows where col 4 contains 'to be announced'\n",
    "    df_filtered = df_expanded[df_expanded[4] != \"To be announced.\"]\n",
    "\n",
    "    # Filtering rows where col6 contains \"Never Faced Any Players on this Team.\"\n",
    "    df_never_faced_the_team = df_filtered[df_filtered[6] == \"Never Faced Any Players on this Team.\"].copy()\n",
    "\n",
    "    # Removing rows from the filtered DataFrame\n",
    "    df_filtered = df_filtered[df_filtered[6] != \"Never Faced Any Players on this Team.\"]\n",
    "\n",
    "    # Filtering rows where col9 does not contains \"Exit Velo Launch Angle xBA xSLG xwOBA\"\n",
    "    df_not_complete = df_filtered[df_filtered[9] != \"Exit Velo Launch Angle xBA xSLG xwOBA\"].copy()\n",
    "\n",
    "    # Removing rows from the filtered DataFrame\n",
    "    df_filtered = df_filtered[df_filtered[9] == \"Exit Velo Launch Angle xBA xSLG xwOBA\"]\n",
    "\n",
    "    # Convert empty strings to NaN for better handling\n",
    "    df_filtered[17] = df_filtered[17].replace(\"\", pd.NA)\n",
    "\n",
    "    # Count occurrences of each unique non-empty value\n",
    "    value_counts = df_filtered[17].dropna().value_counts()\n",
    "\n",
    "    if not value_counts.empty:\n",
    "        # Identify the most frequent value\n",
    "        most_frequent_value = value_counts.idxmax()\n",
    "\n",
    "        # Fill NaN values with the most frequent value\n",
    "        df_filtered[17] = df_filtered[17].fillna(most_frequent_value)\n",
    "\n",
    "    # Splitting on the \" ET \". Splitting only col 2 while keeping other columns\n",
    "    df_expanded = new_df.copy()  # Preserve other columns\n",
    "    df_expanded[['Column1', 'Column2']] = df_expanded[2].str.split(\"ET\", expand=True)\n",
    "\n",
    "    # Drop original column\n",
    "    df_expanded = df_expanded.drop(columns=[2, 3])\n",
    "\n",
    "    # Splitting columns\n",
    "    df_split = df_filtered.copy()  # Preserve other columns\n",
    "    df_split[['PA_away_pitcher', 'K%_away_pitcher', 'BB%_away_pitcher', 'AVG_away_pitcher', 'wOBA_away_pitcher']] = df_split[8].str.split(\" \", expand=True)\n",
    "    df_split[['Exit_Velo_away_pitcher', 'unit_away', 'Lunch_Angle_away_pitcher', 'xBA_away_pitcher', 'xSLG_away_pitcher', 'xwOBA_away_pitcher']] = df_split[10].str.split(\" \", expand=True)\n",
    "\n",
    "    df_split[['PA_home_pitcher', 'K%_home_pitcher', 'BB%_home_pitcher', 'AVG_home_pitcher', 'wOBA_home_pitcher']] = df_split[16].str.split(\" \", expand=True)\n",
    "    df_split[['Exit_Velo_home_pitcher', 'unit_home', 'Lunch_Angle_home_pitcher', 'xBA_home_pitcher', 'xSLG_home_pitcher', 'xwOBA_home_pitcher']] = df_split[18].str.split(\" \", expand=True)\n",
    "\n",
    "    # Drop original columns if needed\n",
    "    df_split = df_split.drop(columns=[6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19])\n",
    "\n",
    "    # Function to remove city names\n",
    "    def remove_city_names(text):\n",
    "        return re.sub(r'\\b(?:San Diego|Pittsburgh|Arizona|Philadelphia|Kansas City|Baltimore|Tampa Bay|New York|Cleveland|Toronto|Minnesota|Boston|Los Angeles|Atlanta|Houston|Chicago|Seattle|Texas|Milwaukee|St. Louis|Detroit|Colorado|San Francisco)\\b ', '', text)\n",
    "\n",
    "    # Apply the function to the first column of the DataFrame\n",
    "    df_split[0] = df_split[0].apply(remove_city_names)\n",
    "\n",
    "    # Replace @ in col 0 with \"vs\"\n",
    "    df_split[0] = df_split[0].str.replace(\" @ \", \" vs \", regex= True)\n",
    "\n",
    "    # Splitting on the \" vs \". Splitting only col 0 while keeping other columns.\n",
    "    df_split = df_split.copy()  # Preserve other columns\n",
    "    df_split[['away_team', 'home_team']] = df_split[0].str.split(\" vs \", expand=True)\n",
    "\n",
    "    # Update the column with the date\n",
    "    df_split[1] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # # Add the headers\n",
    "    # new_df.columns = ['teams', 'date', 'time_and_park', 'away_pitcher_name', 'away_pitcher_throws', 'away_pitcher_info', \n",
    "    #                     'away_pitcher_fields_1', 'away_pitcher_data_1', 'away_pitcher_fields_2', 'away_pitcher_data_2',]\n",
    "    \n",
    "    return df_split, df_never_faced_the_team, df_not_complete\n",
    "\n",
    "\n",
    "# Call the function to get today's matchups with advanced stats\n",
    "advanced_matchups_df, never_faced_the_team_df, df_not_complete = advanced_matchups()\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "advanced_matchups_df.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\advanced_matchups\\\\advanced_matchups.csv', index=False)\n",
    "never_faced_the_team_df.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\advanced_matchups\\\\never_faced_the_team.csv', index=False)\n",
    "df_not_complete.to_csv('D:\\\\mlb_analyzer\\\\output\\\\teams\\\\advanced_matchups\\\\not_complete.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b4df6",
   "metadata": {},
   "source": [
    "# Import advanced data and gamelogs for each team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d2900",
   "metadata": {},
   "source": [
    "#### This works for hitting and pitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84020fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def team_advanced_stats(analysis_type):\n",
    "#     \"\"\"Get the advanced stats for the team.\n",
    "#     The function scrapes the advanced stats from the MLB website using Selenium and returns three dataframes:\n",
    "#     statcast, plate_discipline and batted_ball_profile.\n",
    "\n",
    "#     Returns:\n",
    "#         _type_: _description_\n",
    "#     \"\"\"\n",
    "#     # Load the options\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "#     options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "#     year = datetime.now().year\n",
    "\n",
    "#     # Set up the WebDriver\n",
    "#     driver = webdriver.Chrome(options= options)    \n",
    "#     driver.get(f\"https://baseballsavant.mlb.com/team/114?view=statcast&nav={analysis_type}&season={year}\")\n",
    "\n",
    "#     datatable_id = 'div_statcast'\n",
    "#     datatable_xpath = f\"//div[@id='{datatable_id}']\"  # Update XPATH as needed\n",
    "\n",
    "#     # Initialize an empty dataframe\n",
    "#     team_advanced_stats_df = pd.DataFrame()\n",
    "\n",
    "#     try:\n",
    "#         # Explicitly wait for the table element to load\n",
    "#         WebDriverWait(driver, 20).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "#         )\n",
    "#         print(f\"{datatable_id} table loaded successfully.\")\n",
    "\n",
    "#         # Locate the table\n",
    "#         table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "#         text_content = table_element.text\n",
    "\n",
    "#         # Process the table content\n",
    "#         rows = text_content.split(\"\\n\")\n",
    "#         table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "#         # Convert to dataframe\n",
    "#         team_advanced_stats_df = pd.DataFrame(table_data)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: Table {datatable_id} did not load. Returning an empty dataframe. Details: {e}\")\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "    \n",
    "#     # Check if the dataframe is empty\n",
    "#     # If the dataframe is empty, return empty dataframes\n",
    "#     if team_advanced_stats_df.empty:\n",
    "#         print(\"No data found. Skipping table creation.\")        \n",
    "#         return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()  # Return empty dataframes\n",
    "#     else:\n",
    "        \n",
    "#         #! STATCAST TABLE\n",
    "#         #! Cleaning the data for the statcast table\n",
    "\n",
    "\n",
    "#         def combine_rows(df, row_number_1, row_number_2):\n",
    "#             \"\"\"Combine two rows in a DataFrame into one row.\n",
    "#             The first row will contain the combined data, and the second row will be dropped.\n",
    "\n",
    "#             Args:\n",
    "#                 df (_type_): _description_\n",
    "#                 row_number_1 (_type_): _description_\n",
    "#                 row_number_2 (_type_): _description_\n",
    "\n",
    "#             Returns:\n",
    "#                 _type_: _description_\n",
    "#             \"\"\"\n",
    "#             # Combine the two rows\n",
    "#             df.loc[row_number_1, 0] = df.loc[row_number_1, 0] + ' ' + df.loc[row_number_2, 0]\n",
    "\n",
    "#             # Drop the second row\n",
    "#             df = df.drop(row_number_2).reset_index(drop=True)\n",
    "            \n",
    "#             return df\n",
    "\n",
    "\n",
    "#         # Join rows\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 19, 20)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 23, 24)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 24, 25)\n",
    "\n",
    "#         #! Create the headers for the first table (statcast)\n",
    "#         # Find the first occurrence of 'Player' and 'XWOBACON'\n",
    "#         # Find indices\n",
    "#         start_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Player'].index[0]\n",
    "#         end_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'XWOBACON'].index[0]\n",
    "\n",
    "#         # Slice and transpose\n",
    "#         headers_statcast = team_advanced_stats_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "#         # Reset column names\n",
    "#         headers_statcast.columns = headers_statcast.iloc[0]\n",
    "#         headers_statcast         = headers_statcast[1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove those rows from original dataframe\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(team_advanced_stats_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "#         # Remove the first 3 rows\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[3:].reset_index(drop=True)  # Using iloc\n",
    "\n",
    "#         # Use regex to split the column while preserving negative numbers\n",
    "#         team_advanced_stats_df[['Name', 'Numbers']] = team_advanced_stats_df[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "\n",
    "#         # Split the 'Numbers' column into separate columns (25 columns)\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.join(team_advanced_stats_df['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "\n",
    "#         # Drop the original 'Numbers' column\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(columns=['Numbers'])\n",
    "\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Extract rows from the start until the first empty row\n",
    "#         statcast = team_advanced_stats_df.iloc[:first_empty_idx]\n",
    "\n",
    "#         # Drop the first column\n",
    "#         statcast = statcast.drop(columns=[0])\n",
    "\n",
    "#         # Swap last name and first name\n",
    "#         statcast['Name'] = statcast['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "#         # Add the headers to the dataframe\n",
    "#         statcast.columns = headers_statcast.columns\n",
    "\n",
    "#         #! PLATE DISCIPLINE TABLE\n",
    "#         #! Cleaning the data for the plate discipline table\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Remove rows from the first row until the first empty row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove the first row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "#         # Join rows\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 4, 5)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 5, 6)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 7, 8)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 9, 10)\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 13, 14)\n",
    "\n",
    "#         #! Create the headers for the second table (plate discipline)\n",
    "#         # Find the first occurrence of 'Player' and 'Meatball Swing %'\n",
    "#         # Find indices\n",
    "#         start_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Player'].index[0]\n",
    "#         end_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Meatball Swing %'].index[0]\n",
    "\n",
    "#         # Slice and transpose\n",
    "#         headers_plate_discipline = team_advanced_stats_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "#         # Reset column names\n",
    "#         headers_plate_discipline.columns = headers_plate_discipline.iloc[0]\n",
    "#         headers_plate_discipline         = headers_plate_discipline[1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove those rows from original dataframe\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(team_advanced_stats_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Extract rows from the start until the first empty row\n",
    "#         plate_discipline = team_advanced_stats_df.iloc[:first_empty_idx]\n",
    "\n",
    "#         # Drop the first column\n",
    "#         plate_discipline = plate_discipline.drop(columns=[0])\n",
    "\n",
    "#         # Swap last name and first name\n",
    "#         plate_discipline['Name'] = plate_discipline['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "#         # Drop columns where all values are NaN (empty)\n",
    "#         plate_discipline = plate_discipline.dropna(axis= 1, how= 'all')\n",
    "\n",
    "#         # Add the headers to the dataframe\n",
    "#         plate_discipline.columns = headers_plate_discipline.columns\n",
    "\n",
    "#         #! BATTED BALL PROFILE TABLE\n",
    "#         #! Cleaning the data for the batted ball profile table\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         # Remove rows from the first row until the first empty row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove the first row\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "#         # Join rows\n",
    "#         team_advanced_stats_df = combine_rows(team_advanced_stats_df, 15, 16)\n",
    "\n",
    "#         #! Create the headers for the third table (batted ball profile)\n",
    "#         # Find the first occurrence of 'Player' and 'Barrel %'\n",
    "#         # Find indices\n",
    "#         start_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Player'].index[0]\n",
    "#         end_idx = team_advanced_stats_df[team_advanced_stats_df[0] == 'Barrel %'].index[0]\n",
    "\n",
    "#         # Slice and transpose\n",
    "#         headers_batted_ball_profile = team_advanced_stats_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "#         # Reset column names\n",
    "#         headers_batted_ball_profile.columns = headers_batted_ball_profile.iloc[0]\n",
    "#         headers_batted_ball_profile         = headers_batted_ball_profile[1:].reset_index(drop=True)\n",
    "\n",
    "#         # Remove those rows from original dataframe\n",
    "#         team_advanced_stats_df = team_advanced_stats_df.drop(team_advanced_stats_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "#         # Find the first empty row\n",
    "#         first_empty_idx = team_advanced_stats_df[team_advanced_stats_df[0] == ''].index.min()\n",
    "\n",
    "#         if pd.isna(first_empty_idx):\n",
    "#             batted_ball_profile = team_advanced_stats_df.copy()  # If no empty row, use the entire DataFrame\n",
    "#         else:\n",
    "#             # Extract rows from the start until the first empty row\n",
    "#             batted_ball_profile = team_advanced_stats_df.iloc[:first_empty_idx]\n",
    "\n",
    "#         # Drop the first column\n",
    "#         batted_ball_profile = batted_ball_profile.drop(columns=[0])\n",
    "\n",
    "#         # Swap last name and first name\n",
    "#         batted_ball_profile['Name'] = batted_ball_profile['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "#         # Drop columns where all values are NaN (empty)\n",
    "#         batted_ball_profile = batted_ball_profile.dropna(axis= 1, how= 'all')\n",
    "\n",
    "#         # Add the headers to the dataframe\n",
    "#         batted_ball_profile.columns = headers_batted_ball_profile.columns\n",
    "        \n",
    "#         return statcast, plate_discipline, batted_ball_profile\n",
    "\n",
    "\n",
    "# # Call the function to get today's matchups with advanced stats\n",
    "# #hitting_statcast_df,  hitting_plate_discipline_df,  hitting_batted_ball_profile_df  = team_advanced_stats(analysis_type= 'hitting')\n",
    "# pitching_statcast_df, pitching_plate_discipline_df, pitching_batted_ball_profile_df = team_advanced_stats(analysis_type= 'pitching')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #! STATCAST TABLE\n",
    "# #! Cleaning the data for the statcast table\n",
    "\n",
    "# def combine_rows(df, row_number_1, row_number_2):\n",
    "#     \"\"\"Combine two rows in a DataFrame into one row.\n",
    "#     The first row will contain the combined data, and the second row will be dropped.\n",
    "\n",
    "#     Args:\n",
    "#         df (_type_): _description_\n",
    "#         row_number_1 (_type_): _description_\n",
    "#         row_number_2 (_type_): _description_\n",
    "\n",
    "#     Returns:\n",
    "#         _type_: _description_\n",
    "#     \"\"\"\n",
    "#     # Combine the two rows\n",
    "#     df.loc[row_number_1, 0] = df.loc[row_number_1, 0] + ' ' + df.loc[row_number_2, 0]\n",
    "\n",
    "#     # Drop the second row\n",
    "#     df = df.drop(row_number_2).reset_index(drop=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# # Join rows\n",
    "# test_df = combine_rows(test_df, 19, 20)\n",
    "# test_df = combine_rows(test_df, 23, 24)\n",
    "# test_df = combine_rows(test_df, 24, 25)\n",
    "\n",
    "# #! Create the headers for the first table (statcast)\n",
    "# # Find the first occurrence of 'Player' and 'XWOBACON'\n",
    "# # Find indices\n",
    "# start_idx = test_df[test_df[0] == 'Player'].index[0]\n",
    "# end_idx = test_df[test_df[0] == 'XWOBACON'].index[0]\n",
    "\n",
    "# # Slice and transpose\n",
    "# headers_statcast = test_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "# # Reset column names\n",
    "# headers_statcast.columns = headers_statcast.iloc[0]\n",
    "# headers_statcast         = headers_statcast[1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove those rows from original dataframe\n",
    "# test_df = test_df.drop(test_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "# # Remove the first 3 rows\n",
    "# test_df = test_df.iloc[3:].reset_index(drop=True)  # Using iloc\n",
    "\n",
    "# # Use regex to split the column while preserving negative numbers\n",
    "# test_df[['Name', 'Numbers']] = test_df[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "\n",
    "# # Split the 'Numbers' column into separate columns (25 columns)\n",
    "# test_df = test_df.join(test_df['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "\n",
    "# # Drop the original 'Numbers' column\n",
    "# test_df = test_df.drop(columns=['Numbers'])\n",
    "\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Extract rows from the start until the first empty row\n",
    "# statcast = test_df.iloc[:first_empty_idx]\n",
    "\n",
    "# # Drop the first column\n",
    "# statcast = statcast.drop(columns=[0])\n",
    "\n",
    "# # Swap last name and first name\n",
    "# statcast['Name'] = statcast['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "# # Add the headers to the dataframe\n",
    "# statcast.columns = headers_statcast.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89195ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #! PLATE DISCIPLINE TABLE\n",
    "# #! Cleaning the data for the plate discipline table\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Remove rows from the first row until the first empty row\n",
    "# test_df = test_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove the first row\n",
    "# test_df = test_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "# # Join rows\n",
    "# test_df = combine_rows(test_df, 4, 5)\n",
    "# test_df = combine_rows(test_df, 5, 6)\n",
    "# test_df = combine_rows(test_df, 7, 8)\n",
    "# test_df = combine_rows(test_df, 9, 10)\n",
    "# test_df = combine_rows(test_df, 13, 14)\n",
    "\n",
    "# #! Create the headers for the second table (plate discipline)\n",
    "# # Find the first occurrence of 'Player' and 'Meatball Swing %'\n",
    "# # Find indices\n",
    "# start_idx = test_df[test_df[0] == 'Player'].index[0]\n",
    "# end_idx = test_df[test_df[0] == 'Meatball Swing %'].index[0]\n",
    "\n",
    "# # Slice and transpose\n",
    "# headers_plate_discipline = test_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "# # Reset column names\n",
    "# headers_plate_discipline.columns = headers_plate_discipline.iloc[0]\n",
    "# headers_plate_discipline         = headers_plate_discipline[1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove those rows from original dataframe\n",
    "# test_df = test_df.drop(test_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Extract rows from the start until the first empty row\n",
    "# plate_discipline = test_df.iloc[:first_empty_idx]\n",
    "\n",
    "# # Drop the first column\n",
    "# plate_discipline = plate_discipline.drop(columns=[0])\n",
    "\n",
    "# # Swap last name and first name\n",
    "# plate_discipline['Name'] = plate_discipline['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "# # Drop columns where all values are NaN (empty)\n",
    "# plate_discipline = plate_discipline.dropna(axis= 1, how= 'all')\n",
    "\n",
    "# # Add the headers to the dataframe\n",
    "# plate_discipline.columns = headers_plate_discipline.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536985b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #! BATTED BALL PROFILE TABLE\n",
    "# #! Cleaning the data for the batted ball profile table\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# # Remove rows from the first row until the first empty row\n",
    "# test_df = test_df.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove the first row\n",
    "# test_df = test_df.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "# # Join rows\n",
    "# test_df = combine_rows(test_df, 15, 16)\n",
    "\n",
    "# #! Create the headers for the third table (batted ball profile)\n",
    "# # Find the first occurrence of 'Player' and 'Barrel %'\n",
    "# # Find indices\n",
    "# start_idx = test_df[test_df[0] == 'Player'].index[0]\n",
    "# end_idx = test_df[test_df[0] == 'Barrel %'].index[0]\n",
    "\n",
    "# # Slice and transpose\n",
    "# headers_batted_ball_profile = test_df.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "# # Reset column names\n",
    "# headers_batted_ball_profile.columns = headers_batted_ball_profile.iloc[0]\n",
    "# headers_batted_ball_profile         = headers_batted_ball_profile[1:].reset_index(drop=True)\n",
    "\n",
    "# # Remove those rows from original dataframe\n",
    "# test_df = test_df.drop(test_df.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "# # Find the first empty row\n",
    "# first_empty_idx = test_df[test_df[0] == ''].index.min()\n",
    "\n",
    "# if pd.isna(first_empty_idx):\n",
    "#     batted_ball_profile = test_df.copy()  # If no empty row, use the entire DataFrame\n",
    "# else:\n",
    "#     # Extract rows from the start until the first empty row\n",
    "#     batted_ball_profile = test_df.iloc[:first_empty_idx]\n",
    "\n",
    "# # Drop the first column\n",
    "# batted_ball_profile = batted_ball_profile.drop(columns=[0])\n",
    "\n",
    "# # Swap last name and first name\n",
    "# batted_ball_profile['Name'] = batted_ball_profile['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "# # Drop columns where all values are NaN (empty)\n",
    "# batted_ball_profile = batted_ball_profile.dropna(axis= 1, how= 'all')\n",
    "\n",
    "# # Add the headers to the dataframe\n",
    "# batted_ball_profile.columns = headers_batted_ball_profile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitching data successfully retrieved for team 108\n",
      "pitching data successfully retrieved for team 117\n",
      "pitching data successfully retrieved for team 133\n",
      "pitching data successfully retrieved for team 141\n",
      "pitching data successfully retrieved for team 144\n",
      "pitching data successfully retrieved for team 158\n",
      "pitching data successfully retrieved for team 138\n",
      "pitching data successfully retrieved for team 112\n",
      "pitching data successfully retrieved for team 109\n",
      "pitching data successfully retrieved for team 119\n",
      "pitching data successfully retrieved for team 137\n",
      "pitching data successfully retrieved for team 114\n",
      "pitching data successfully retrieved for team 137\n",
      "pitching data successfully retrieved for team 114\n",
      "pitching data successfully retrieved for team 136\n",
      "pitching data successfully retrieved for team 146\n",
      "pitching data successfully retrieved for team 121\n",
      "pitching data successfully retrieved for team 120\n",
      "pitching data successfully retrieved for team 110\n",
      "pitching data successfully retrieved for team 135\n",
      "pitching data successfully retrieved for team 143\n",
      "pitching data successfully retrieved for team 134\n",
      "pitching data successfully retrieved for team 140\n",
      "pitching data successfully retrieved for team 139\n",
      "pitching data successfully retrieved for team 113\n",
      "pitching data successfully retrieved for team 111\n",
      "pitching data successfully retrieved for team 115\n",
      "pitching data successfully retrieved for team 118\n",
      "pitching data successfully retrieved for team 116\n",
      "pitching data successfully retrieved for team 142\n",
      "pitching data successfully retrieved for team 145\n",
      "pitching data successfully retrieved for team 147\n",
      "hitting data successfully retrieved for team 108\n",
      "hitting data successfully retrieved for team 117\n",
      "hitting data successfully retrieved for team 133\n",
      "hitting data successfully retrieved for team 141\n",
      "hitting data successfully retrieved for team 144\n",
      "hitting data successfully retrieved for team 158\n",
      "hitting data successfully retrieved for team 138\n",
      "hitting data successfully retrieved for team 112\n",
      "hitting data successfully retrieved for team 109\n",
      "hitting data successfully retrieved for team 119\n",
      "hitting data successfully retrieved for team 137\n",
      "hitting data successfully retrieved for team 114\n",
      "hitting data successfully retrieved for team 137\n",
      "hitting data successfully retrieved for team 114\n",
      "hitting data successfully retrieved for team 136\n",
      "hitting data successfully retrieved for team 146\n",
      "hitting data successfully retrieved for team 121\n",
      "hitting data successfully retrieved for team 120\n",
      "hitting data successfully retrieved for team 110\n",
      "hitting data successfully retrieved for team 135\n",
      "hitting data successfully retrieved for team 143\n",
      "hitting data successfully retrieved for team 134\n",
      "hitting data successfully retrieved for team 140\n",
      "hitting data successfully retrieved for team 139\n",
      "hitting data successfully retrieved for team 113\n",
      "hitting data successfully retrieved for team 111\n",
      "hitting data successfully retrieved for team 115\n",
      "hitting data successfully retrieved for team 118\n",
      "hitting data successfully retrieved for team 116\n",
      "Error retrieving data for teams. Details: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7451AA145+76773]\n",
      "\tGetHandleVerifier [0x00007FF7451AA1A0+76864]\n",
      "\t(No symbol) [0x00007FF744F68F7A]\n",
      "\t(No symbol) [0x00007FF744FBF496]\n",
      "\t(No symbol) [0x00007FF744FBF74C]\n",
      "\t(No symbol) [0x00007FF745012287]\n",
      "\t(No symbol) [0x00007FF744FE739F]\n",
      "\t(No symbol) [0x00007FF74500F0CF]\n",
      "\t(No symbol) [0x00007FF744FE7133]\n",
      "\t(No symbol) [0x00007FF744FB04D1]\n",
      "\t(No symbol) [0x00007FF744FB1263]\n",
      "\tGetHandleVerifier [0x00007FF74546A8ED+2962317]\n",
      "\tGetHandleVerifier [0x00007FF745464EC2+2939234]\n",
      "\tGetHandleVerifier [0x00007FF745482FF3+3062419]\n",
      "\tGetHandleVerifier [0x00007FF7451C4B9A+185914]\n",
      "\tGetHandleVerifier [0x00007FF7451CC78F+217647]\n",
      "\tGetHandleVerifier [0x00007FF7451B2A44+111844]\n",
      "\tGetHandleVerifier [0x00007FF7451B2BF2+112274]\n",
      "\tGetHandleVerifier [0x00007FF745198A79+5401]\n",
      "\tBaseThreadInitThunk [0x00007FFFA6BB7374+20]\n",
      "\tRtlUserThreadStart [0x00007FFFA889CC91+33]\n",
      "\n",
      "pitching gamelogs successfully retrieved for team 108\n",
      "pitching gamelogs successfully retrieved for team 117\n",
      "pitching gamelogs successfully retrieved for team 133\n",
      "pitching gamelogs successfully retrieved for team 141\n",
      "pitching gamelogs successfully retrieved for team 144\n",
      "pitching gamelogs successfully retrieved for team 158\n",
      "pitching gamelogs successfully retrieved for team 138\n",
      "pitching gamelogs successfully retrieved for team 112\n",
      "pitching gamelogs successfully retrieved for team 109\n",
      "pitching gamelogs successfully retrieved for team 119\n",
      "pitching gamelogs successfully retrieved for team 137\n",
      "pitching gamelogs successfully retrieved for team 114\n",
      "pitching gamelogs successfully retrieved for team 137\n",
      "pitching gamelogs successfully retrieved for team 114\n",
      "pitching gamelogs successfully retrieved for team 136\n",
      "pitching gamelogs successfully retrieved for team 146\n",
      "pitching gamelogs successfully retrieved for team 121\n",
      "pitching gamelogs successfully retrieved for team 120\n",
      "pitching gamelogs successfully retrieved for team 110\n",
      "pitching gamelogs successfully retrieved for team 135\n",
      "pitching gamelogs successfully retrieved for team 143\n",
      "pitching gamelogs successfully retrieved for team 134\n",
      "pitching gamelogs successfully retrieved for team 140\n",
      "pitching gamelogs successfully retrieved for team 139\n",
      "pitching gamelogs successfully retrieved for team 113\n",
      "pitching gamelogs successfully retrieved for team 111\n",
      "pitching gamelogs successfully retrieved for team 115\n",
      "pitching gamelogs successfully retrieved for team 118\n",
      "pitching gamelogs successfully retrieved for team 116\n",
      "pitching gamelogs successfully retrieved for team 142\n",
      "pitching gamelogs successfully retrieved for team 145\n",
      "pitching gamelogs successfully retrieved for team 147\n",
      "hitting gamelogs successfully retrieved for team 108\n",
      "hitting gamelogs successfully retrieved for team 117\n",
      "hitting gamelogs successfully retrieved for team 133\n",
      "hitting gamelogs successfully retrieved for team 141\n",
      "hitting gamelogs successfully retrieved for team 144\n",
      "hitting gamelogs successfully retrieved for team 158\n",
      "hitting gamelogs successfully retrieved for team 138\n",
      "hitting gamelogs successfully retrieved for team 112\n",
      "Error retrieving data for teams. Details: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7451AA145+76773]\n",
      "\tGetHandleVerifier [0x00007FF7451AA1A0+76864]\n",
      "\t(No symbol) [0x00007FF744F68F7A]\n",
      "\t(No symbol) [0x00007FF744FBF496]\n",
      "\t(No symbol) [0x00007FF744FBF74C]\n",
      "\t(No symbol) [0x00007FF745012287]\n",
      "\t(No symbol) [0x00007FF744FE739F]\n",
      "\t(No symbol) [0x00007FF74500F0CF]\n",
      "\t(No symbol) [0x00007FF744FE7133]\n",
      "\t(No symbol) [0x00007FF744FB04D1]\n",
      "\t(No symbol) [0x00007FF744FB1263]\n",
      "\tGetHandleVerifier [0x00007FF74546A8ED+2962317]\n",
      "\tGetHandleVerifier [0x00007FF745464EC2+2939234]\n",
      "\tGetHandleVerifier [0x00007FF745482FF3+3062419]\n",
      "\tGetHandleVerifier [0x00007FF7451C4B9A+185914]\n",
      "\tGetHandleVerifier [0x00007FF7451CC78F+217647]\n",
      "\tGetHandleVerifier [0x00007FF7451B2A44+111844]\n",
      "\tGetHandleVerifier [0x00007FF7451B2BF2+112274]\n",
      "\tGetHandleVerifier [0x00007FF745198A79+5401]\n",
      "\tBaseThreadInitThunk [0x00007FFFA6BB7374+20]\n",
      "\tRtlUserThreadStart [0x00007FFFA889CC91+33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def import_advanced_stats(analysis_type, team_ids, category):\n",
    "    \"\"\"Get the advanced stats for multiple teams.\n",
    "    The function scrapes the advanced stats from the MLB website using Selenium and returns a dictionary of dataframes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping team IDs to their respective advanced stats dataframe.\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    options.binary_location = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "\n",
    "    year = datetime.now().year\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    team_data = {}\n",
    "\n",
    "    for team_id in team_ids:\n",
    "        try:\n",
    "            if category == \"statcast\":\n",
    "                url = f\"https://baseballsavant.mlb.com/team/{team_id}?view={category}&nav={analysis_type}&season={year}\"\n",
    "                datatable_id = 'div_statcast'\n",
    "            elif category == \"gamelogs\":\n",
    "                url = f\"https://baseballsavant.mlb.com/team/{team_id}?view={category}&nav={analysis_type}&season={year}\"\n",
    "                datatable_id = 'div_gamelogs'\n",
    "\n",
    "            driver.get(url)\n",
    "            \n",
    "            datatable_xpath = f\"//div[@id='{datatable_id}']\"\n",
    "\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, datatable_xpath))\n",
    "            )\n",
    "\n",
    "            table_element = driver.find_element(By.XPATH, datatable_xpath)\n",
    "            text_content = table_element.text\n",
    "\n",
    "            rows = text_content.split(\"\\n\")\n",
    "            table_data = [row.split(\"\\t\") for row in rows]\n",
    "\n",
    "            team_data[team_id] = pd.DataFrame(table_data)\n",
    "            \n",
    "            # Print the status of the data retrieval\n",
    "            if category == \"statcast\":\n",
    "                print(f\"{analysis_type} data successfully retrieved for team {team_id}\")\n",
    "            elif category == \"gamelogs\":\n",
    "                print(f\"{analysis_type} gamelogs successfully retrieved for team {team_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping team {team_id} due to error: {e}\")\n",
    "            continue  # Skip to the next iteration\n",
    "        \n",
    "    driver.quit()\n",
    "    return team_data\n",
    "\n",
    "# Import the tables for each team\n",
    "team_ids = [108, 117, 133, 141, 144, 158, 138, 112, 109, 119, 137, 114,\n",
    "            137, 114, 136, 146, 121, 120, 110, 135, 143, 134, 140, 139,\n",
    "            113, 111, 115, 118, 116, 142, 145, 147]\n",
    "\n",
    "# Import the advanced stats for each team\n",
    "data_pitching = import_advanced_stats(\"pitching\", team_ids, \"statcast\")\n",
    "data_hitting = import_advanced_stats(\"hitting\", team_ids, \"statcast\")\n",
    "\n",
    "# Import the gamelogs for each team\n",
    "gamelog_data_pitching = import_advanced_stats(\"pitching\", team_ids, \"gamelogs\")\n",
    "gamelog_data_hitting = import_advanced_stats(\"hitting\", team_ids, \"gamelogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ce2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_advanced_stats(dataframe_name):\n",
    "    \"\"\"Get the advanced stats for the team.\n",
    "    The function scrapes the advanced stats from the MLB website using Selenium and returns three dataframes:\n",
    "    statcast, plate_discipline and batted_ball_profile.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"   \n",
    "        \n",
    "    #! STATCAST TABLE\n",
    "    #! Cleaning the data for the statcast table\n",
    "    def combine_rows(df, row_number_1, row_number_2):\n",
    "        \"\"\"Combine two rows in a DataFrame into one row.\n",
    "        The first row will contain the combined data, and the second row will be dropped.\n",
    "\n",
    "        Args:\n",
    "            df (_type_): _description_\n",
    "            row_number_1 (_type_): _description_\n",
    "            row_number_2 (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Combine the two rows\n",
    "        df.loc[row_number_1, 0] = df.loc[row_number_1, 0] + ' ' + df.loc[row_number_2, 0]\n",
    "\n",
    "        # Drop the second row\n",
    "        df = df.drop(row_number_2).reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    # Join rows\n",
    "    dataframe_name = combine_rows(dataframe_name, 19, 20)\n",
    "    dataframe_name = combine_rows(dataframe_name, 23, 24)\n",
    "    dataframe_name = combine_rows(dataframe_name, 24, 25)\n",
    "\n",
    "    #! Create the headers for the first table (statcast)\n",
    "    # Find the first occurrence of 'Player' and 'XWOBACON'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Player'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'XWOBACON'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_statcast = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_statcast.columns = headers_statcast.iloc[0]\n",
    "    headers_statcast         = headers_statcast[1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "    # Remove the first 3 rows\n",
    "    dataframe_name = dataframe_name.iloc[3:].reset_index(drop=True)  # Using iloc\n",
    "\n",
    "    # Use regex to split the column while preserving negative numbers\n",
    "    dataframe_name[['Name', 'Numbers']] = dataframe_name[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "    \n",
    "    # Remove initial dot from values that start with \".\"\n",
    "    dataframe_name['Numbers'] = dataframe_name['Numbers'].apply(lambda x: x[1:] if x.startswith('. ') else x)\n",
    "\n",
    "    # Split the 'Numbers' column into separate columns (25 columns)\n",
    "    dataframe_name = dataframe_name.join(dataframe_name['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "\n",
    "    # Drop the original 'Numbers' column\n",
    "    dataframe_name = dataframe_name.drop(columns=['Numbers'])\n",
    "\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Extract rows from the start until the first empty row\n",
    "    statcast = dataframe_name.iloc[:first_empty_idx]\n",
    "\n",
    "    # Drop the first column\n",
    "    statcast = statcast.drop(columns=[0])\n",
    "\n",
    "    # Swap last name and first name\n",
    "    statcast['Name'] = statcast['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "    # Add the headers to the dataframe\n",
    "    statcast.columns = headers_statcast.columns\n",
    "\n",
    "    #! PLATE DISCIPLINE TABLE\n",
    "    #! Cleaning the data for the plate discipline table\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Remove rows from the first row until the first empty row\n",
    "    dataframe_name = dataframe_name.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove the first row\n",
    "    dataframe_name = dataframe_name.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "    # Join rows\n",
    "    dataframe_name = combine_rows(dataframe_name, 4, 5)\n",
    "    dataframe_name = combine_rows(dataframe_name, 5, 6)\n",
    "    dataframe_name = combine_rows(dataframe_name, 7, 8)\n",
    "    dataframe_name = combine_rows(dataframe_name, 9, 10)\n",
    "    dataframe_name = combine_rows(dataframe_name, 13, 14)\n",
    "\n",
    "    #! Create the headers for the second table (plate discipline)\n",
    "    # Find the first occurrence of 'Player' and 'Meatball Swing %'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Player'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'Meatball Swing %'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_plate_discipline = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_plate_discipline.columns = headers_plate_discipline.iloc[0]\n",
    "    headers_plate_discipline         = headers_plate_discipline[1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Extract rows from the start until the first empty row\n",
    "    plate_discipline = dataframe_name.iloc[:first_empty_idx]\n",
    "\n",
    "    # Drop the first column\n",
    "    plate_discipline = plate_discipline.drop(columns=[0])\n",
    "\n",
    "    # Swap last name and first name\n",
    "    plate_discipline['Name'] = plate_discipline['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "    # Drop columns where all values are NaN (empty)\n",
    "    plate_discipline = plate_discipline.dropna(axis= 1, how= 'all')\n",
    "\n",
    "    # Add the headers to the dataframe\n",
    "    plate_discipline.columns = headers_plate_discipline.columns\n",
    "\n",
    "    #! BATTED BALL PROFILE TABLE\n",
    "    #! Cleaning the data for the batted ball profile table\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    # Remove rows from the first row until the first empty row\n",
    "    dataframe_name = dataframe_name.iloc[first_empty_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove the first row\n",
    "    dataframe_name = dataframe_name.iloc[1:].reset_index(drop= True)  # Using iloc\n",
    "\n",
    "    # Join rows\n",
    "    dataframe_name = combine_rows(dataframe_name, 15, 16)\n",
    "\n",
    "    #! Create the headers for the third table (batted ball profile)\n",
    "    # Find the first occurrence of 'Player' and 'Barrel %'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Player'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'Barrel %'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_batted_ball_profile = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_batted_ball_profile.columns = headers_batted_ball_profile.iloc[0]\n",
    "    headers_batted_ball_profile         = headers_batted_ball_profile[1:].reset_index(drop=True)\n",
    "\n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "\n",
    "    # Find the first empty row\n",
    "    first_empty_idx = dataframe_name[dataframe_name[0] == ''].index.min()\n",
    "\n",
    "    if pd.isna(first_empty_idx):\n",
    "        batted_ball_profile = dataframe_name.copy()  # If no empty row, use the entire DataFrame\n",
    "    else:\n",
    "        # Extract rows from the start until the first empty row\n",
    "        batted_ball_profile = dataframe_name.iloc[:first_empty_idx]\n",
    "\n",
    "    # Drop the first column\n",
    "    batted_ball_profile = batted_ball_profile.drop(columns=[0])\n",
    "\n",
    "    # Swap last name and first name\n",
    "    batted_ball_profile['Name'] = batted_ball_profile['Name'].str.split(', ').str[::-1].str.join(' ')\n",
    "\n",
    "    # Drop columns where all values are NaN (empty)\n",
    "    batted_ball_profile = batted_ball_profile.dropna(axis= 1, how= 'all')\n",
    "\n",
    "    # Add the headers to the dataframe\n",
    "    batted_ball_profile.columns = headers_batted_ball_profile.columns\n",
    "    \n",
    "    return statcast, plate_discipline, batted_ball_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gamelogs(dataframe_name):        \n",
    "    #! Create the headers for the gamelogs\n",
    "    # Find the first occurrence of 'Game Date' and 'Hard Hit %'\n",
    "    # Find indices\n",
    "    start_idx = dataframe_name[dataframe_name[0] == 'Game Date'].index[0]\n",
    "    end_idx = dataframe_name[dataframe_name[0] == 'Hard Hit %'].index[0]\n",
    "\n",
    "    # Slice and transpose\n",
    "    headers_gamelogs = dataframe_name.iloc[start_idx:end_idx + 1].T\n",
    "\n",
    "    # Reset column names\n",
    "    headers_gamelogs.columns = headers_gamelogs.iloc[0]\n",
    "    headers_gamelogs         = headers_gamelogs[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Remove those rows from original dataframe\n",
    "    dataframe_name = dataframe_name.drop(dataframe_name.index[start_idx:end_idx + 1]).reset_index(drop=True)\n",
    "    \n",
    "    # Use regex to split the column while preserving negative numbers\n",
    "    dataframe_name[['Name', 'Numbers']] = dataframe_name[0].str.extract(r'^(.*?)([-\\d\\s.,]*)$')\n",
    "        \n",
    "    # Split the 'Numbers' column into separate columns (25 columns)\n",
    "    dataframe_name = dataframe_name.join(dataframe_name['Numbers'].str.split(expand=True).rename(lambda x: f'col_{x+1}', axis=1))\n",
    "    \n",
    "    # Drop the first column\n",
    "    dataframe_name = dataframe_name.drop(columns=[0])\n",
    "    \n",
    "    # Drop the original 'Numbers' column\n",
    "    dataframe_name = dataframe_name.drop(columns=['Numbers'])\n",
    "    \n",
    "    # Remove the first 2 rows\n",
    "    dataframe_name = dataframe_name.iloc[2:].reset_index(drop= True)  # Using iloc\n",
    "    \n",
    "    # # Split the 'Name' column into 'Game Date' and 'Opponent'\n",
    "    # dataframe_name[['Game Date', 'Opponent']] = dataframe_name['Name'].str.split(\" \", n=2, expand=True)\n",
    "    \n",
    "    # Create a copy of the dataframe to store the gamelogs\n",
    "    gamelogs = dataframe_name.copy()\n",
    "    \n",
    "    # Split the first column into two\n",
    "    gamelogs[['Game Date', 'Opponent']] = gamelogs['Name'].str.split(\" \", n=1, expand=True)\n",
    "    \n",
    "    # Drop the first column\n",
    "    gamelogs = gamelogs.drop(columns= ['Name'])\n",
    "    \n",
    "    # Get the last two column names\n",
    "    last_two_cols  = gamelogs.columns[-2:].tolist()\n",
    "    remaining_cols = gamelogs.columns[:-2].tolist()\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    gamelogs = gamelogs[last_two_cols + remaining_cols]\n",
    "    \n",
    "    # Add the headers to the dataframe\n",
    "    gamelogs.columns = headers_gamelogs.columns\n",
    "\n",
    "    return gamelogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbed8c9",
   "metadata": {},
   "source": [
    "### Apply a function to the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! STATCAST TABLES\n",
    "# Initialize dictionaries to store the results for each team\n",
    "statcast_data_pitching            = {}\n",
    "plate_discipline_data_pitching    = {}\n",
    "batted_ball_profile_data_pitching = {}\n",
    "\n",
    "statcast_data_hitting             = {}\n",
    "plate_discipline_data_hitting     = {}\n",
    "batted_ball_profile_data_hitting  = {}\n",
    "\n",
    "#? Pitching\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in data_pitching.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    statcast, plate_discipline, batted_ball_profile = team_advanced_stats(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    statcast_data_pitching[team_id]            = statcast\n",
    "    plate_discipline_data_pitching[team_id]    = plate_discipline\n",
    "    batted_ball_profile_data_pitching[team_id] = batted_ball_profile\n",
    "\n",
    "#? Hitting\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in data_hitting.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    statcast, plate_discipline, batted_ball_profile = team_advanced_stats(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    statcast_data_hitting[team_id]            = statcast\n",
    "    plate_discipline_data_hitting[team_id]    = plate_discipline\n",
    "    batted_ball_profile_data_hitting[team_id] = batted_ball_profile\n",
    "    \n",
    "\n",
    "#! GAMELOGS\n",
    "# Initialize dictionaries to store the results for each team\n",
    "gamelog_pitching = {}\n",
    "gamelog_hitting  = {}\n",
    "\n",
    "#? Pitching\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in gamelog_data_pitching.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    gamelogs = process_gamelogs(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    gamelog_pitching[team_id] = gamelogs\n",
    "\n",
    "#? Hitting\n",
    "# Iterate over the team_data dictionary\n",
    "for team_id, df in gamelog_data_hitting.items():\n",
    "    # Call the team_advanced_stats function and unpack the returned DataFrames\n",
    "    gamelogs = process_gamelogs(df)\n",
    "    \n",
    "    # Store the results in the respective dictionaries\n",
    "    gamelog_hitting[team_id] = gamelogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2623dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_files(data_dict, suffix):\n",
    "    \"\"\"Create Parquet files from a dictionary of DataFrames.\"\"\"\n",
    "\n",
    "    # Create new DataFrames with a suffix\n",
    "    df_store = {}\n",
    "\n",
    "    for idx, (key, df) in enumerate(data_dict.items(), start= 1):\n",
    "        new_name = f\"{key}{suffix}\"  # Assigning a numbered suffix\n",
    "        df_store[new_name] = df.copy()  # Storing the DataFrame in a dictionary\n",
    "        \n",
    "    for name, df in df_store.items():\n",
    "        # Save hitting statcast DataFrames\n",
    "        df.to_parquet(f\"D:\\\\mlb_analyzer\\\\output\\\\teams\\\\{suffix}\\\\{name}.parquet\")\n",
    "\n",
    "\n",
    "# Call the function and export the DataFrames to Parquet files\n",
    "create_parquet_files(statcast_data_hitting, \"_hitting_statcast\")\n",
    "create_parquet_files(plate_discipline_data_hitting, \"_hitting_plate_discipline\")\n",
    "create_parquet_files(batted_ball_profile_data_hitting, \"_hitting_batted_ball_profile\")\n",
    "\n",
    "create_parquet_files(statcast_data_pitching, \"_pitching_statcast\")\n",
    "create_parquet_files(plate_discipline_data_pitching, \"_pitching_plate_discipline\")\n",
    "create_parquet_files(batted_ball_profile_data_pitching, \"_pitching_batted_ball_profile\")\n",
    "\n",
    "create_parquet_files(gamelog_hitting, \"_gamelogs_hitting\")\n",
    "create_parquet_files(gamelog_pitching, \"_gamelogs_pitching\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY_312_DEVELOPMENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
